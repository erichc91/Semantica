{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2075ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: en, Vocabulary Size: 1165190\n",
      "Sample Vocabulary: ['0', '0.22_inch_calibre', \"0.3_of_world's_population\", '0.4_percent_aluminum', '0.6_percent_calcium', '000', '007', '007s', '00_s', '00s']\n",
      "\n",
      "Language: de, Vocabulary Size: 519138\n",
      "Sample Vocabulary: ['0', '00', '0815', '08_15', '1', '1,1,1_trichlor_2,2_bis_4_chlorophenyl_ethan', '1,1_dimethylhydrazin', '1,2,3_propentricarbonsäure', '1,2_benzoldicarbonsäure', '1,2_dichlorethan']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_clean_triples(file_path, lang_code='en', sample_size=None):\n",
    "    \"\"\"\n",
    "    Load and clean ConceptNet triples for a specific language.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Full path to ConceptNet TSV file\n",
    "        lang_code (str): Language code to filter (e.g. 'en', 'de')\n",
    "        sample_size (int): Optional max number of triples to return\n",
    "\n",
    "    Returns:\n",
    "        List of (start, rel, end) tuples\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path, sep='\\t', header=None,\n",
    "        names=['uri', 'rel', 'start', 'end', 'info'], dtype=str\n",
    "    )\n",
    "    \n",
    "    lang_prefix = f'/c/{lang_code}/'\n",
    "    df = df[\n",
    "        df['start'].str.startswith(lang_prefix) &\n",
    "        df['end'].str.startswith(lang_prefix)\n",
    "    ].copy()\n",
    "\n",
    "    df['start'] = df['start'].str.split('/').str[3]\n",
    "    df['rel']   = df['rel'].str.split('/').str[-1]\n",
    "    df['end']   = df['end'].str.split('/').str[3]\n",
    "\n",
    "    triples = list(df[['start', 'rel', 'end']].itertuples(index=False, name=None))\n",
    "    \n",
    "    if sample_size and sample_size < len(triples):\n",
    "        import random\n",
    "        triples = random.sample(triples, sample_size)\n",
    "\n",
    "    return triples\n",
    "\n",
    "def extract_vocab(triples):\n",
    "    \"\"\"\n",
    "    Given list of (start, rel, end), return sorted list of unique words.\n",
    "    \"\"\"\n",
    "    vocab = sorted(set([s for s, _, _ in triples] + [e for _, _, e in triples]))\n",
    "    return vocab\n",
    "\n",
    "data_path  = r'C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data'\n",
    "input_data_path = os.path.join(data_path, 'Input')\n",
    "\n",
    "en_file_name = 'conceptnet-assertions-5.7.0.en.tsv'\n",
    "en_file_path = os.path.join(input_data_path, en_file_name)\n",
    "de_file_name = 'conceptnet-assertions-5.7.0.de.tsv'\n",
    "de_file_path = os.path.join(input_data_path, de_file_name)\n",
    "\n",
    "# en_triples = load_clean_triples(en_file_path, lang_code='en', sample_size=100_000)\n",
    "# de_triples = load_clean_triples(de_file_path, lang_code='de', sample_size=100_000)\n",
    "\n",
    "de_triples = load_clean_triples(de_file_path, lang_code='de')\n",
    "en_triples = load_clean_triples(en_file_path, lang_code='en')\n",
    "for lang, triples in [('en', en_triples), ('de', de_triples)]:\n",
    "    vocab = extract_vocab(triples)\n",
    "    print(f\"Language: {lang}, Vocabulary Size: {len(vocab)}\")\n",
    "    print(f\"Sample Vocabulary: {vocab[:10]}\")  # Print first 10 words as sample\n",
    "    print()  # Newline for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4756f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results: {'shared_vocab_size': 26284, 'shared_relations': 0, 'semantic_anchor_overlap': 0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class RSCAgent:\n",
    "    def __init__(self, initial_vocab: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize an agent with an initial vocabulary and relational memory.\n",
    "        \n",
    "        Args:\n",
    "            initial_vocab (List[str]): Starting vocabulary for the agent\n",
    "        \"\"\"\n",
    "        # Relational memory tracks symbol interactions\n",
    "        self.relational_memory = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # Vocabulary of known symbols\n",
    "        self.vocab = set(initial_vocab)\n",
    "        \n",
    "        # Semantic anchors tracking symbol relationships\n",
    "        self.semantic_anchors = defaultdict(set)\n",
    "    \n",
    "    def evaluate_relation(self, start: str, rel: str, end: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the consistency of a proposed relation.\n",
    "        \n",
    "        Args:\n",
    "            start (str): Starting symbol\n",
    "            rel (str): Relation type\n",
    "            end (str): Ending symbol\n",
    "        \n",
    "        Returns:\n",
    "            float: Confidence score for the relation (0-1)\n",
    "        \"\"\"\n",
    "        # If symbols not in vocab, initial skepticism\n",
    "        if start not in self.vocab or end not in self.vocab:\n",
    "            return 0.1\n",
    "        \n",
    "        # Existing relation count increases confidence\n",
    "        existing_count = self.relational_memory[start][rel]\n",
    "        total_relations = sum(self.relational_memory[start].values())\n",
    "        \n",
    "        # Compute a confidence score based on existing relations\n",
    "        if total_relations == 0:\n",
    "            return 0.5  # Neutral initial stance\n",
    "        \n",
    "        confidence = (existing_count / total_relations) * 0.9 + 0.1\n",
    "        return confidence\n",
    "    \n",
    "    def update_relation(self, start: str, rel: str, end: str, accept: bool):\n",
    "        \"\"\"\n",
    "        Update relational memory based on relation validation.\n",
    "        \n",
    "        Args:\n",
    "            start (str): Starting symbol\n",
    "            rel (str): Relation type\n",
    "            end (str): Ending symbol\n",
    "            accept (bool): Whether the relation was accepted\n",
    "        \"\"\"\n",
    "        # Add new symbols to vocabulary if not present\n",
    "        self.vocab.update([start, end])\n",
    "        \n",
    "        if accept:\n",
    "            # Increment relation count\n",
    "            self.relational_memory[start][rel] += 1\n",
    "            \n",
    "            # Update semantic anchors\n",
    "            self.semantic_anchors[start].add((rel, end))\n",
    "            self.semantic_anchors[end].add((rel.replace('Inverse', '') if 'Inverse' in rel else f'{rel}Inverse', start))\n",
    "\n",
    "def run_rsc_experiment(\n",
    "    en_triples: List[Tuple[str, str, str]], \n",
    "    de_triples: List[Tuple[str, str, str]], \n",
    "    num_iterations: int = 1000\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run the Relational Semantic Convergence experiment.\n",
    "    \n",
    "    Args:\n",
    "        en_triples (List[Tuple[str, str, str]]): English language triples\n",
    "        de_triples (List[Tuple[str, str, str]]): German language triples\n",
    "        num_iterations (int): Number of interaction iterations\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Experiment results and metrics\n",
    "    \"\"\"\n",
    "    # Initial vocabularies\n",
    "    en_vocab = list(set([s for s, _, _ in en_triples] + [e for _, _, e in en_triples]))\n",
    "    de_vocab = list(set([s for s, _, _ in de_triples] + [e for _, _, e in de_triples]))\n",
    "    \n",
    "    # Create agents\n",
    "    en_agent = RSCAgent(en_vocab)\n",
    "    de_agent = RSCAgent(de_vocab)\n",
    "    \n",
    "    # Tracking convergence metrics\n",
    "    convergence_metrics = {\n",
    "        'shared_vocab_size': 0,\n",
    "        'shared_relations': 0,\n",
    "        'semantic_anchor_overlap': 0\n",
    "    }\n",
    "    \n",
    "    # Experiment iterations\n",
    "    for _ in range(num_iterations):\n",
    "        # Randomly select triples from each language\n",
    "        en_triple = random.choice(en_triples)\n",
    "        de_triple = random.choice(de_triples)\n",
    "        \n",
    "        # Evaluate and potentially accept relations\n",
    "        en_confidence = en_agent.evaluate_relation(*en_triple)\n",
    "        de_confidence = de_agent.evaluate_relation(*de_triple)\n",
    "        \n",
    "        # Simple probabilistic acceptance based on confidence\n",
    "        en_accept = random.random() < en_confidence\n",
    "        de_accept = random.random() < de_confidence\n",
    "        \n",
    "        # Update agents' relational memories\n",
    "        en_agent.update_relation(*en_triple, en_accept)\n",
    "        de_agent.update_relation(*de_triple, de_accept)\n",
    "        \n",
    "        # Compute convergence metrics\n",
    "        convergence_metrics['shared_vocab_size'] = len(en_agent.vocab.intersection(de_agent.vocab))\n",
    "        \n",
    "        # Compute shared relations\n",
    "        shared_relations = 0\n",
    "        for start in en_agent.semantic_anchors:\n",
    "            if start in de_agent.semantic_anchors:\n",
    "                shared_relations += len(en_agent.semantic_anchors[start].intersection(de_agent.semantic_anchors[start]))\n",
    "        convergence_metrics['shared_relations'] = shared_relations\n",
    "    \n",
    "    return convergence_metrics\n",
    "\n",
    "# Experiment execution (commented out for now)\n",
    "results = run_rsc_experiment(en_triples, de_triples)\n",
    "print(\"Experiment Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47075b",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5761cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RSC Experiment:   0%|          | 5/10000 [00:00<07:08, 23.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10000 - Shared Vocab: 26284, Shared Relations: 0, Semantic Anchor Overlap: 0, Cross-Lingual Relations: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RSC Experiment:   1%|          | 105/10000 [00:05<07:08, 23.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 101/10000 - Shared Vocab: 26284, Shared Relations: 4, Semantic Anchor Overlap: 4, Cross-Lingual Relations: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RSC Experiment:   2%|▏         | 205/10000 [00:10<07:57, 20.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 201/10000 - Shared Vocab: 26284, Shared Relations: 4, Semantic Anchor Overlap: 4, Cross-Lingual Relations: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RSC Experiment:   3%|▎         | 304/10000 [00:14<07:40, 21.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 301/10000 - Shared Vocab: 26284, Shared Relations: 8, Semantic Anchor Overlap: 8, Cross-Lingual Relations: 182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RSC Experiment:   4%|▎         | 354/10000 [00:17<07:46, 20.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\erich\\AppData\\Local\\Temp\\ipykernel_24872\\3974507376.py\", line 196, in <module>\n",
      "    results = run_rsc_experiment(en_triples, de_triples)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Local\\Temp\\ipykernel_24872\\3974507376.py\", line -1, in run_rsc_experiment\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\erich\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RSCAgent:\n",
    "    def __init__(self, initial_vocab: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize an agent with an initial vocabulary and relational memory.\n",
    "        \n",
    "        Args:\n",
    "            initial_vocab (List[str]): Starting vocabulary for the agent\n",
    "        \"\"\"\n",
    "        # Relational memory tracks symbol interactions\n",
    "        self.relational_memory = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # Vocabulary of known symbols\n",
    "        self.vocab = set(initial_vocab)\n",
    "        \n",
    "        # Semantic anchors tracking symbol relationships\n",
    "        self.semantic_anchors = defaultdict(set)\n",
    "    \n",
    "    def evaluate_relation(self, start: str, rel: str, end: str) -> float:\n",
    "        \"\"\"\n",
    "        Enhanced relation evaluation with more flexible confidence scoring.\n",
    "        \n",
    "        Args:\n",
    "            start (str): Starting symbol\n",
    "            rel (str): Relation type\n",
    "            end (str): Ending symbol\n",
    "        \n",
    "        Returns:\n",
    "            float: Confidence score for the relation (0-1)\n",
    "        \"\"\"\n",
    "        # If symbols not in vocab, moderate initial openness\n",
    "        if start not in self.vocab or end not in self.vocab:\n",
    "            return 0.4  # More open to new relations\n",
    "        \n",
    "        # Existing relation count increases confidence\n",
    "        existing_count = self.relational_memory[start][rel]\n",
    "        total_relations = sum(self.relational_memory[start].values())\n",
    "        \n",
    "        # More nuanced confidence calculation\n",
    "        if total_relations == 0:\n",
    "            return 0.6  # More optimistic initial stance\n",
    "        \n",
    "        # Exponential decay to prevent over-fitting to existing relations\n",
    "        confidence = min(\n",
    "            0.9,  # Cap confidence\n",
    "            (existing_count / total_relations) ** 0.7 * 0.8 + 0.2\n",
    "        )\n",
    "        \n",
    "        return confidence\n",
    "\n",
    "    def find_potential_translation(self, word: str, other_vocab: set) -> str:\n",
    "        \"\"\"\n",
    "        Attempt to find potential translation using similarity heuristics.\n",
    "        \n",
    "        Args:\n",
    "            word (str): Source word\n",
    "            other_vocab (set): Vocabulary of target language\n",
    "        \n",
    "        Returns:\n",
    "            str: Potential translation or original word\n",
    "        \"\"\"\n",
    "        # Simple heuristics for finding potential translations\n",
    "        # 1. Exact match\n",
    "        if word in other_vocab:\n",
    "            return word\n",
    "        \n",
    "        # 2. Substring match (very naive, for demonstration)\n",
    "        for candidate in other_vocab:\n",
    "            if word.lower() in candidate.lower() or candidate.lower() in word.lower():\n",
    "                return candidate\n",
    "        \n",
    "        return word  # Fallback to original word\n",
    "    \n",
    "    def update_relation(self, start: str, rel: str, end: str, accept: bool):\n",
    "        \"\"\"\n",
    "        Update relational memory based on relation validation.\n",
    "        \n",
    "        Args:\n",
    "            start (str): Starting symbol\n",
    "            rel (str): Relation type\n",
    "            end (str): Ending symbol\n",
    "            accept (bool): Whether the relation was accepted\n",
    "        \"\"\"\n",
    "        # Add new symbols to vocabulary if not present\n",
    "        self.vocab.update([start, end])\n",
    "        \n",
    "        if accept:\n",
    "            # Increment relation count\n",
    "            self.relational_memory[start][rel] += 1\n",
    "            \n",
    "            # Update semantic anchors\n",
    "            self.semantic_anchors[start].add((rel, end))\n",
    "            self.semantic_anchors[end].add((rel.replace('Inverse', '') if 'Inverse' in rel else f'{rel}Inverse', start))\n",
    "\n",
    "def run_rsc_experiment(\n",
    "    en_triples: List[Tuple[str, str, str]], \n",
    "    de_triples: List[Tuple[str, str, str]], \n",
    "    num_iterations: int = 10000  # Increased iterations\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced Relational Semantic Convergence experiment.\n",
    "    \n",
    "    Args:\n",
    "        en_triples (List[Tuple[str, str, str]]): English language triples\n",
    "        de_triples (List[Tuple[str, str, str]]): German language triples\n",
    "        num_iterations (int): Number of interaction iterations\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Experiment results and metrics\n",
    "    \"\"\"\n",
    "    # Initial vocabularies\n",
    "    en_vocab = list(set([s for s, _, _ in en_triples] + [e for _, _, e in en_triples]))\n",
    "    de_vocab = list(set([s for s, _, _ in de_triples] + [e for _, _, e in de_triples]))\n",
    "    \n",
    "    # Create agents\n",
    "    en_agent = RSCAgent(en_vocab)\n",
    "    de_agent = RSCAgent(de_vocab)\n",
    "    \n",
    "    # Tracking convergence metrics\n",
    "    convergence_metrics = {\n",
    "        'shared_vocab_size': 0,\n",
    "        'shared_relations': 0,\n",
    "        'semantic_anchor_overlap': 0,\n",
    "        'cross_lingual_relations': 0\n",
    "    }\n",
    "    \n",
    "    # Cross-lingual translation mapping\n",
    "    de_vocab_set = set(de_vocab)\n",
    "    \n",
    "    # Experiment iterations\n",
    "    for _ in tqdm(range(num_iterations), desc=\"Running RSC Experiment\"):\n",
    "        # Cross-language interaction\n",
    "        en_triple = random.choice(en_triples)\n",
    "        de_triple = random.choice(de_triples)\n",
    "        \n",
    "        # Enhanced translation attempt\n",
    "        en_start_translation = en_agent.find_potential_translation(en_triple[0], de_vocab_set)\n",
    "        en_end_translation = en_agent.find_potential_translation(en_triple[2], de_vocab_set)\n",
    "        \n",
    "        # Evaluate and potentially accept relations\n",
    "        en_confidence = en_agent.evaluate_relation(*en_triple)\n",
    "        de_confidence = de_agent.evaluate_relation(*de_triple)\n",
    "        \n",
    "        # Cross-lingual relation mapping\n",
    "        cross_lingual_triple = (\n",
    "            en_start_translation, \n",
    "            en_triple[1],  # Assume relation type is language-independent \n",
    "            en_end_translation\n",
    "        )\n",
    "        \n",
    "        # Probabilistic acceptance with enhanced confidence\n",
    "        en_accept = random.random() < (en_confidence * 0.9 + 0.1)\n",
    "        de_accept = random.random() < (de_confidence * 0.9 + 0.1)\n",
    "        \n",
    "        # Update agents' relational memories\n",
    "        en_agent.update_relation(*en_triple, en_accept)\n",
    "        de_agent.update_relation(*de_triple, de_accept)\n",
    "        \n",
    "        # Cross-lingual relation exploration\n",
    "        cross_confidence = (en_confidence + de_confidence) / 2\n",
    "        cross_accept = random.random() < cross_confidence\n",
    "        if cross_accept:\n",
    "            de_agent.update_relation(*cross_lingual_triple, True)\n",
    "            convergence_metrics['cross_lingual_relations'] += 1\n",
    "        \n",
    "        # Compute convergence metrics\n",
    "        convergence_metrics['shared_vocab_size'] = len(en_agent.vocab.intersection(de_agent.vocab))\n",
    "        \n",
    "        # Compute shared relations\n",
    "        shared_relations = 0\n",
    "        semantic_anchor_overlap = 0\n",
    "        for start in en_agent.semantic_anchors:\n",
    "            if start in de_agent.semantic_anchors:\n",
    "                relation_intersection = en_agent.semantic_anchors[start].intersection(de_agent.semantic_anchors[start])\n",
    "                shared_relations += len(relation_intersection)\n",
    "                semantic_anchor_overlap += len(relation_intersection)\n",
    "        \n",
    "        convergence_metrics['shared_relations'] = shared_relations\n",
    "        convergence_metrics['semantic_anchor_overlap'] = semantic_anchor_overlap\n",
    "        if _ % 100 == 0:\n",
    "            # Print progress every 100 iterations\n",
    "            tqdm.write(f\"Iteration {_+1}/{num_iterations} - Shared Vocab: {convergence_metrics['shared_vocab_size']}, \"\n",
    "                       f\"Shared Relations: {convergence_metrics['shared_relations']}, \"\n",
    "                       f\"Semantic Anchor Overlap: {convergence_metrics['semantic_anchor_overlap']}, \"\n",
    "                       f\"Cross-Lingual Relations: {convergence_metrics['cross_lingual_relations']}\")\n",
    "            \n",
    "    return convergence_metrics\n",
    "\n",
    "# Experiment execution (commented out for now)\n",
    "results = run_rsc_experiment(en_triples, de_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0248358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results: {'shared_vocab_size': 26284, 'shared_relations': 0, 'semantic_anchor_overlap': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiment Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d6da1",
   "metadata": {},
   "source": [
    "# Optmized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0637b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10000 - Shared Vocab: 26284, Shared Relations: 0, Semantic Anchor Overlap: 0, Cross-Lingual Relations: 1\n",
      "Iteration 101/10000 - Shared Vocab: 26284, Shared Relations: 0, Semantic Anchor Overlap: 0, Cross-Lingual Relations: 61\n",
      "Iteration 201/10000 - Shared Vocab: 26284, Shared Relations: 0, Semantic Anchor Overlap: 0, Cross-Lingual Relations: 121\n",
      "Iteration 301/10000 - Shared Vocab: 26284, Shared Relations: 2, Semantic Anchor Overlap: 2, Cross-Lingual Relations: 180\n",
      "Iteration 401/10000 - Shared Vocab: 26284, Shared Relations: 4, Semantic Anchor Overlap: 4, Cross-Lingual Relations: 239\n",
      "Iteration 501/10000 - Shared Vocab: 26284, Shared Relations: 8, Semantic Anchor Overlap: 8, Cross-Lingual Relations: 299\n",
      "Iteration 601/10000 - Shared Vocab: 26284, Shared Relations: 14, Semantic Anchor Overlap: 14, Cross-Lingual Relations: 355\n",
      "Iteration 701/10000 - Shared Vocab: 26284, Shared Relations: 14, Semantic Anchor Overlap: 14, Cross-Lingual Relations: 416\n",
      "Iteration 801/10000 - Shared Vocab: 26284, Shared Relations: 16, Semantic Anchor Overlap: 16, Cross-Lingual Relations: 478\n",
      "Iteration 901/10000 - Shared Vocab: 26284, Shared Relations: 20, Semantic Anchor Overlap: 20, Cross-Lingual Relations: 541\n",
      "Iteration 1001/10000 - Shared Vocab: 26284, Shared Relations: 28, Semantic Anchor Overlap: 28, Cross-Lingual Relations: 607\n",
      "Iteration 1101/10000 - Shared Vocab: 26284, Shared Relations: 30, Semantic Anchor Overlap: 30, Cross-Lingual Relations: 668\n",
      "Iteration 1201/10000 - Shared Vocab: 26284, Shared Relations: 30, Semantic Anchor Overlap: 30, Cross-Lingual Relations: 724\n",
      "Iteration 1301/10000 - Shared Vocab: 26284, Shared Relations: 34, Semantic Anchor Overlap: 34, Cross-Lingual Relations: 785\n",
      "Iteration 1401/10000 - Shared Vocab: 26284, Shared Relations: 38, Semantic Anchor Overlap: 38, Cross-Lingual Relations: 847\n",
      "Iteration 1501/10000 - Shared Vocab: 26284, Shared Relations: 40, Semantic Anchor Overlap: 40, Cross-Lingual Relations: 907\n",
      "Iteration 1601/10000 - Shared Vocab: 26284, Shared Relations: 40, Semantic Anchor Overlap: 40, Cross-Lingual Relations: 967\n",
      "Iteration 1701/10000 - Shared Vocab: 26284, Shared Relations: 42, Semantic Anchor Overlap: 42, Cross-Lingual Relations: 1023\n",
      "Iteration 1801/10000 - Shared Vocab: 26284, Shared Relations: 42, Semantic Anchor Overlap: 42, Cross-Lingual Relations: 1090\n",
      "Iteration 1901/10000 - Shared Vocab: 26284, Shared Relations: 46, Semantic Anchor Overlap: 46, Cross-Lingual Relations: 1153\n",
      "Iteration 2001/10000 - Shared Vocab: 26284, Shared Relations: 50, Semantic Anchor Overlap: 50, Cross-Lingual Relations: 1217\n",
      "Iteration 2101/10000 - Shared Vocab: 26284, Shared Relations: 50, Semantic Anchor Overlap: 50, Cross-Lingual Relations: 1281\n",
      "Iteration 2201/10000 - Shared Vocab: 26284, Shared Relations: 56, Semantic Anchor Overlap: 56, Cross-Lingual Relations: 1342\n",
      "Iteration 2301/10000 - Shared Vocab: 26284, Shared Relations: 60, Semantic Anchor Overlap: 60, Cross-Lingual Relations: 1391\n",
      "Iteration 2401/10000 - Shared Vocab: 26284, Shared Relations: 62, Semantic Anchor Overlap: 62, Cross-Lingual Relations: 1453\n",
      "Iteration 2501/10000 - Shared Vocab: 26284, Shared Relations: 62, Semantic Anchor Overlap: 62, Cross-Lingual Relations: 1512\n",
      "Iteration 2601/10000 - Shared Vocab: 26284, Shared Relations: 64, Semantic Anchor Overlap: 64, Cross-Lingual Relations: 1576\n",
      "Iteration 2701/10000 - Shared Vocab: 26284, Shared Relations: 66, Semantic Anchor Overlap: 66, Cross-Lingual Relations: 1631\n",
      "Iteration 2801/10000 - Shared Vocab: 26284, Shared Relations: 70, Semantic Anchor Overlap: 70, Cross-Lingual Relations: 1695\n",
      "Iteration 2901/10000 - Shared Vocab: 26284, Shared Relations: 72, Semantic Anchor Overlap: 72, Cross-Lingual Relations: 1758\n",
      "Iteration 3001/10000 - Shared Vocab: 26284, Shared Relations: 72, Semantic Anchor Overlap: 72, Cross-Lingual Relations: 1822\n",
      "Iteration 3101/10000 - Shared Vocab: 26284, Shared Relations: 72, Semantic Anchor Overlap: 72, Cross-Lingual Relations: 1886\n",
      "Iteration 3201/10000 - Shared Vocab: 26284, Shared Relations: 82, Semantic Anchor Overlap: 82, Cross-Lingual Relations: 1950\n",
      "Iteration 3301/10000 - Shared Vocab: 26284, Shared Relations: 84, Semantic Anchor Overlap: 84, Cross-Lingual Relations: 2012\n",
      "Iteration 3401/10000 - Shared Vocab: 26284, Shared Relations: 84, Semantic Anchor Overlap: 84, Cross-Lingual Relations: 2068\n",
      "Iteration 3501/10000 - Shared Vocab: 26284, Shared Relations: 90, Semantic Anchor Overlap: 90, Cross-Lingual Relations: 2128\n",
      "Iteration 3601/10000 - Shared Vocab: 26284, Shared Relations: 92, Semantic Anchor Overlap: 92, Cross-Lingual Relations: 2187\n",
      "Iteration 3701/10000 - Shared Vocab: 26284, Shared Relations: 94, Semantic Anchor Overlap: 94, Cross-Lingual Relations: 2252\n",
      "Iteration 3801/10000 - Shared Vocab: 26284, Shared Relations: 96, Semantic Anchor Overlap: 96, Cross-Lingual Relations: 2324\n",
      "Iteration 3901/10000 - Shared Vocab: 26284, Shared Relations: 98, Semantic Anchor Overlap: 98, Cross-Lingual Relations: 2386\n",
      "Iteration 4001/10000 - Shared Vocab: 26285, Shared Relations: 104, Semantic Anchor Overlap: 104, Cross-Lingual Relations: 2450\n",
      "Iteration 4101/10000 - Shared Vocab: 26285, Shared Relations: 106, Semantic Anchor Overlap: 106, Cross-Lingual Relations: 2503\n",
      "Iteration 4201/10000 - Shared Vocab: 26285, Shared Relations: 108, Semantic Anchor Overlap: 108, Cross-Lingual Relations: 2557\n",
      "Iteration 4301/10000 - Shared Vocab: 26285, Shared Relations: 108, Semantic Anchor Overlap: 108, Cross-Lingual Relations: 2605\n",
      "Iteration 4401/10000 - Shared Vocab: 26285, Shared Relations: 110, Semantic Anchor Overlap: 110, Cross-Lingual Relations: 2663\n",
      "Iteration 4501/10000 - Shared Vocab: 26285, Shared Relations: 110, Semantic Anchor Overlap: 110, Cross-Lingual Relations: 2725\n",
      "Iteration 4601/10000 - Shared Vocab: 26285, Shared Relations: 114, Semantic Anchor Overlap: 114, Cross-Lingual Relations: 2783\n",
      "Iteration 4701/10000 - Shared Vocab: 26285, Shared Relations: 114, Semantic Anchor Overlap: 114, Cross-Lingual Relations: 2852\n",
      "Iteration 4801/10000 - Shared Vocab: 26285, Shared Relations: 114, Semantic Anchor Overlap: 114, Cross-Lingual Relations: 2914\n",
      "Iteration 4901/10000 - Shared Vocab: 26285, Shared Relations: 118, Semantic Anchor Overlap: 118, Cross-Lingual Relations: 2980\n",
      "Iteration 5001/10000 - Shared Vocab: 26285, Shared Relations: 120, Semantic Anchor Overlap: 120, Cross-Lingual Relations: 3033\n",
      "Iteration 5101/10000 - Shared Vocab: 26285, Shared Relations: 122, Semantic Anchor Overlap: 122, Cross-Lingual Relations: 3091\n",
      "Iteration 5201/10000 - Shared Vocab: 26285, Shared Relations: 124, Semantic Anchor Overlap: 124, Cross-Lingual Relations: 3151\n",
      "Iteration 5301/10000 - Shared Vocab: 26285, Shared Relations: 128, Semantic Anchor Overlap: 128, Cross-Lingual Relations: 3213\n",
      "Iteration 5401/10000 - Shared Vocab: 26285, Shared Relations: 128, Semantic Anchor Overlap: 128, Cross-Lingual Relations: 3272\n",
      "Iteration 5501/10000 - Shared Vocab: 26285, Shared Relations: 130, Semantic Anchor Overlap: 130, Cross-Lingual Relations: 3326\n",
      "Iteration 5601/10000 - Shared Vocab: 26285, Shared Relations: 130, Semantic Anchor Overlap: 130, Cross-Lingual Relations: 3389\n",
      "Iteration 5701/10000 - Shared Vocab: 26285, Shared Relations: 134, Semantic Anchor Overlap: 134, Cross-Lingual Relations: 3448\n",
      "Iteration 5801/10000 - Shared Vocab: 26285, Shared Relations: 136, Semantic Anchor Overlap: 136, Cross-Lingual Relations: 3509\n",
      "Iteration 5901/10000 - Shared Vocab: 26285, Shared Relations: 138, Semantic Anchor Overlap: 138, Cross-Lingual Relations: 3573\n",
      "Iteration 6001/10000 - Shared Vocab: 26285, Shared Relations: 140, Semantic Anchor Overlap: 140, Cross-Lingual Relations: 3635\n",
      "Iteration 6101/10000 - Shared Vocab: 26285, Shared Relations: 142, Semantic Anchor Overlap: 142, Cross-Lingual Relations: 3696\n",
      "Iteration 6201/10000 - Shared Vocab: 26285, Shared Relations: 142, Semantic Anchor Overlap: 142, Cross-Lingual Relations: 3761\n",
      "Iteration 6301/10000 - Shared Vocab: 26285, Shared Relations: 146, Semantic Anchor Overlap: 146, Cross-Lingual Relations: 3825\n",
      "Iteration 6401/10000 - Shared Vocab: 26285, Shared Relations: 148, Semantic Anchor Overlap: 148, Cross-Lingual Relations: 3887\n",
      "Iteration 6501/10000 - Shared Vocab: 26285, Shared Relations: 150, Semantic Anchor Overlap: 150, Cross-Lingual Relations: 3947\n",
      "Iteration 6601/10000 - Shared Vocab: 26285, Shared Relations: 150, Semantic Anchor Overlap: 150, Cross-Lingual Relations: 4017\n",
      "Iteration 6701/10000 - Shared Vocab: 26285, Shared Relations: 154, Semantic Anchor Overlap: 154, Cross-Lingual Relations: 4080\n",
      "Iteration 6801/10000 - Shared Vocab: 26285, Shared Relations: 162, Semantic Anchor Overlap: 162, Cross-Lingual Relations: 4147\n",
      "Iteration 6901/10000 - Shared Vocab: 26285, Shared Relations: 166, Semantic Anchor Overlap: 166, Cross-Lingual Relations: 4199\n",
      "Iteration 7001/10000 - Shared Vocab: 26285, Shared Relations: 166, Semantic Anchor Overlap: 166, Cross-Lingual Relations: 4266\n",
      "Iteration 7101/10000 - Shared Vocab: 26285, Shared Relations: 166, Semantic Anchor Overlap: 166, Cross-Lingual Relations: 4325\n",
      "Iteration 7201/10000 - Shared Vocab: 26285, Shared Relations: 168, Semantic Anchor Overlap: 168, Cross-Lingual Relations: 4387\n",
      "Iteration 7301/10000 - Shared Vocab: 26285, Shared Relations: 168, Semantic Anchor Overlap: 168, Cross-Lingual Relations: 4452\n",
      "Iteration 7401/10000 - Shared Vocab: 26285, Shared Relations: 170, Semantic Anchor Overlap: 170, Cross-Lingual Relations: 4521\n",
      "Iteration 7501/10000 - Shared Vocab: 26285, Shared Relations: 172, Semantic Anchor Overlap: 172, Cross-Lingual Relations: 4575\n",
      "Iteration 7601/10000 - Shared Vocab: 26285, Shared Relations: 174, Semantic Anchor Overlap: 174, Cross-Lingual Relations: 4636\n",
      "Iteration 7701/10000 - Shared Vocab: 26285, Shared Relations: 176, Semantic Anchor Overlap: 176, Cross-Lingual Relations: 4692\n",
      "Iteration 7801/10000 - Shared Vocab: 26285, Shared Relations: 180, Semantic Anchor Overlap: 180, Cross-Lingual Relations: 4756\n",
      "Iteration 7901/10000 - Shared Vocab: 26285, Shared Relations: 182, Semantic Anchor Overlap: 182, Cross-Lingual Relations: 4811\n",
      "Iteration 8001/10000 - Shared Vocab: 26285, Shared Relations: 184, Semantic Anchor Overlap: 184, Cross-Lingual Relations: 4876\n",
      "Iteration 8101/10000 - Shared Vocab: 26285, Shared Relations: 188, Semantic Anchor Overlap: 188, Cross-Lingual Relations: 4932\n",
      "Iteration 8201/10000 - Shared Vocab: 26285, Shared Relations: 188, Semantic Anchor Overlap: 188, Cross-Lingual Relations: 4994\n",
      "Iteration 8301/10000 - Shared Vocab: 26285, Shared Relations: 194, Semantic Anchor Overlap: 194, Cross-Lingual Relations: 5056\n",
      "Iteration 8401/10000 - Shared Vocab: 26285, Shared Relations: 198, Semantic Anchor Overlap: 198, Cross-Lingual Relations: 5112\n",
      "Iteration 8501/10000 - Shared Vocab: 26285, Shared Relations: 200, Semantic Anchor Overlap: 200, Cross-Lingual Relations: 5172\n",
      "Iteration 8601/10000 - Shared Vocab: 26285, Shared Relations: 202, Semantic Anchor Overlap: 202, Cross-Lingual Relations: 5237\n",
      "Iteration 8701/10000 - Shared Vocab: 26285, Shared Relations: 204, Semantic Anchor Overlap: 204, Cross-Lingual Relations: 5297\n",
      "Iteration 8801/10000 - Shared Vocab: 26285, Shared Relations: 212, Semantic Anchor Overlap: 212, Cross-Lingual Relations: 5363\n",
      "Iteration 8901/10000 - Shared Vocab: 26285, Shared Relations: 214, Semantic Anchor Overlap: 214, Cross-Lingual Relations: 5419\n",
      "Iteration 9001/10000 - Shared Vocab: 26285, Shared Relations: 218, Semantic Anchor Overlap: 218, Cross-Lingual Relations: 5480\n",
      "Iteration 9101/10000 - Shared Vocab: 26285, Shared Relations: 222, Semantic Anchor Overlap: 222, Cross-Lingual Relations: 5540\n",
      "Iteration 9201/10000 - Shared Vocab: 26285, Shared Relations: 226, Semantic Anchor Overlap: 226, Cross-Lingual Relations: 5609\n",
      "Iteration 9301/10000 - Shared Vocab: 26285, Shared Relations: 228, Semantic Anchor Overlap: 228, Cross-Lingual Relations: 5666\n",
      "Iteration 9401/10000 - Shared Vocab: 26285, Shared Relations: 228, Semantic Anchor Overlap: 228, Cross-Lingual Relations: 5729\n",
      "Iteration 9501/10000 - Shared Vocab: 26285, Shared Relations: 228, Semantic Anchor Overlap: 228, Cross-Lingual Relations: 5791\n",
      "Iteration 9601/10000 - Shared Vocab: 26285, Shared Relations: 234, Semantic Anchor Overlap: 234, Cross-Lingual Relations: 5844\n",
      "Iteration 9701/10000 - Shared Vocab: 26285, Shared Relations: 234, Semantic Anchor Overlap: 234, Cross-Lingual Relations: 5901\n",
      "Iteration 9801/10000 - Shared Vocab: 26285, Shared Relations: 236, Semantic Anchor Overlap: 236, Cross-Lingual Relations: 5960\n",
      "Iteration 9901/10000 - Shared Vocab: 26285, Shared Relations: 236, Semantic Anchor Overlap: 236, Cross-Lingual Relations: 6018\n",
      "\n",
      "Experiment Performance:\n",
      "Total Execution Time: 146.73 seconds\n",
      "Iterations per Second: 68.15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "class OptimizedRSCAgent:\n",
    "    __slots__ = ['relational_memory', 'vocab', 'semantic_anchors']\n",
    "    \n",
    "    def __init__(self, initial_vocab: List[str]):\n",
    "        \"\"\"\n",
    "        Optimized initialization with memory-efficient data structures.\n",
    "        \n",
    "        Args:\n",
    "            initial_vocab (List[str]): Starting vocabulary for the agent\n",
    "        \"\"\"\n",
    "        # Use dict instead of defaultdict for slight performance boost\n",
    "        self.relational_memory = {}\n",
    "        \n",
    "        # Use a set for constant-time lookup\n",
    "        self.vocab = set(initial_vocab)\n",
    "        \n",
    "        # Use a dict of frozensets for hashability and memory efficiency\n",
    "        self.semantic_anchors = {}\n",
    "    \n",
    "    def evaluate_relation(self, start: str, rel: str, end: str) -> float:\n",
    "        \"\"\"\n",
    "        Optimized relation evaluation with faster calculations.\n",
    "        \n",
    "        Args:\n",
    "            start (str): Starting symbol\n",
    "            rel (str): Relation type\n",
    "            end (str): Ending symbol\n",
    "        \n",
    "        Returns:\n",
    "            float: Confidence score for the relation (0-1)\n",
    "        \"\"\"\n",
    "        # Quick vocab check\n",
    "        if start not in self.vocab or end not in self.vocab:\n",
    "            return 0.4\n",
    "        \n",
    "        # Faster relation counting\n",
    "        start_relations = self.relational_memory.get(start, {})\n",
    "        existing_count = start_relations.get(rel, 0)\n",
    "        total_relations = sum(start_relations.values()) if start_relations else 0\n",
    "        \n",
    "        # Quick confidence calculation\n",
    "        if total_relations == 0:\n",
    "            return 0.6\n",
    "        \n",
    "        # Streamlined confidence computation\n",
    "        confidence = min(\n",
    "            0.9,\n",
    "            (existing_count / total_relations) ** 0.7 * 0.8 + 0.2\n",
    "        )\n",
    "        \n",
    "        return confidence\n",
    "\n",
    "    def find_potential_translation(self, word: str, other_vocab: Set[str]) -> str:\n",
    "        \"\"\"\n",
    "        Fast translation lookup with optimized matching.\n",
    "        \n",
    "        Args:\n",
    "            word (str): Source word\n",
    "            other_vocab (Set[str]): Vocabulary of target language\n",
    "        \n",
    "        Returns:\n",
    "            str: Potential translation or original word\n",
    "        \"\"\"\n",
    "        # Exact match first (fastest)\n",
    "        if word in other_vocab:\n",
    "            return word\n",
    "        \n",
    "        # Faster substring matching\n",
    "        word_lower = word.lower()\n",
    "        for candidate in other_vocab:\n",
    "            candidate_lower = candidate.lower()\n",
    "            if (word_lower in candidate_lower or \n",
    "                candidate_lower in word_lower):\n",
    "                return candidate\n",
    "        \n",
    "        return word\n",
    "    \n",
    "    def update_relation(self, start: str, rel: str, end: str, accept: bool):\n",
    "        \"\"\"\n",
    "        Optimized relation memory update.\n",
    "        \n",
    "        Args:\n",
    "            start (str): Starting symbol\n",
    "            rel (str): Relation type\n",
    "            end (str): Ending symbol\n",
    "            accept (bool): Whether the relation was accepted\n",
    "        \"\"\"\n",
    "        # Efficient vocab update\n",
    "        self.vocab.update([start, end])\n",
    "        \n",
    "        if accept:\n",
    "            # Efficient relation tracking\n",
    "            if start not in self.relational_memory:\n",
    "                self.relational_memory[start] = {}\n",
    "            self.relational_memory[start][rel] = self.relational_memory[start].get(rel, 0) + 1\n",
    "            \n",
    "            # Efficient semantic anchor update\n",
    "            if start not in self.semantic_anchors:\n",
    "                self.semantic_anchors[start] = set()\n",
    "            self.semantic_anchors[start].add((rel, end))\n",
    "            \n",
    "            if end not in self.semantic_anchors:\n",
    "                self.semantic_anchors[end] = set()\n",
    "            inverse_rel = rel.replace('Inverse', '') if 'Inverse' in rel else f'{rel}Inverse'\n",
    "            self.semantic_anchors[end].add((inverse_rel, start))\n",
    "\n",
    "def run_rsc_experiment(\n",
    "    en_triples: List[Tuple[str, str, str]], \n",
    "    de_triples: List[Tuple[str, str, str]], \n",
    "    num_iterations: int = 10000\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Highly optimized Relational Semantic Convergence experiment.\n",
    "    \n",
    "    Args:\n",
    "        en_triples (List[Tuple[str, str, str]]): English language triples\n",
    "        de_triples (List[Tuple[str, str, str]]): German language triples\n",
    "        num_iterations (int): Number of interaction iterations\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Experiment results and metrics\n",
    "    \"\"\"\n",
    "    # Precompute vocabularies\n",
    "    en_vocab = list(set([s for s, _, _ in en_triples] + [e for _, _, e in en_triples]))\n",
    "    de_vocab = list(set([s for s, _, _ in de_triples] + [e for _, _, e in de_triples]))\n",
    "    de_vocab_set = set(de_vocab)\n",
    "    \n",
    "    # Create agents\n",
    "    en_agent = OptimizedRSCAgent(en_vocab)\n",
    "    de_agent = OptimizedRSCAgent(de_vocab)\n",
    "    \n",
    "    # Preallocate metrics dictionary\n",
    "    convergence_metrics = {\n",
    "        'shared_vocab_size': 0,\n",
    "        'shared_relations': 0,\n",
    "        'semantic_anchor_overlap': 0,\n",
    "        'cross_lingual_relations': 0\n",
    "    }\n",
    "    \n",
    "    # Performance optimization: Precompute random choices\n",
    "    en_random_triples = [random.choice(en_triples) for _ in range(num_iterations)]\n",
    "    de_random_triples = [random.choice(de_triples) for _ in range(num_iterations)]\n",
    "    \n",
    "    # Main experiment loop\n",
    "    for i in range(num_iterations):\n",
    "        # Retrieve precomputed random triples\n",
    "        en_triple = en_random_triples[i]\n",
    "        de_triple = de_random_triples[i]\n",
    "        \n",
    "        # Efficient translation attempt\n",
    "        en_start_translation = en_agent.find_potential_translation(en_triple[0], de_vocab_set)\n",
    "        en_end_translation = en_agent.find_potential_translation(en_triple[2], de_vocab_set)\n",
    "        \n",
    "        # Evaluate relations\n",
    "        en_confidence = en_agent.evaluate_relation(*en_triple)\n",
    "        de_confidence = de_agent.evaluate_relation(*de_triple)\n",
    "        \n",
    "        # Probabilistic acceptance\n",
    "        en_accept = random.random() < (en_confidence * 0.9 + 0.1)\n",
    "        de_accept = random.random() < (de_confidence * 0.9 + 0.1)\n",
    "        \n",
    "        # Update agents\n",
    "        en_agent.update_relation(*en_triple, en_accept)\n",
    "        de_agent.update_relation(*de_triple, de_accept)\n",
    "        \n",
    "        # Cross-lingual relation exploration\n",
    "        cross_confidence = (en_confidence + de_confidence) / 2\n",
    "        cross_accept = random.random() < cross_confidence\n",
    "        if cross_accept:\n",
    "            cross_lingual_triple = (\n",
    "                en_start_translation, \n",
    "                en_triple[1],  # Language-independent relation \n",
    "                en_end_translation\n",
    "            )\n",
    "            de_agent.update_relation(*cross_lingual_triple, True)\n",
    "            convergence_metrics['cross_lingual_relations'] += 1\n",
    "        \n",
    "        # Compute convergence metrics periodically\n",
    "        if i % 100 == 0:\n",
    "            # Shared vocabulary\n",
    "            convergence_metrics['shared_vocab_size'] = len(en_agent.vocab.intersection(de_agent.vocab))\n",
    "            \n",
    "            # Shared relations computation\n",
    "            shared_relations = 0\n",
    "            semantic_anchor_overlap = 0\n",
    "            for start in en_agent.semantic_anchors:\n",
    "                if start in de_agent.semantic_anchors:\n",
    "                    relation_intersection = en_agent.semantic_anchors[start].intersection(de_agent.semantic_anchors[start])\n",
    "                    shared_relations += len(relation_intersection)\n",
    "                    semantic_anchor_overlap += len(relation_intersection)\n",
    "            \n",
    "            convergence_metrics['shared_relations'] = shared_relations\n",
    "            convergence_metrics['semantic_anchor_overlap'] = semantic_anchor_overlap\n",
    "            \n",
    "            # Optional: Print progress\n",
    "            print(f\"Iteration {i+1}/{num_iterations} - \"\n",
    "                  f\"Shared Vocab: {convergence_metrics['shared_vocab_size']}, \"\n",
    "                  f\"Shared Relations: {convergence_metrics['shared_relations']}, \"\n",
    "                  f\"Semantic Anchor Overlap: {convergence_metrics['semantic_anchor_overlap']}, \"\n",
    "                  f\"Cross-Lingual Relations: {convergence_metrics['cross_lingual_relations']}\")\n",
    "    \n",
    "    return convergence_metrics\n",
    "\n",
    "# Performance timing wrapper\n",
    "def time_experiment(en_triples, de_triples, num_iterations=10000):\n",
    "    \"\"\"\n",
    "    Time the RSC experiment and provide performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        en_triples (List[Tuple]): English language triples\n",
    "        de_triples (List[Tuple]): German language triples\n",
    "        num_iterations (int): Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (experiment results, execution time)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    results = run_rsc_experiment(en_triples, de_triples, num_iterations)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\nExperiment Performance:\")\n",
    "    print(f\"Total Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Iterations per Second: {num_iterations / (end_time - start_time):.2f}\")\n",
    "    \n",
    "    return results, end_time - start_time\n",
    "\n",
    "# Optional: Parallel experiment execution\n",
    "def parallel_experiment_runner(en_triples, de_triples, num_iterations=10000, num_runs=3):\n",
    "    \"\"\"\n",
    "    Run multiple experiments in parallel to assess consistency.\n",
    "    \n",
    "    Args:\n",
    "        en_triples (List[Tuple]): English language triples\n",
    "        de_triples (List[Tuple]): German language triples\n",
    "        num_iterations (int): Number of iterations per experiment\n",
    "        num_runs (int): Number of parallel experiment runs\n",
    "    \n",
    "    Returns:\n",
    "        List of experiment results\n",
    "    \"\"\"\n",
    "    with multiprocessing.Pool(processes=max(1, multiprocessing.cpu_count() - 1)) as pool:\n",
    "        # Create arguments for each experiment run\n",
    "        experiment_args = [(en_triples, de_triples, num_iterations) for _ in range(num_runs)]\n",
    "        \n",
    "        # Run experiments in parallel\n",
    "        results = pool.starmap(run_rsc_experiment, experiment_args)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment when ready)\n",
    "results = time_experiment(en_triples, de_triples)\n",
    "parallel_results = parallel_experiment_runner(en_triples, de_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ed199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
