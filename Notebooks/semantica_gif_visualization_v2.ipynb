{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deec642b",
   "metadata": {},
   "source": [
    "# Semantica relation GRaph visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6f5276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Semantica visualization with sentence relations\n",
      "Animation will be saved to: c:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Notebooks\\..\\Data\\Output\\semantica_animation_20250430-211824.gif\n",
      "Initializing animation...\n",
      "Creating initial semantic graph structure...\n",
      "Added 22 nodes to the graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating frames:  26%|â–ˆâ–ˆâ–‹       | 32/121 [00:04<00:11,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/120 frames (25.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating frames:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 62/121 [00:08<00:08,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/120 frames (50.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating frames:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 92/121 [00:12<00:04,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/120 frames (75.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating frames:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 105/121 [00:14<00:02,  7.22it/s]C:\\Users\\erich\\AppData\\Local\\Temp\\ipykernel_40684\\1823510520.py:559: UserWarning: Glyph 127822 (\\N{RED APPLE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(buf, format='png', dpi=100)\n",
      "Generating frames:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 107/121 [00:14<00:01,  7.30it/s]C:\\Users\\erich\\AppData\\Local\\Temp\\ipykernel_40684\\1823510520.py:559: UserWarning: Glyph 128021 (\\N{DOG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(buf, format='png', dpi=100)\n",
      "Generating frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 121/121 [00:16<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 120/120 frames (100.0%)\n",
      "Saving animation to c:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Notebooks\\..\\Data\\Output\\semantica_animation_20250430-211824.gif...\n",
      "Animation saved!\n",
      "Animation created at: c:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Notebooks\\..\\Data\\Output\\semantica_animation_20250430-211824.gif\n",
      "Process complete!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up the output directory structure\n",
    "curr_dir = os.getcwd()\n",
    "output_data_path = os.path.join(curr_dir + r\"\\..\", \"Data\", \"Output\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_data_path, exist_ok=True)\n",
    "\n",
    "# Define our word pairs (English-German)\n",
    "word_pairs = [\n",
    "    ('apple', 'Apfel'),\n",
    "    ('dog', 'Hund'),\n",
    "    ('cat', 'Katze'),\n",
    "    ('tree', 'Baum'),\n",
    "    ('house', 'Haus')\n",
    "]\n",
    "\n",
    "# Define sentence relations\n",
    "# Format: (sentence_id, language, sentence, [(word, role)])\n",
    "sentences = [\n",
    "    # Apple sentences\n",
    "    (1, 'english', \"The apple is red\", [('apple', 'subject'), ('red', 'property')]),\n",
    "    (1, 'german', \"Der Apfel ist rot\", [('Apfel', 'subject'), ('rot', 'property')]),\n",
    "    \n",
    "    # Animal sentences\n",
    "    (2, 'english', \"The dog chases the cat\", [('dog', 'agent'), ('cat', 'patient')]),\n",
    "    (2, 'german', \"Der Hund jagt die Katze\", [('Hund', 'agent'), ('Katze', 'patient')]),\n",
    "    \n",
    "    # Location sentences\n",
    "    (3, 'english', \"The dog sleeps under the tree\", [('dog', 'agent'), ('tree', 'location')]),\n",
    "    (3, 'german', \"Der Hund schlÃ¤ft unter dem Baum\", [('Hund', 'agent'), ('Baum', 'location')]),\n",
    "    \n",
    "    # Property sentences\n",
    "    (4, 'english', \"The house is big\", [('house', 'subject'), ('big', 'property')]),\n",
    "    (4, 'german', \"Das Haus ist groÃŸ\", [('Haus', 'subject'), ('groÃŸ', 'property')]),\n",
    "]\n",
    "\n",
    "# Additional words from sentences\n",
    "additional_words = {\n",
    "    'red': {'type': 'property', 'emoji': 'ðŸ”´'},\n",
    "    'rot': {'type': 'property', 'emoji': 'ðŸ”´'},\n",
    "    'big': {'type': 'property', 'emoji': 'ðŸ“'},\n",
    "    'groÃŸ': {'type': 'property', 'emoji': 'ðŸ“'},\n",
    "}\n",
    "\n",
    "# Types of words (for different shapes)\n",
    "word_types = {\n",
    "    'apple': 'fruit',\n",
    "    'Apfel': 'fruit',\n",
    "    'dog': 'animal',\n",
    "    'Hund': 'animal',\n",
    "    'cat': 'animal',\n",
    "    'Katze': 'animal',\n",
    "    'tree': 'plant',\n",
    "    'Baum': 'plant',\n",
    "    'house': 'object',\n",
    "    'Haus': 'object',\n",
    "    'red': 'property',\n",
    "    'rot': 'property',\n",
    "    'big': 'property',\n",
    "    'groÃŸ': 'property',\n",
    "}\n",
    "\n",
    "# Emoji mappings\n",
    "emoji_map = {\n",
    "    'apple': 'ðŸŽ',\n",
    "    'dog': 'ðŸ•',\n",
    "    'cat': 'ðŸˆ',\n",
    "    'tree': 'ðŸŒ³',\n",
    "    'house': 'ðŸ ',\n",
    "    'red': 'ðŸ”´',\n",
    "    'big': 'ðŸ“',\n",
    "}\n",
    "\n",
    "# Colors based on language\n",
    "language_colors = {\n",
    "    'english': '#3498db',  # Blue\n",
    "    'german': '#e74c3c'    # Red\n",
    "}\n",
    "\n",
    "# Colors based on word type\n",
    "type_colors = {\n",
    "    'fruit': '#2ecc71',    # Green\n",
    "    'animal': '#9b59b6',   # Purple\n",
    "    'plant': '#27ae60',    # Dark Green\n",
    "    'object': '#f39c12',   # Orange\n",
    "    'property': '#34495e', # Dark Gray\n",
    "    'sentence': '#7f8c8d'  # Light Gray\n",
    "}\n",
    "\n",
    "# Colors based on role in sentence\n",
    "role_colors = {\n",
    "    'subject': '#1abc9c',  # Teal\n",
    "    'property': '#9b59b6', # Purple\n",
    "    'agent': '#e67e22',    # Orange\n",
    "    'patient': '#3498db',  # Blue\n",
    "    'location': '#2ecc71'  # Green\n",
    "}\n",
    "\n",
    "# Shape based on type\n",
    "def get_node_shape(node_type):\n",
    "    shapes = {\n",
    "        'fruit': 'o',      # Circle\n",
    "        'animal': 's',     # Square\n",
    "        'plant': '^',      # Triangle\n",
    "        'object': 'd',     # Diamond\n",
    "        'property': 'p',   # Pentagon\n",
    "        'sentence': 'h'    # Hexagon\n",
    "    }\n",
    "    return shapes.get(node_type, 'o')\n",
    "\n",
    "# Create initial graph\n",
    "def create_initial_graph():\n",
    "    print(\"Creating initial semantic graph structure...\")\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes for words\n",
    "    for en, de in word_pairs:\n",
    "        # English words\n",
    "        G.add_node(en, \n",
    "                  lang='english',\n",
    "                  type=word_types[en],\n",
    "                  emoji=emoji_map[en],\n",
    "                  color=language_colors['english'],\n",
    "                  position=np.array([-1.5 + random.uniform(-0.1, 0.1), random.uniform(-0.8, 0.8)]),\n",
    "                  merged=False,\n",
    "                  size=300)\n",
    "        \n",
    "        # German words\n",
    "        G.add_node(de, \n",
    "                  lang='german',\n",
    "                  type=word_types[de],\n",
    "                  emoji=emoji_map[en],  # Use English word for emoji mapping\n",
    "                  color=language_colors['german'],\n",
    "                  position=np.array([1.5 + random.uniform(-0.1, 0.1), random.uniform(-0.8, 0.8)]),\n",
    "                  merged=False,\n",
    "                  size=300)\n",
    "    \n",
    "    # Add additional words from sentences\n",
    "    for word, info in additional_words.items():\n",
    "        if word not in G:\n",
    "            lang = 'english' if word in ['red', 'big'] else 'german'\n",
    "            emoji_key = word if word in emoji_map else 'red' if word in ['rot'] else 'big'\n",
    "            \n",
    "            G.add_node(word,\n",
    "                      lang=lang,\n",
    "                      type=info['type'],\n",
    "                      emoji=info['emoji'],\n",
    "                      color=language_colors[lang],\n",
    "                      position=np.array([-1.5 if lang == 'english' else 1.5, random.uniform(-0.8, 0.8)]),\n",
    "                      merged=False,\n",
    "                      size=300)\n",
    "    \n",
    "    # Add sentence nodes\n",
    "    sentence_positions = {}\n",
    "    for sent_id, lang, text, roles in sentences:\n",
    "        node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "        x_pos = -1.0 if lang == 'english' else 1.0\n",
    "        y_pos = 0.5 * sent_id\n",
    "        \n",
    "        sentence_positions[(sent_id, lang)] = np.array([x_pos, y_pos])\n",
    "        \n",
    "        G.add_node(node_id,\n",
    "                  lang=lang,\n",
    "                  type='sentence',\n",
    "                  text=text,\n",
    "                  color=type_colors['sentence'],\n",
    "                  position=np.array([x_pos, y_pos]),\n",
    "                  roles=roles,\n",
    "                  merged=False,\n",
    "                  size=400)\n",
    "    \n",
    "    print(f\"Added {len(G.nodes)} nodes to the graph\")\n",
    "    return G, sentence_positions\n",
    "\n",
    "# Add edges gradually in each frame\n",
    "def update_graph(G, sentence_positions, frame, total_frames):\n",
    "    progress = frame / total_frames\n",
    "    \n",
    "    # Phase 1: Add word-sentence relations (0-30% of frames)\n",
    "    if progress <= 0.3:\n",
    "        phase_progress = progress / 0.3\n",
    "        \n",
    "        # Process each sentence\n",
    "        for sent_id, lang, text, roles in sentences:\n",
    "            node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "            \n",
    "            # Add edges from words to sentences\n",
    "            for word, role in roles:\n",
    "                if word in G.nodes and not G.has_edge(node_id, word):\n",
    "                    # Determine if we should add this edge based on progress\n",
    "                    role_idx = ['subject', 'property', 'agent', 'patient', 'location'].index(role) if role in ['subject', 'property', 'agent', 'patient', 'location'] else 0\n",
    "                    threshold = 0.1 + (role_idx * 0.05)\n",
    "                    \n",
    "                    if phase_progress > threshold:\n",
    "                        # Add edge with role information\n",
    "                        edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                        G.add_edge(node_id, word, \n",
    "                                  role=role, \n",
    "                                  weight=edge_strength, \n",
    "                                  alpha=edge_strength,\n",
    "                                  color=role_colors.get(role, 'gray'))\n",
    "                \n",
    "                # If edge exists, increase its strength\n",
    "                elif G.has_edge(node_id, word):\n",
    "                    G[node_id][word]['weight'] = min(2.0, G[node_id][word]['weight'] + 0.05)\n",
    "                    G[node_id][word]['alpha'] = min(1.0, G[node_id][word]['alpha'] + 0.02)\n",
    "    \n",
    "    # Phase 2: Add cross-language word relations (30-60% of frames)\n",
    "    if 0.3 < progress <= 0.6:\n",
    "        phase_progress = (progress - 0.3) / 0.3\n",
    "        \n",
    "        # Add edges between equivalent words\n",
    "        for i, (en, de) in enumerate(word_pairs):\n",
    "            # Skip if already merged\n",
    "            if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                continue\n",
    "                \n",
    "            # Calculate threshold based on word pair index\n",
    "            threshold = i / len(word_pairs) * 0.8\n",
    "            \n",
    "            # Add the edge if we've progressed past the threshold for this pair\n",
    "            if phase_progress > threshold:\n",
    "                if not G.has_edge(en, de):\n",
    "                    # Add the edge with increasing opacity and width\n",
    "                    edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                    G.add_edge(en, de, weight=edge_strength, alpha=edge_strength, color='purple')\n",
    "                else:\n",
    "                    # Increase existing edge weight\n",
    "                    current_weight = G[en][de]['weight']\n",
    "                    G[en][de]['weight'] = min(3.0, current_weight + 0.1)\n",
    "                    G[en][de]['alpha'] = min(1.0, G[en][de]['alpha'] + 0.05)\n",
    "        \n",
    "        # Add edges between additional word pairs\n",
    "        additional_pairs = [('red', 'rot'), ('big', 'groÃŸ')]\n",
    "        for i, (en, de) in enumerate(additional_pairs):\n",
    "            if en in G.nodes and de in G.nodes:\n",
    "                # Skip if already merged\n",
    "                if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate threshold\n",
    "                threshold = 0.4 + (i / len(additional_pairs) * 0.4)\n",
    "                \n",
    "                # Add edge if we've progressed past threshold\n",
    "                if phase_progress > threshold:\n",
    "                    if not G.has_edge(en, de):\n",
    "                        edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                        G.add_edge(en, de, weight=edge_strength, alpha=edge_strength, color='purple')\n",
    "                    else:\n",
    "                        G[en][de]['weight'] = min(3.0, G[en][de]['weight'] + 0.1)\n",
    "                        G[en][de]['alpha'] = min(1.0, G[en][de]['alpha'] + 0.05)\n",
    "    \n",
    "    # Phase 3: Move nodes toward their equivalents (60-90% of frames)\n",
    "    if 0.6 < progress <= 0.9:\n",
    "        phase_progress = (progress - 0.6) / 0.3\n",
    "        \n",
    "        # Move word nodes toward their equivalents\n",
    "        for i, (en, de) in enumerate(word_pairs + [('red', 'rot'), ('big', 'groÃŸ')]):\n",
    "            if en in G.nodes and de in G.nodes:\n",
    "                # Skip if already merged\n",
    "                if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate threshold\n",
    "                threshold = i / (len(word_pairs) + 2) * 0.7\n",
    "                \n",
    "                # Move nodes if we've progressed past threshold\n",
    "                if phase_progress > threshold and G.has_edge(en, de):\n",
    "                    # Calculate target position (middle point with some variance)\n",
    "                    en_pos = G.nodes[en]['position']\n",
    "                    de_pos = G.nodes[de]['position']\n",
    "                    \n",
    "                    # Move gradually toward center\n",
    "                    midpoint = (en_pos + de_pos) / 2\n",
    "                    \n",
    "                    # Speed up as we get closer to the end\n",
    "                    move_factor = min(0.1, (phase_progress - threshold) * 0.3)\n",
    "                    \n",
    "                    G.nodes[en]['position'] = en_pos + (midpoint - en_pos) * move_factor\n",
    "                    G.nodes[de]['position'] = de_pos + (midpoint - de_pos) * move_factor\n",
    "                    \n",
    "                    # Merge nodes if they're close enough\n",
    "                    if np.linalg.norm(G.nodes[en]['position'] - G.nodes[de]['position']) < 0.2 and phase_progress > 0.9:\n",
    "                        # Mark these nodes as merged\n",
    "                        G.nodes[en]['merged'] = True\n",
    "                        G.nodes[de]['merged'] = True\n",
    "                        \n",
    "                        # Visually indicate they're merged by making them small and transparent\n",
    "                        G.nodes[en]['size'] = 0\n",
    "                        G.nodes[de]['size'] = 0\n",
    "                        G.nodes[en]['alpha'] = 0\n",
    "                        G.nodes[de]['alpha'] = 0\n",
    "                        \n",
    "                        # Create merged emoji node\n",
    "                        emoji = emoji_map.get(en, 'â“')\n",
    "                        merged_id = f\"merged_{en}_{de}\"\n",
    "                        \n",
    "                        G.add_node(merged_id,\n",
    "                                  type='merged',\n",
    "                                  emoji=emoji,\n",
    "                                  position=midpoint,\n",
    "                                  size=500,\n",
    "                                  alpha=1.0,\n",
    "                                  color='#1abc9c')  # Teal color for merged nodes\n",
    "                        \n",
    "                        # Copy all connections from original nodes to merged node\n",
    "                        for neighbor in set(G.neighbors(en)).union(set(G.neighbors(de))):\n",
    "                            if neighbor not in [en, de]:\n",
    "                                # Get the edge with highest weight\n",
    "                                weight1 = G.get_edge_data(en, neighbor, {'weight': 0})['weight'] if G.has_edge(en, neighbor) else 0\n",
    "                                weight2 = G.get_edge_data(de, neighbor, {'weight': 0})['weight'] if G.has_edge(de, neighbor) else 0\n",
    "                                \n",
    "                                max_weight = max(weight1, weight2)\n",
    "                                \n",
    "                                if neighbor.startswith('sentence_'):\n",
    "                                    # Get role information for sentence connections\n",
    "                                    role = None\n",
    "                                    if G.has_edge(en, neighbor):\n",
    "                                        role = G.get_edge_data(en, neighbor).get('role', 'default')\n",
    "                                    elif G.has_edge(de, neighbor):\n",
    "                                        role = G.get_edge_data(de, neighbor).get('role', 'default')\n",
    "                                    \n",
    "                                    G.add_edge(merged_id, neighbor,\n",
    "                                              weight=max_weight,\n",
    "                                              alpha=1.0,\n",
    "                                              role=role,\n",
    "                                              color=role_colors.get(role, 'gray'))\n",
    "                                else:\n",
    "                                    G.add_edge(merged_id, neighbor,\n",
    "                                              weight=max_weight,\n",
    "                                              alpha=1.0,\n",
    "                                              color='gray')\n",
    "        \n",
    "        # Move sentence nodes as well based on progress\n",
    "        for sent_id, lang, text, roles in sentences:\n",
    "            node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "            if node_id in G.nodes:\n",
    "                # Move English sentences left, German sentences right\n",
    "                x_target = -2.0 if lang == 'english' else 2.0\n",
    "                y_target = sentence_positions[(sent_id, lang)][1]\n",
    "                \n",
    "                current_pos = G.nodes[node_id]['position']\n",
    "                target_pos = np.array([x_target, y_target])\n",
    "                \n",
    "                # Calculate move factor based on progress\n",
    "                move_factor = phase_progress * 0.1\n",
    "                \n",
    "                # Update position\n",
    "                G.nodes[node_id]['position'] = current_pos + (target_pos - current_pos) * move_factor\n",
    "    \n",
    "    # Phase 4: Final stabilization (90-100% of frames)\n",
    "    if progress > 0.9:\n",
    "        # Add pulsating effect to merged nodes\n",
    "        phase_progress = (progress - 0.9) / 0.1\n",
    "        pulse = 1.0 + 0.2 * np.sin(phase_progress * 10 * np.pi)\n",
    "        \n",
    "        for node in list(G.nodes()):\n",
    "            if node.startswith('merged_'):\n",
    "                G.nodes[node]['size'] = 500 * pulse\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Draw the graph for animation\n",
    "def draw_graph(G, ax, frame, total_frames):\n",
    "    ax.clear()\n",
    "    \n",
    "    # Set up plot\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add title\n",
    "    percentage = int((frame / total_frames) * 100)\n",
    "    ax.set_title(f\"Semantica: Cross-Lingual Relational Convergence - {percentage}%\", fontsize=14)\n",
    "    \n",
    "    # Draw legend\n",
    "    # Language Legend\n",
    "    ax.text(-2.4, 1.3, \"Languages:\", fontsize=10, fontweight='bold')\n",
    "    ax.plot(-2.3, 1.2, 'o', color=language_colors['english'], markersize=8)\n",
    "    ax.text(-2.2, 1.2, \"English\", fontsize=9)\n",
    "    ax.plot(-2.3, 1.1, 'o', color=language_colors['german'], markersize=8)\n",
    "    ax.text(-2.2, 1.1, \"German\", fontsize=9)\n",
    "    \n",
    "    # Word Type Legend\n",
    "    ax.text(-2.4, 0.9, \"Word Types:\", fontsize=10, fontweight='bold')\n",
    "    \n",
    "    shapes = {'fruit': 'o', 'animal': 's', 'plant': '^', 'object': 'd', 'property': 'p', 'sentence': 'h'}\n",
    "    y_pos = 0.8\n",
    "    for type_name, shape in shapes.items():\n",
    "        ax.plot(-2.3, y_pos, shape, color=type_colors[type_name], markersize=8)\n",
    "        ax.text(-2.2, y_pos, type_name, fontsize=9)\n",
    "        y_pos -= 0.1\n",
    "    \n",
    "    # Relationship Role Legend\n",
    "    ax.text(1.8, 1.3, \"Relation Roles:\", fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_pos = 1.2\n",
    "    for role, color in role_colors.items():\n",
    "        ax.plot([1.9, 2.1], [y_pos, y_pos], '-', color=color, linewidth=2)\n",
    "        ax.text(2.2, y_pos, role, fontsize=9)\n",
    "        y_pos -= 0.1\n",
    "    \n",
    "    # Draw edges\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Skip edges connected to merged nodes (original words that merged)\n",
    "        if (G.nodes[u].get('merged', False) and not u.startswith('merged_')) or \\\n",
    "           (G.nodes[v].get('merged', False) and not v.startswith('merged_')):\n",
    "            continue\n",
    "            \n",
    "        pos1 = G.nodes[u]['position']\n",
    "        pos2 = G.nodes[v]['position']\n",
    "        \n",
    "        # Draw edge with appropriate thickness and transparency\n",
    "        weight = data.get('weight', 1.0)\n",
    "        alpha = data.get('alpha', 0.5)\n",
    "        color = data.get('color', 'gray')\n",
    "        \n",
    "        arrow = FancyArrowPatch(\n",
    "            posA=pos1, posB=pos2,\n",
    "            arrowstyle='-',\n",
    "            color=color,\n",
    "            linewidth=weight,\n",
    "            alpha=alpha,\n",
    "            mutation_scale=10\n",
    "        )\n",
    "        ax.add_patch(arrow)\n",
    "        \n",
    "        # Add role label if it's a sentence-word edge\n",
    "        if (u.startswith('sentence_') or v.startswith('sentence_')) and 'role' in data:\n",
    "            # Calculate midpoint\n",
    "            mid_x = (pos1[0] + pos2[0]) / 2\n",
    "            mid_y = (pos1[1] + pos2[1]) / 2\n",
    "            \n",
    "            # Add small offset to avoid overlap\n",
    "            offset = 0.05\n",
    "            label_pos = (mid_x + offset, mid_y + offset)\n",
    "            \n",
    "            # Add role label\n",
    "            if alpha > 0.7:  # Only show labels for strong edges\n",
    "                ax.text(label_pos[0], label_pos[1], data['role'], fontsize=7, \n",
    "                       ha='center', va='center', color='black',\n",
    "                       bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))\n",
    "    \n",
    "    # Draw sentence nodes first (so they are in back)\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if node.startswith('sentence_') and not data.get('merged', False):\n",
    "            pos = data['position']\n",
    "            \n",
    "            # Draw the node as a rounded rectangle\n",
    "            rect = plt.Rectangle(\n",
    "                (pos[0] - 0.3, pos[1] - 0.1),\n",
    "                0.6, 0.2,\n",
    "                facecolor=data['color'],\n",
    "                alpha=0.7,\n",
    "                edgecolor='black',\n",
    "                linewidth=1,\n",
    "                zorder=1\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add the sentence text\n",
    "            lang = data['lang']\n",
    "            text = data['text']\n",
    "            text_color = 'white' if np.mean(plt.matplotlib.colors.to_rgb(data['color'])) < 0.5 else 'black'\n",
    "            \n",
    "            ax.text(pos[0], pos[1], text, fontsize=7, ha='center', va='center', \n",
    "                   color=text_color, zorder=2,\n",
    "                   bbox=dict(facecolor=data['color'], alpha=0.0, pad=1))\n",
    "    \n",
    "    # Draw word nodes\n",
    "    for node, data in G.nodes(data=True):\n",
    "        # Skip rendering merged original nodes\n",
    "        if data.get('merged', False) and not node.startswith('merged_'):\n",
    "            continue\n",
    "            \n",
    "        # Skip sentence nodes (already drawn)\n",
    "        if node.startswith('sentence_'):\n",
    "            continue\n",
    "            \n",
    "        pos = data['position']\n",
    "        \n",
    "        # For merged nodes, draw the emoji\n",
    "        if node.startswith('merged_'):\n",
    "            emoji = data['emoji']\n",
    "            size = data['size']\n",
    "            ax.text(pos[0], pos[1], emoji, fontsize=24, ha='center', va='center')\n",
    "            \n",
    "            # Draw a subtle highlight circle\n",
    "            circle = plt.Circle(\n",
    "                pos, 0.15,\n",
    "                facecolor=data['color'],\n",
    "                alpha=0.3,\n",
    "                edgecolor=None,\n",
    "                zorder=3\n",
    "            )\n",
    "            ax.add_patch(circle)\n",
    "            continue\n",
    "        \n",
    "        # Regular word nodes\n",
    "        node_type = data['type']\n",
    "        shape = get_node_shape(node_type)\n",
    "        color = data['color']\n",
    "        alpha = data.get('alpha', 1.0)\n",
    "        size = data.get('size', 300)\n",
    "        \n",
    "        if size > 0:\n",
    "            ax.scatter(pos[0], pos[1], c=color, marker=shape, s=size, alpha=alpha, edgecolors='black', linewidths=1, zorder=4)\n",
    "            \n",
    "            # Label the node\n",
    "            label_color = 'white' if np.mean(plt.matplotlib.colors.to_rgb(color)) < 0.5 else 'black'\n",
    "            ax.text(pos[0], pos[1], node, fontsize=9, ha='center', va='center', color=label_color, zorder=5)\n",
    "\n",
    "# Create the animation function\n",
    "def create_semantica_animation():\n",
    "    # Set up the output path\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_file = os.path.join(output_data_path, f\"semantica_animation_{timestamp}.gif\")\n",
    "    \n",
    "    print(f\"Animation will be saved to: {output_file}\")\n",
    "    print(\"Initializing animation...\")\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Create initial graph\n",
    "    G, sentence_positions = create_initial_graph()\n",
    "    \n",
    "    # Set up animation\n",
    "    total_frames = 120\n",
    "    \n",
    "    # Create list to store frames for GIF\n",
    "    frames = []\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    for frame in tqdm(range(total_frames + 1), desc=\"Generating frames\"):\n",
    "        # Update graph based on frame\n",
    "        G = update_graph(G, sentence_positions, frame, total_frames)\n",
    "        \n",
    "        # Draw the current state\n",
    "        draw_graph(G, ax, frame, total_frames)\n",
    "        \n",
    "        # Capture the frame\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', dpi=100)\n",
    "        buf.seek(0)\n",
    "        img = Image.open(buf)\n",
    "        frames.append(img)\n",
    "        \n",
    "        # Print progress outside of tqdm\n",
    "        if frame % 30 == 0 and frame > 0:\n",
    "            print(f\"Processed {frame}/{total_frames} frames ({frame/total_frames*100:.1f}%)\")\n",
    "    \n",
    "    # Save the gif\n",
    "    print(f\"Saving animation to {output_file}...\")\n",
    "    frames[0].save(\n",
    "        output_file,\n",
    "        format='GIF',\n",
    "        append_images=frames[1:],\n",
    "        save_all=True,\n",
    "        duration=100,  # milliseconds per frame\n",
    "        loop=0  # loop forever\n",
    "    )\n",
    "    print(\"Animation saved!\")\n",
    "    \n",
    "    plt.close()\n",
    "    return output_file\n",
    "\n",
    "# Execute the function to create the animation\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Semantica visualization with sentence relations\")\n",
    "    gif_path = create_semantica_animation()\n",
    "    print(f\"Animation created at: {gif_path}\")\n",
    "    print(\"Process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61c9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
