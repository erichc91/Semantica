{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deec642b",
   "metadata": {},
   "source": [
    "# Semantica relation GRaph visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ad742",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up the output directory structure\n",
    "curr_dir = os.getcwd()\n",
    "output_data_path = os.path.join(curr_dir + r\"\\..\", \"Data\", \"Output\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_data_path, exist_ok=True)\n",
    "\n",
    "# Define our word pairs (English-German)\n",
    "word_pairs = [\n",
    "    ('apple', 'Apfel'),\n",
    "    ('dog', 'Hund'),\n",
    "    ('cat', 'Katze'),\n",
    "    ('tree', 'Baum'),\n",
    "    ('house', 'Haus')\n",
    "]\n",
    "\n",
    "# Define sentence relations\n",
    "# Format: (sentence_id, language, sentence, [(word, role)])\n",
    "sentences = [\n",
    "    # Apple sentences\n",
    "    (1, 'english', \"The apple is red\", [('apple', 'subject'), ('red', 'property')]),\n",
    "    (1, 'german', \"Der Apfel ist rot\", [('Apfel', 'subject'), ('rot', 'property')]),\n",
    "    \n",
    "    # Animal sentences\n",
    "    (2, 'english', \"The dog chases the cat\", [('dog', 'agent'), ('cat', 'patient')]),\n",
    "    (2, 'german', \"Der Hund jagt die Katze\", [('Hund', 'agent'), ('Katze', 'patient')]),\n",
    "    \n",
    "    # Location sentences\n",
    "    (3, 'english', \"The dog sleeps under the tree\", [('dog', 'agent'), ('tree', 'location')]),\n",
    "    (3, 'german', \"Der Hund schl√§ft unter dem Baum\", [('Hund', 'agent'), ('Baum', 'location')]),\n",
    "    \n",
    "    # Property sentences\n",
    "    (4, 'english', \"The house is big\", [('house', 'subject'), ('big', 'property')]),\n",
    "    (4, 'german', \"Das Haus ist gro√ü\", [('Haus', 'subject'), ('gro√ü', 'property')]),\n",
    "]\n",
    "\n",
    "# Additional words from sentences\n",
    "additional_words = {\n",
    "    'red': {'type': 'property', 'emoji': 'üî¥'},\n",
    "    'rot': {'type': 'property', 'emoji': 'üî¥'},\n",
    "    'big': {'type': 'property', 'emoji': 'üìè'},\n",
    "    'gro√ü': {'type': 'property', 'emoji': 'üìè'},\n",
    "}\n",
    "\n",
    "# Types of words (for different shapes)\n",
    "word_types = {\n",
    "    'apple': 'fruit',\n",
    "    'Apfel': 'fruit',\n",
    "    'dog': 'animal',\n",
    "    'Hund': 'animal',\n",
    "    'cat': 'animal',\n",
    "    'Katze': 'animal',\n",
    "    'tree': 'plant',\n",
    "    'Baum': 'plant',\n",
    "    'house': 'object',\n",
    "    'Haus': 'object',\n",
    "    'red': 'property',\n",
    "    'rot': 'property',\n",
    "    'big': 'property',\n",
    "    'gro√ü': 'property',\n",
    "}\n",
    "\n",
    "# Emoji mappings\n",
    "emoji_map = {\n",
    "    'apple': 'üçé',\n",
    "    'dog': 'üêï',\n",
    "    'cat': 'üêà',\n",
    "    'tree': 'üå≥',\n",
    "    'house': 'üè†',\n",
    "    'red': 'üî¥',\n",
    "    'big': 'üìè',\n",
    "}\n",
    "\n",
    "# Colors based on language\n",
    "language_colors = {\n",
    "    'english': '#3498db',  # Blue\n",
    "    'german': '#e74c3c'    # Red\n",
    "}\n",
    "\n",
    "# Colors based on word type\n",
    "type_colors = {\n",
    "    'fruit': '#2ecc71',    # Green\n",
    "    'animal': '#9b59b6',   # Purple\n",
    "    'plant': '#27ae60',    # Dark Green\n",
    "    'object': '#f39c12',   # Orange\n",
    "    'property': '#34495e', # Dark Gray\n",
    "    'sentence': '#7f8c8d'  # Light Gray\n",
    "}\n",
    "\n",
    "# Colors based on role in sentence\n",
    "role_colors = {\n",
    "    'subject': '#1abc9c',  # Teal\n",
    "    'property': '#9b59b6', # Purple\n",
    "    'agent': '#e67e22',    # Orange\n",
    "    'patient': '#3498db',  # Blue\n",
    "    'location': '#2ecc71'  # Green\n",
    "}\n",
    "\n",
    "# Shape based on type\n",
    "def get_node_shape(node_type):\n",
    "    shapes = {\n",
    "        'fruit': 'o',      # Circle\n",
    "        'animal': 's',     # Square\n",
    "        'plant': '^',      # Triangle\n",
    "        'object': 'd',     # Diamond\n",
    "        'property': 'p',   # Pentagon\n",
    "        'sentence': 'h'    # Hexagon\n",
    "    }\n",
    "    return shapes.get(node_type, 'o')\n",
    "\n",
    "# Create initial graph\n",
    "def create_initial_graph():\n",
    "    print(\"Creating initial semantic graph structure...\")\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes for words\n",
    "    for en, de in word_pairs:\n",
    "        # English words\n",
    "        G.add_node(en, \n",
    "                  lang='english',\n",
    "                  type=word_types[en],\n",
    "                  emoji=emoji_map[en],\n",
    "                  color=language_colors['english'],\n",
    "                  position=np.array([-1.5 + random.uniform(-0.1, 0.1), random.uniform(-0.8, 0.8)]),\n",
    "                  merged=False,\n",
    "                  size=300)\n",
    "        \n",
    "        # German words\n",
    "        G.add_node(de, \n",
    "                  lang='german',\n",
    "                  type=word_types[de],\n",
    "                  emoji=emoji_map[en],  # Use English word for emoji mapping\n",
    "                  color=language_colors['german'],\n",
    "                  position=np.array([1.5 + random.uniform(-0.1, 0.1), random.uniform(-0.8, 0.8)]),\n",
    "                  merged=False,\n",
    "                  size=300)\n",
    "    \n",
    "    # Add additional words from sentences\n",
    "    for word, info in additional_words.items():\n",
    "        if word not in G:\n",
    "            lang = 'english' if word in ['red', 'big'] else 'german'\n",
    "            emoji_key = word if word in emoji_map else 'red' if word in ['rot'] else 'big'\n",
    "            \n",
    "            G.add_node(word,\n",
    "                      lang=lang,\n",
    "                      type=info['type'],\n",
    "                      emoji=info['emoji'],\n",
    "                      color=language_colors[lang],\n",
    "                      position=np.array([-1.5 if lang == 'english' else 1.5, random.uniform(-0.8, 0.8)]),\n",
    "                      merged=False,\n",
    "                      size=300)\n",
    "    \n",
    "    # Add sentence nodes\n",
    "    sentence_positions = {}\n",
    "    for sent_id, lang, text, roles in sentences:\n",
    "        node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "        x_pos = -1.0 if lang == 'english' else 1.0\n",
    "        y_pos = 0.5 * sent_id\n",
    "        \n",
    "        sentence_positions[(sent_id, lang)] = np.array([x_pos, y_pos])\n",
    "        \n",
    "        G.add_node(node_id,\n",
    "                  lang=lang,\n",
    "                  type='sentence',\n",
    "                  text=text,\n",
    "                  color=type_colors['sentence'],\n",
    "                  position=np.array([x_pos, y_pos]),\n",
    "                  roles=roles,\n",
    "                  merged=False,\n",
    "                  size=400)\n",
    "    \n",
    "    print(f\"Added {len(G.nodes)} nodes to the graph\")\n",
    "    return G, sentence_positions\n",
    "\n",
    "# Add edges gradually in each frame\n",
    "def update_graph(G, sentence_positions, frame, total_frames):\n",
    "    progress = frame / total_frames\n",
    "    \n",
    "    # Phase 1: Add word-sentence relations (0-30% of frames)\n",
    "    if progress <= 0.3:\n",
    "        phase_progress = progress / 0.3\n",
    "        \n",
    "        # Process each sentence\n",
    "        for sent_id, lang, text, roles in sentences:\n",
    "            node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "            \n",
    "            # Add edges from words to sentences\n",
    "            for word, role in roles:\n",
    "                if word in G.nodes and not G.has_edge(node_id, word):\n",
    "                    # Determine if we should add this edge based on progress\n",
    "                    role_idx = ['subject', 'property', 'agent', 'patient', 'location'].index(role) if role in ['subject', 'property', 'agent', 'patient', 'location'] else 0\n",
    "                    threshold = 0.1 + (role_idx * 0.05)\n",
    "                    \n",
    "                    if phase_progress > threshold:\n",
    "                        # Add edge with role information\n",
    "                        edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                        G.add_edge(node_id, word, \n",
    "                                  role=role, \n",
    "                                  weight=edge_strength, \n",
    "                                  alpha=edge_strength,\n",
    "                                  color=role_colors.get(role, 'gray'))\n",
    "                \n",
    "                # If edge exists, increase its strength\n",
    "                elif G.has_edge(node_id, word):\n",
    "                    G[node_id][word]['weight'] = min(2.0, G[node_id][word]['weight'] + 0.05)\n",
    "                    G[node_id][word]['alpha'] = min(1.0, G[node_id][word]['alpha'] + 0.02)\n",
    "    \n",
    "    # Phase 2: Add cross-language word relations (30-60% of frames)\n",
    "    if 0.3 < progress <= 0.6:\n",
    "        phase_progress = (progress - 0.3) / 0.3\n",
    "        \n",
    "        # Add edges between equivalent words\n",
    "        for i, (en, de) in enumerate(word_pairs):\n",
    "            # Skip if already merged\n",
    "            if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                continue\n",
    "                \n",
    "            # Calculate threshold based on word pair index\n",
    "            threshold = i / len(word_pairs) * 0.8\n",
    "            \n",
    "            # Add the edge if we've progressed past the threshold for this pair\n",
    "            if phase_progress > threshold:\n",
    "                if not G.has_edge(en, de):\n",
    "                    # Add the edge with increasing opacity and width\n",
    "                    edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                    G.add_edge(en, de, weight=edge_strength, alpha=edge_strength, color='purple')\n",
    "                else:\n",
    "                    # Increase existing edge weight\n",
    "                    current_weight = G[en][de]['weight']\n",
    "                    G[en][de]['weight'] = min(3.0, current_weight + 0.1)\n",
    "                    G[en][de]['alpha'] = min(1.0, G[en][de]['alpha'] + 0.05)\n",
    "        \n",
    "        # Add edges between additional word pairs\n",
    "        additional_pairs = [('red', 'rot'), ('big', 'gro√ü')]\n",
    "        for i, (en, de) in enumerate(additional_pairs):\n",
    "            if en in G.nodes and de in G.nodes:\n",
    "                # Skip if already merged\n",
    "                if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate threshold\n",
    "                threshold = 0.4 + (i / len(additional_pairs) * 0.4)\n",
    "                \n",
    "                # Add edge if we've progressed past threshold\n",
    "                if phase_progress > threshold:\n",
    "                    if not G.has_edge(en, de):\n",
    "                        edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                        G.add_edge(en, de, weight=edge_strength, alpha=edge_strength, color='purple')\n",
    "                    else:\n",
    "                        G[en][de]['weight'] = min(3.0, G[en][de]['weight'] + 0.1)\n",
    "                        G[en][de]['alpha'] = min(1.0, G[en][de]['alpha'] + 0.05)\n",
    "    \n",
    "    # Phase 3: Move nodes toward their equivalents (60-90% of frames)\n",
    "    if 0.6 < progress <= 0.9:\n",
    "        phase_progress = (progress - 0.6) / 0.3\n",
    "        \n",
    "        # Move word nodes toward their equivalents\n",
    "        for i, (en, de) in enumerate(word_pairs + [('red', 'rot'), ('big', 'gro√ü')]):\n",
    "            if en in G.nodes and de in G.nodes:\n",
    "                # Skip if already merged\n",
    "                if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate threshold\n",
    "                threshold = i / (len(word_pairs) + 2) * 0.7\n",
    "                \n",
    "                # Move nodes if we've progressed past threshold\n",
    "                if phase_progress > threshold and G.has_edge(en, de):\n",
    "                    # Calculate target position (middle point with some variance)\n",
    "                    en_pos = G.nodes[en]['position']\n",
    "                    de_pos = G.nodes[de]['position']\n",
    "                    \n",
    "                    # Move gradually toward center\n",
    "                    midpoint = (en_pos + de_pos) / 2\n",
    "                    \n",
    "                    # Speed up as we get closer to the end\n",
    "                    move_factor = min(0.1, (phase_progress - threshold) * 0.3)\n",
    "                    \n",
    "                    G.nodes[en]['position'] = en_pos + (midpoint - en_pos) * move_factor\n",
    "                    G.nodes[de]['position'] = de_pos + (midpoint - de_pos) * move_factor\n",
    "                    \n",
    "                    # Merge nodes if they're close enough\n",
    "                    if np.linalg.norm(G.nodes[en]['position'] - G.nodes[de]['position']) < 0.2 and phase_progress > 0.9:\n",
    "                        # Mark these nodes as merged\n",
    "                        G.nodes[en]['merged'] = True\n",
    "                        G.nodes[de]['merged'] = True\n",
    "                        \n",
    "                        # Visually indicate they're merged by making them small and transparent\n",
    "                        G.nodes[en]['size'] = 0\n",
    "                        G.nodes[de]['size'] = 0\n",
    "                        G.nodes[en]['alpha'] = 0\n",
    "                        G.nodes[de]['alpha'] = 0\n",
    "                        \n",
    "                        # Create merged emoji node\n",
    "                        emoji = emoji_map.get(en, '‚ùì')\n",
    "                        merged_id = f\"merged_{en}_{de}\"\n",
    "                        \n",
    "                        G.add_node(merged_id,\n",
    "                                  type='merged',\n",
    "                                  emoji=emoji,\n",
    "                                  position=midpoint,\n",
    "                                  size=500,\n",
    "                                  alpha=1.0,\n",
    "                                  color='#1abc9c')  # Teal color for merged nodes\n",
    "                        \n",
    "                        # Copy all connections from original nodes to merged node\n",
    "                        for neighbor in set(G.neighbors(en)).union(set(G.neighbors(de))):\n",
    "                            if neighbor not in [en, de]:\n",
    "                                # Get the edge with highest weight\n",
    "                                weight1 = G.get_edge_data(en, neighbor, {'weight': 0})['weight'] if G.has_edge(en, neighbor) else 0\n",
    "                                weight2 = G.get_edge_data(de, neighbor, {'weight': 0})['weight'] if G.has_edge(de, neighbor) else 0\n",
    "                                \n",
    "                                max_weight = max(weight1, weight2)\n",
    "                                \n",
    "                                if neighbor.startswith('sentence_'):\n",
    "                                    # Get role information for sentence connections\n",
    "                                    role = None\n",
    "                                    if G.has_edge(en, neighbor):\n",
    "                                        role = G.get_edge_data(en, neighbor).get('role', 'default')\n",
    "                                    elif G.has_edge(de, neighbor):\n",
    "                                        role = G.get_edge_data(de, neighbor).get('role', 'default')\n",
    "                                    \n",
    "                                    G.add_edge(merged_id, neighbor,\n",
    "                                              weight=max_weight,\n",
    "                                              alpha=1.0,\n",
    "                                              role=role,\n",
    "                                              color=role_colors.get(role, 'gray'))\n",
    "                                else:\n",
    "                                    G.add_edge(merged_id, neighbor,\n",
    "                                              weight=max_weight,\n",
    "                                              alpha=1.0,\n",
    "                                              color='gray')\n",
    "        \n",
    "        # Move sentence nodes as well based on progress\n",
    "        for sent_id, lang, text, roles in sentences:\n",
    "            node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "            if node_id in G.nodes:\n",
    "                # Move English sentences left, German sentences right\n",
    "                x_target = -2.0 if lang == 'english' else 2.0\n",
    "                y_target = sentence_positions[(sent_id, lang)][1]\n",
    "                \n",
    "                current_pos = G.nodes[node_id]['position']\n",
    "                target_pos = np.array([x_target, y_target])\n",
    "                \n",
    "                # Calculate move factor based on progress\n",
    "                move_factor = phase_progress * 0.1\n",
    "                \n",
    "                # Update position\n",
    "                G.nodes[node_id]['position'] = current_pos + (target_pos - current_pos) * move_factor\n",
    "    \n",
    "    # Phase 4: Final stabilization (90-100% of frames)\n",
    "    if progress > 0.9:\n",
    "        # Add pulsating effect to merged nodes\n",
    "        phase_progress = (progress - 0.9) / 0.1\n",
    "        pulse = 1.0 + 0.2 * np.sin(phase_progress * 10 * np.pi)\n",
    "        \n",
    "        for node in list(G.nodes()):\n",
    "            if node.startswith('merged_'):\n",
    "                G.nodes[node]['size'] = 500 * pulse\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Draw the graph for animation\n",
    "def draw_graph(G, ax, frame, total_frames):\n",
    "    ax.clear()\n",
    "    \n",
    "    # Set up plot\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add title\n",
    "    percentage = int((frame / total_frames) * 100)\n",
    "    ax.set_title(f\"Semantica: Cross-Lingual Relational Convergence - {percentage}%\", fontsize=14)\n",
    "    \n",
    "    # Draw legend\n",
    "    # Language Legend\n",
    "    ax.text(-2.4, 1.3, \"Languages:\", fontsize=10, fontweight='bold')\n",
    "    ax.plot(-2.3, 1.2, 'o', color=language_colors['english'], markersize=8)\n",
    "    ax.text(-2.2, 1.2, \"English\", fontsize=9)\n",
    "    ax.plot(-2.3, 1.1, 'o', color=language_colors['german'], markersize=8)\n",
    "    ax.text(-2.2, 1.1, \"German\", fontsize=9)\n",
    "    \n",
    "    # Word Type Legend\n",
    "    ax.text(-2.4, 0.9, \"Word Types:\", fontsize=10, fontweight='bold')\n",
    "    \n",
    "    shapes = {'fruit': 'o', 'animal': 's', 'plant': '^', 'object': 'd', 'property': 'p', 'sentence': 'h'}\n",
    "    y_pos = 0.8\n",
    "    for type_name, shape in shapes.items():\n",
    "        ax.plot(-2.3, y_pos, shape, color=type_colors[type_name], markersize=8)\n",
    "        ax.text(-2.2, y_pos, type_name, fontsize=9)\n",
    "        y_pos -= 0.1\n",
    "    \n",
    "    # Relationship Role Legend\n",
    "    ax.text(1.8, 1.3, \"Relation Roles:\", fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_pos = 1.2\n",
    "    for role, color in role_colors.items():\n",
    "        ax.plot([1.9, 2.1], [y_pos, y_pos], '-', color=color, linewidth=2)\n",
    "        ax.text(2.2, y_pos, role, fontsize=9)\n",
    "        y_pos -= 0.1\n",
    "    \n",
    "    # Draw edges\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Skip edges connected to merged nodes (original words that merged)\n",
    "        if (G.nodes[u].get('merged', False) and not u.startswith('merged_')) or \\\n",
    "           (G.nodes[v].get('merged', False) and not v.startswith('merged_')):\n",
    "            continue\n",
    "            \n",
    "        pos1 = G.nodes[u]['position']\n",
    "        pos2 = G.nodes[v]['position']\n",
    "        \n",
    "        # Draw edge with appropriate thickness and transparency\n",
    "        weight = data.get('weight', 1.0)\n",
    "        alpha = data.get('alpha', 0.5)\n",
    "        color = data.get('color', 'gray')\n",
    "        \n",
    "        arrow = FancyArrowPatch(\n",
    "            posA=pos1, posB=pos2,\n",
    "            arrowstyle='-',\n",
    "            color=color,\n",
    "            linewidth=weight,\n",
    "            alpha=alpha,\n",
    "            mutation_scale=10\n",
    "        )\n",
    "        ax.add_patch(arrow)\n",
    "        \n",
    "        # Add role label if it's a sentence-word edge\n",
    "        if (u.startswith('sentence_') or v.startswith('sentence_')) and 'role' in data:\n",
    "            # Calculate midpoint\n",
    "            mid_x = (pos1[0] + pos2[0]) / 2\n",
    "            mid_y = (pos1[1] + pos2[1]) / 2\n",
    "            \n",
    "            # Add small offset to avoid overlap\n",
    "            offset = 0.05\n",
    "            label_pos = (mid_x + offset, mid_y + offset)\n",
    "            \n",
    "            # Add role label\n",
    "            if alpha > 0.7:  # Only show labels for strong edges\n",
    "                ax.text(label_pos[0], label_pos[1], data['role'], fontsize=7, \n",
    "                       ha='center', va='center', color='black',\n",
    "                       bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))\n",
    "    \n",
    "    # Draw sentence nodes first (so they are in back)\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if node.startswith('sentence_') and not data.get('merged', False):\n",
    "            pos = data['position']\n",
    "            \n",
    "            # Draw the node as a rounded rectangle\n",
    "            rect = plt.Rectangle(\n",
    "                (pos[0] - 0.3, pos[1] - 0.1),\n",
    "                0.6, 0.2,\n",
    "                facecolor=data['color'],\n",
    "                alpha=0.7,\n",
    "                edgecolor='black',\n",
    "                linewidth=1,\n",
    "                zorder=1\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add the sentence text\n",
    "            lang = data['lang']\n",
    "            text = data['text']\n",
    "            text_color = 'white' if np.mean(plt.matplotlib.colors.to_rgb(data['color'])) < 0.5 else 'black'\n",
    "            \n",
    "            ax.text(pos[0], pos[1], text, fontsize=7, ha='center', va='center', \n",
    "                   color=text_color, zorder=2,\n",
    "                   bbox=dict(facecolor=data['color'], alpha=0.0, pad=1))\n",
    "    \n",
    "    # Draw word nodes\n",
    "    for node, data in G.nodes(data=True):\n",
    "        # Skip rendering merged original nodes\n",
    "        if data.get('merged', False) and not node.startswith('merged_'):\n",
    "            continue\n",
    "            \n",
    "        # Skip sentence nodes (already drawn)\n",
    "        if node.startswith('sentence_'):\n",
    "            continue\n",
    "            \n",
    "        pos = data['position']\n",
    "        \n",
    "        # For merged nodes, draw the emoji\n",
    "        if node.startswith('merged_'):\n",
    "            emoji = data['emoji']\n",
    "            size = data['size']\n",
    "            ax.text(pos[0], pos[1], emoji, fontsize=24, ha='center', va='center')\n",
    "            \n",
    "            # Draw a subtle highlight circle\n",
    "            circle = plt.Circle(\n",
    "                pos, 0.15,\n",
    "                facecolor=data['color'],\n",
    "                alpha=0.3,\n",
    "                edgecolor=None,\n",
    "                zorder=3\n",
    "            )\n",
    "            ax.add_patch(circle)\n",
    "            continue\n",
    "        \n",
    "        # Regular word nodes\n",
    "        node_type = data['type']\n",
    "        shape = get_node_shape(node_type)\n",
    "        color = data['color']\n",
    "        alpha = data.get('alpha', 1.0)\n",
    "        size = data.get('size', 300)\n",
    "        \n",
    "        if size > 0:\n",
    "            ax.scatter(pos[0], pos[1], c=color, marker=shape, s=size, alpha=alpha, edgecolors='black', linewidths=1, zorder=4)\n",
    "            \n",
    "            # Label the node\n",
    "            label_color = 'white' if np.mean(plt.matplotlib.colors.to_rgb(color)) < 0.5 else 'black'\n",
    "            ax.text(pos[0], pos[1], node, fontsize=9, ha='center', va='center', color=label_color, zorder=5)\n",
    "\n",
    "# Create the animation function\n",
    "def create_semantica_animation():\n",
    "    # Set up the output path\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_file = os.path.join(output_data_path, f\"semantica_animation_{timestamp}.gif\")\n",
    "    \n",
    "    print(f\"Animation will be saved to: {output_file}\")\n",
    "    print(\"Initializing animation...\")\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Create initial graph\n",
    "    G, sentence_positions = create_initial_graph()\n",
    "    \n",
    "    # Set up animation\n",
    "    total_frames = 120\n",
    "    \n",
    "    # Create list to store frames for GIF\n",
    "    frames = []\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    for frame in tqdm(range(total_frames + 1), desc=\"Generating frames\"):\n",
    "        # Update graph based on frame\n",
    "        G = update_graph(G, sentence_positions, frame, total_frames)\n",
    "        \n",
    "        # Draw the current state\n",
    "        draw_graph(G, ax, frame, total_frames)\n",
    "        \n",
    "        # Capture the frame\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', dpi=100)\n",
    "        buf.seek(0)\n",
    "        img = Image.open(buf)\n",
    "        frames.append(img)\n",
    "        \n",
    "        # Print progress outside of tqdm\n",
    "        if frame % 30 == 0 and frame > 0:\n",
    "            print(f\"Processed {frame}/{total_frames} frames ({frame/total_frames*100:.1f}%)\")\n",
    "    \n",
    "    # Save the gif\n",
    "    print(f\"Saving animation to {output_file}...\")\n",
    "    frames[0].save(\n",
    "        output_file,\n",
    "        format='GIF',\n",
    "        append_images=frames[1:],\n",
    "        save_all=True,\n",
    "        duration=100,  # milliseconds per frame\n",
    "        loop=0  # loop forever\n",
    "    )\n",
    "    print(\"Animation saved!\")\n",
    "    \n",
    "    plt.close()\n",
    "    return output_file\n",
    "\n",
    "# Execute the function to create the animation\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Semantica visualization with sentence relations\")\n",
    "    gif_path = create_semantica_animation()\n",
    "    print(f\"Animation created at: {gif_path}\")\n",
    "    print(\"Process complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e44b02",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from matplotlib.patches import FancyArrowPatch, Polygon, PathPatch\n",
    "from matplotlib.path import Path\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image, ImageDraw, ImageFilter, ImageEnhance, ImageOps\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.patheffects as path_effects\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_rgba\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import gridspec\n",
    "import colorsys\n",
    "\n",
    "# Set up the output directory structure\n",
    "curr_dir = os.getcwd()\n",
    "output_data_path = os.path.join(curr_dir + r\"\\..\", \"Data\", \"Output\")\n",
    "os.makedirs(output_data_path, exist_ok=True)\n",
    "\n",
    "# Define our word pairs (English-German)\n",
    "word_pairs = [\n",
    "    ('apple', 'Apfel'),\n",
    "    ('dog', 'Hund'),\n",
    "    ('cat', 'Katze'),\n",
    "    ('tree', 'Baum'),\n",
    "    ('house', 'Haus')\n",
    "]\n",
    "\n",
    "# Define sentence relations\n",
    "# Format: (sentence_id, language, sentence, [(word, role)])\n",
    "sentences = [\n",
    "    # Apple sentences\n",
    "    (1, 'english', \"The apple is red\", [('apple', 'subject'), ('red', 'property')]),\n",
    "    (1, 'german', \"Der Apfel ist rot\", [('Apfel', 'subject'), ('rot', 'property')]),\n",
    "    \n",
    "    # Animal sentences\n",
    "    (2, 'english', \"The dog chases the cat\", [('dog', 'agent'), ('cat', 'patient')]),\n",
    "    (2, 'german', \"Der Hund jagt die Katze\", [('Hund', 'agent'), ('Katze', 'patient')]),\n",
    "    \n",
    "    # Location sentences\n",
    "    (3, 'english', \"The dog sleeps under the tree\", [('dog', 'agent'), ('tree', 'location')]),\n",
    "    (3, 'german', \"Der Hund schl√§ft unter dem Baum\", [('Hund', 'agent'), ('Baum', 'location')]),\n",
    "    \n",
    "    # Property sentences\n",
    "    (4, 'english', \"The house is big\", [('house', 'subject'), ('big', 'property')]),\n",
    "    (4, 'german', \"Das Haus ist gro√ü\", [('Haus', 'subject'), ('gro√ü', 'property')]),\n",
    "]\n",
    "\n",
    "# Additional words from sentences\n",
    "additional_words = {\n",
    "    'red': {'type': 'property', 'emoji': 'üî¥'},\n",
    "    'rot': {'type': 'property', 'emoji': 'üî¥'},\n",
    "    'big': {'type': 'property', 'emoji': 'üìè'},\n",
    "    'gro√ü': {'type': 'property', 'emoji': 'üìè'},\n",
    "}\n",
    "\n",
    "# Types of words (for different shapes)\n",
    "word_types = {\n",
    "    'apple': 'fruit',\n",
    "    'Apfel': 'fruit',\n",
    "    'dog': 'animal',\n",
    "    'Hund': 'animal',\n",
    "    'cat': 'animal',\n",
    "    'Katze': 'animal',\n",
    "    'tree': 'plant',\n",
    "    'Baum': 'plant',\n",
    "    'house': 'object',\n",
    "    'Haus': 'object',\n",
    "    'red': 'property',\n",
    "    'rot': 'property',\n",
    "    'big': 'property',\n",
    "    'gro√ü': 'property',\n",
    "}\n",
    "\n",
    "# Emoji mappings\n",
    "emoji_map = {\n",
    "    'apple': 'üçé',\n",
    "    'dog': 'üêï',\n",
    "    'cat': 'üêà',\n",
    "    'tree': 'üå≥',\n",
    "    'house': 'üè†',\n",
    "    'red': 'üî¥',\n",
    "    'big': 'üìè',\n",
    "}\n",
    "\n",
    "# Create custom color gradients for each language\n",
    "# Blue gradient for English\n",
    "english_cmap = LinearSegmentedColormap.from_list('english_gradient',\n",
    "                                                 ['#0a4c75', '#3498db', '#85c1e9'])\n",
    "\n",
    "# Red gradient for German\n",
    "german_cmap = LinearSegmentedColormap.from_list('german_gradient',\n",
    "                                               ['#7d1a0c', '#e74c3c', '#f5b7b1'])\n",
    "\n",
    "# Main colors based on language\n",
    "language_colors = {\n",
    "    'english': '#3498db',  # Blue\n",
    "    'german': '#e74c3c'    # Red\n",
    "}\n",
    "\n",
    "# Custom color schemes for visualization\n",
    "type_colors = {\n",
    "    'fruit': '#2ecc71',    # Green\n",
    "    'animal': '#9b59b6',   # Purple\n",
    "    'plant': '#27ae60',    # Dark Green\n",
    "    'object': '#f39c12',   # Orange\n",
    "    'property': '#34495e', # Dark Gray\n",
    "    'sentence': '#7f8c8d'  # Light Gray\n",
    "}\n",
    "\n",
    "# Colors based on role in sentence\n",
    "role_colors = {\n",
    "    'subject': '#1abc9c',  # Teal\n",
    "    'property': '#9b59b6', # Purple\n",
    "    'agent': '#e67e22',    # Orange\n",
    "    'patient': '#3498db',  # Blue\n",
    "    'location': '#2ecc71'  # Green\n",
    "}\n",
    "\n",
    "# Define custom node shapes\n",
    "def custom_node_shape(node_type, size=1.0):\n",
    "    \"\"\"Generate custom node shapes based on word type\"\"\"\n",
    "    # Base shapes with more visual interest\n",
    "    if node_type == 'fruit':\n",
    "        # Apple shape - circle with stem\n",
    "        r = 0.5 * size\n",
    "        theta = np.linspace(0, 2*np.pi, 30)\n",
    "        x = r * np.cos(theta)\n",
    "        y = r * np.sin(theta)\n",
    "        # Add stem\n",
    "        x = np.append(x, [0, 0.1*size, 0.2*size])\n",
    "        y = np.append(y, [r, r+0.2*size, r+0.1*size])\n",
    "        return x, y\n",
    "    \n",
    "    elif node_type == 'animal':\n",
    "        # Paw print shape\n",
    "        r = 0.3 * size\n",
    "        points = []\n",
    "        # Main pad\n",
    "        for theta in np.linspace(0, 2*np.pi, 20):\n",
    "            points.append((r * 1.2 * np.cos(theta), r * np.sin(theta) - 0.1*size))\n",
    "        # Toes\n",
    "        for i, angle in enumerate([0, np.pi/6, np.pi/3, np.pi/2]):\n",
    "            cx = r * np.cos(angle)\n",
    "            cy = r * np.sin(angle) + 0.3*size\n",
    "            for theta in np.linspace(0, 2*np.pi, 10):\n",
    "                points.append((cx + r*0.5*np.cos(theta), cy + r*0.5*np.sin(theta)))\n",
    "        \n",
    "        points = np.array(points)\n",
    "        return points[:, 0], points[:, 1]\n",
    "    \n",
    "    elif node_type == 'plant':\n",
    "        # Leaf shape\n",
    "        t = np.linspace(0, 2*np.pi, 100)\n",
    "        x = 16 * np.sin(t)**3\n",
    "        y = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n",
    "        # Scale and center\n",
    "        x = x / 32 * size\n",
    "        y = y / 32 * size\n",
    "        return x, y\n",
    "    \n",
    "    elif node_type == 'object':\n",
    "        # House shape\n",
    "        x = np.array([-0.5, 0, 0.5, 0.5, -0.5, -0.5]) * size\n",
    "        y = np.array([-0.5, 0.5, -0.5, -0.5, -0.5, -0.5]) * size\n",
    "        return x, y\n",
    "    \n",
    "    elif node_type == 'property':\n",
    "        # Starburst shape\n",
    "        n_points = 10\n",
    "        inner_radius = 0.3 * size\n",
    "        outer_radius = 0.5 * size\n",
    "        angles = np.linspace(0, 2*np.pi, n_points, endpoint=False)\n",
    "        \n",
    "        xs = []\n",
    "        ys = []\n",
    "        for i in range(n_points):\n",
    "            # Outer point\n",
    "            xs.append(outer_radius * np.cos(angles[i]))\n",
    "            ys.append(outer_radius * np.sin(angles[i]))\n",
    "            \n",
    "            # Inner point\n",
    "            xs.append(inner_radius * np.cos(angles[i] + np.pi/n_points))\n",
    "            ys.append(inner_radius * np.sin(angles[i] + np.pi/n_points))\n",
    "        \n",
    "        return np.array(xs), np.array(ys)\n",
    "    \n",
    "    else:  # default or 'sentence'\n",
    "        # Rounded rectangle\n",
    "        x = np.array([-0.6, -0.6, 0.6, 0.6, -0.6]) * size\n",
    "        y = np.array([-0.3, 0.3, 0.3, -0.3, -0.3]) * size\n",
    "        return x, y\n",
    "\n",
    "# Create initial graph\n",
    "def create_initial_graph():\n",
    "    print(\"Creating initial semantic graph structure...\")\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes for words\n",
    "    for en, de in word_pairs:\n",
    "        # English words\n",
    "        G.add_node(en, \n",
    "                  lang='english',\n",
    "                  type=word_types[en],\n",
    "                  emoji=emoji_map[en],\n",
    "                  color=language_colors['english'],\n",
    "                  position=np.array([-1.5 + random.uniform(-0.1, 0.1), random.uniform(-0.8, 0.8)]),\n",
    "                  merged=False,\n",
    "                  size=300,\n",
    "                  highlight=0.0,\n",
    "                  rotation=random.uniform(0, 360),\n",
    "                  z_position=0.0)\n",
    "        \n",
    "        # German words\n",
    "        G.add_node(de, \n",
    "                  lang='german',\n",
    "                  type=word_types[de],\n",
    "                  emoji=emoji_map[en],  # Use English word for emoji mapping\n",
    "                  color=language_colors['german'],\n",
    "                  position=np.array([1.5 + random.uniform(-0.1, 0.1), random.uniform(-0.8, 0.8)]),\n",
    "                  merged=False,\n",
    "                  size=300,\n",
    "                  highlight=0.0,\n",
    "                  rotation=random.uniform(0, 360),\n",
    "                  z_position=0.0)\n",
    "    \n",
    "    # Add additional words from sentences\n",
    "    for word, info in additional_words.items():\n",
    "        if word not in G:\n",
    "            lang = 'english' if word in ['red', 'big'] else 'german'\n",
    "            emoji_key = word if word in emoji_map else 'red' if word in ['rot'] else 'big'\n",
    "            \n",
    "            G.add_node(word,\n",
    "                      lang=lang,\n",
    "                      type=info['type'],\n",
    "                      emoji=info['emoji'],\n",
    "                      color=language_colors[lang],\n",
    "                      position=np.array([-1.5 if lang == 'english' else 1.5, random.uniform(-0.8, 0.8)]),\n",
    "                      merged=False,\n",
    "                      size=300,\n",
    "                      highlight=0.0,\n",
    "                      rotation=random.uniform(0, 360),\n",
    "                      z_position=0.0)\n",
    "    \n",
    "    # Add sentence nodes\n",
    "    sentence_positions = {}\n",
    "    for sent_id, lang, text, roles in sentences:\n",
    "        node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "        x_pos = -1.0 if lang == 'english' else 1.0\n",
    "        y_pos = 0.5 * sent_id\n",
    "        \n",
    "        sentence_positions[(sent_id, lang)] = np.array([x_pos, y_pos])\n",
    "        \n",
    "        G.add_node(node_id,\n",
    "                  lang=lang,\n",
    "                  type='sentence',\n",
    "                  text=text,\n",
    "                  color=type_colors['sentence'],\n",
    "                  position=np.array([x_pos, y_pos]),\n",
    "                  roles=roles,\n",
    "                  merged=False,\n",
    "                  size=400,\n",
    "                  highlight=0.0,\n",
    "                  rotation=0,\n",
    "                  z_position=-0.1)  # Place sentences slightly behind\n",
    "    \n",
    "    print(f\"Added {len(G.nodes)} nodes to the graph\")\n",
    "    return G, sentence_positions\n",
    "\n",
    "# Add pulsating effect to node\n",
    "def pulsate(t, base_size=1.0, amplitude=0.2, frequency=2.0):\n",
    "    return base_size + amplitude * np.sin(frequency * t)\n",
    "\n",
    "# Add edges gradually and update graph for each frame\n",
    "def update_graph(G, sentence_positions, frame, total_frames):\n",
    "    progress = frame / total_frames\n",
    "    \n",
    "    # Background frequency for global waves of activity\n",
    "    global_wave = np.sin(progress * 10 * np.pi) * 0.5 + 0.5\n",
    "    \n",
    "    # Phase 1: Add word-sentence relations (0-25% of frames)\n",
    "    if progress <= 0.25:\n",
    "        phase_progress = progress / 0.25\n",
    "        \n",
    "        # Process each sentence\n",
    "        for sent_id, lang, text, roles in sentences:\n",
    "            node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "            \n",
    "            # Highlight sentence node if it's being connected\n",
    "            if 0.05 < phase_progress < 0.95:\n",
    "                G.nodes[node_id]['highlight'] = min(1.0, G.nodes[node_id].get('highlight', 0) + 0.05)\n",
    "            else:\n",
    "                G.nodes[node_id]['highlight'] = max(0.0, G.nodes[node_id].get('highlight', 0) - 0.05)\n",
    "            \n",
    "            # Add edges from words to sentences\n",
    "            for word, role in roles:\n",
    "                if word in G.nodes:\n",
    "                    # Determine if we should add this edge based on progress\n",
    "                    role_idx = ['subject', 'property', 'agent', 'patient', 'location'].index(role) if role in ['subject', 'property', 'agent', 'patient', 'location'] else 0\n",
    "                    threshold = 0.1 + (role_idx * 0.03)\n",
    "                    \n",
    "                    # Highlight word when it's being connected\n",
    "                    if abs(phase_progress - threshold) < 0.1:\n",
    "                        G.nodes[word]['highlight'] = min(1.0, G.nodes[word].get('highlight', 0) + 0.1)\n",
    "                        # Add some vertical bounce\n",
    "                        G.nodes[word]['z_position'] = 0.1 * np.sin(phase_progress * 20 * np.pi)\n",
    "                    else:\n",
    "                        G.nodes[word]['highlight'] = max(0.0, G.nodes[word].get('highlight', 0) - 0.05)\n",
    "                        G.nodes[word]['z_position'] = max(0.0, G.nodes[word].get('z_position', 0) - 0.01)\n",
    "                    \n",
    "                    if phase_progress > threshold:\n",
    "                        if not G.has_edge(node_id, word):\n",
    "                            # Add edge with role information\n",
    "                            edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                            G.add_edge(node_id, word, \n",
    "                                      role=role, \n",
    "                                      weight=edge_strength, \n",
    "                                      alpha=edge_strength,\n",
    "                                      color=role_colors.get(role, 'gray'),\n",
    "                                      flow=0.0,\n",
    "                                      particles=[])\n",
    "                        \n",
    "                        # If edge exists, increase its strength\n",
    "                        elif G.has_edge(node_id, word):\n",
    "                            G[node_id][word]['weight'] = min(2.0, G[node_id][word]['weight'] + 0.05)\n",
    "                            G[node_id][word]['alpha'] = min(1.0, G[node_id][word]['alpha'] + 0.02)\n",
    "                            \n",
    "                            # Add flow effect to edges\n",
    "                            G[node_id][word]['flow'] = (G[node_id][word]['flow'] + 0.05) % 1.0\n",
    "                            \n",
    "                            # Generate particles along the edge for animation\n",
    "                            if random.random() < 0.1:\n",
    "                                G[node_id][word]['particles'].append(random.random())\n",
    "                            \n",
    "                            # Move existing particles along the edge\n",
    "                            new_particles = []\n",
    "                            for p in G[node_id][word]['particles']:\n",
    "                                p += 0.03\n",
    "                                if p < 1.0:\n",
    "                                    new_particles.append(p)\n",
    "                            G[node_id][word]['particles'] = new_particles\n",
    "    \n",
    "    # Phase 2: Add cross-language word relations (25-50% of frames)\n",
    "    if 0.25 < progress <= 0.5:\n",
    "        phase_progress = (progress - 0.25) / 0.25\n",
    "        \n",
    "        # Add edges between equivalent words\n",
    "        for i, (en, de) in enumerate(word_pairs):\n",
    "            # Skip if already merged\n",
    "            if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                continue\n",
    "                \n",
    "            # Calculate threshold based on word pair index\n",
    "            threshold = i / len(word_pairs) * 0.8\n",
    "            \n",
    "            # Highlight words when they're about to be connected\n",
    "            if abs(phase_progress - threshold) < 0.1:\n",
    "                G.nodes[en]['highlight'] = min(1.0, G.nodes[en].get('highlight', 0) + 0.1)\n",
    "                G.nodes[de]['highlight'] = min(1.0, G.nodes[de].get('highlight', 0) + 0.1)\n",
    "                \n",
    "                # Add anticipatory rotation to nodes\n",
    "                G.nodes[en]['rotation'] = (G.nodes[en]['rotation'] + 2) % 360\n",
    "                G.nodes[de]['rotation'] = (G.nodes[de]['rotation'] + 2) % 360\n",
    "            else:\n",
    "                G.nodes[en]['highlight'] = max(0.0, G.nodes[en].get('highlight', 0) - 0.05)\n",
    "                G.nodes[de]['highlight'] = max(0.0, G.nodes[de].get('highlight', 0) - 0.05)\n",
    "            \n",
    "            # Add the edge if we've progressed past the threshold for this pair\n",
    "            if phase_progress > threshold:\n",
    "                if not G.has_edge(en, de):\n",
    "                    # Add the edge with increasing opacity and width\n",
    "                    edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                    G.add_edge(en, de, \n",
    "                              weight=edge_strength, \n",
    "                              alpha=edge_strength, \n",
    "                              color='purple',\n",
    "                              flow=0.0,\n",
    "                              particles=[],\n",
    "                              electric=0.0)\n",
    "                else:\n",
    "                    # Increase existing edge weight\n",
    "                    current_weight = G[en][de]['weight']\n",
    "                    G[en][de]['weight'] = min(3.0, current_weight + 0.1)\n",
    "                    G[en][de]['alpha'] = min(1.0, G[en][de]['alpha'] + 0.05)\n",
    "                    \n",
    "                    # Add electric effect to cross-language edges\n",
    "                    G[en][de]['electric'] = (G[en][de]['electric'] + 0.05) % 1.0\n",
    "                    \n",
    "                    # Generate particles along the edge for animation\n",
    "                    if random.random() < 0.2:\n",
    "                        G[en][de]['particles'].append(random.random())\n",
    "                    \n",
    "                    # Move existing particles along the edge\n",
    "                    new_particles = []\n",
    "                    for p in G[en][de]['particles']:\n",
    "                        p += 0.04\n",
    "                        if p < 1.0:\n",
    "                            new_particles.append(p)\n",
    "                    G[en][de]['particles'] = new_particles\n",
    "        \n",
    "        # Add edges between additional word pairs\n",
    "        additional_pairs = [('red', 'rot'), ('big', 'gro√ü')]\n",
    "        for i, (en, de) in enumerate(additional_pairs):\n",
    "            if en in G.nodes and de in G.nodes:\n",
    "                # Skip if already merged\n",
    "                if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate threshold\n",
    "                threshold = 0.4 + (i / len(additional_pairs) * 0.4)\n",
    "                \n",
    "                # Highlight words when they're about to be connected\n",
    "                if abs(phase_progress - threshold) < 0.1:\n",
    "                    G.nodes[en]['highlight'] = min(1.0, G.nodes[en].get('highlight', 0) + 0.1)\n",
    "                    G.nodes[de]['highlight'] = min(1.0, G.nodes[de].get('highlight', 0) + 0.1)\n",
    "                else:\n",
    "                    G.nodes[en]['highlight'] = max(0.0, G.nodes[en].get('highlight', 0) - 0.05)\n",
    "                    G.nodes[de]['highlight'] = max(0.0, G.nodes[de].get('highlight', 0) - 0.05)\n",
    "                \n",
    "                # Add edge if we've progressed past threshold\n",
    "                if phase_progress > threshold:\n",
    "                    if not G.has_edge(en, de):\n",
    "                        edge_strength = min(1.0, (phase_progress - threshold) * 5)\n",
    "                        G.add_edge(en, de, \n",
    "                                  weight=edge_strength, \n",
    "                                  alpha=edge_strength, \n",
    "                                  color='purple',\n",
    "                                  flow=0.0,\n",
    "                                  particles=[],\n",
    "                                  electric=0.0)\n",
    "                    else:\n",
    "                        G[en][de]['weight'] = min(3.0, G[en][de]['weight'] + 0.1)\n",
    "                        G[en][de]['alpha'] = min(1.0, G[en][de]['alpha'] + 0.05)\n",
    "                        \n",
    "                        # Add electric effect to cross-language edges\n",
    "                        G[en][de]['electric'] = (G[en][de]['electric'] + 0.05) % 1.0\n",
    "                        \n",
    "                        # Generate particles along the edge for animation\n",
    "                        if random.random() < 0.2:\n",
    "                            G[en][de]['particles'].append(random.random())\n",
    "                        \n",
    "                        # Move existing particles\n",
    "                        new_particles = []\n",
    "                        for p in G[en][de]['particles']:\n",
    "                            p += 0.04\n",
    "                            if p < 1.0:\n",
    "                                new_particles.append(p)\n",
    "                        G[en][de]['particles'] = new_particles\n",
    "    \n",
    "    # Phase 3: Move nodes toward their equivalents (50-80% of frames)\n",
    "    if 0.5 < progress <= 0.8:\n",
    "        phase_progress = (progress - 0.5) / 0.3\n",
    "        \n",
    "        # Calculate global wave for synchronized movement\n",
    "        wave_effect = np.sin(phase_progress * 5 * np.pi) * 0.5 + 0.5\n",
    "        \n",
    "        # Move word nodes toward their equivalents\n",
    "        for i, (en, de) in enumerate(word_pairs + [('red', 'rot'), ('big', 'gro√ü')]):\n",
    "            if en in G.nodes and de in G.nodes:\n",
    "                # Skip if already merged\n",
    "                if G.nodes[en].get('merged') or G.nodes[de].get('merged'):\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate threshold\n",
    "                threshold = i / (len(word_pairs) + 2) * 0.7\n",
    "                \n",
    "                # Move nodes if we've progressed past threshold\n",
    "                if phase_progress > threshold and G.has_edge(en, de):\n",
    "                    # Calculate target position (middle point with some variance)\n",
    "                    en_pos = G.nodes[en]['position']\n",
    "                    de_pos = G.nodes[de]['position']\n",
    "                    \n",
    "                    # Move gradually toward center\n",
    "                    midpoint = (en_pos + de_pos) / 2\n",
    "                    \n",
    "                    # Speed up as we get closer to the end\n",
    "                    move_factor = min(0.1, (phase_progress - threshold) * 0.3 * (1 + wave_effect * 0.5))\n",
    "                    \n",
    "                    # Add oscillation to the movement\n",
    "                    oscillation = 0.03 * np.sin(phase_progress * 20 * np.pi)\n",
    "                    \n",
    "                    G.nodes[en]['position'] = en_pos + (midpoint - en_pos) * move_factor + np.array([oscillation, oscillation])\n",
    "                    G.nodes[de]['position'] = de_pos + (midpoint - de_pos) * move_factor + np.array([-oscillation, oscillation])\n",
    "                    \n",
    "                    # Change rotation as nodes approach each other\n",
    "                    G.nodes[en]['rotation'] = (G.nodes[en]['rotation'] + 1) % 360\n",
    "                    G.nodes[de]['rotation'] = (G.nodes[de]['rotation'] + 1) % 360\n",
    "                    \n",
    "                    # Increase z_position as nodes approach merging point\n",
    "                    approach_factor = 1.0 - np.linalg.norm(G.nodes[en]['position'] - G.nodes[de]['position'])\n",
    "                    G.nodes[en]['z_position'] = max(0.0, approach_factor * 0.3)\n",
    "                    G.nodes[de]['z_position'] = max(0.0, approach_factor * 0.3)\n",
    "                    \n",
    "                    # Enhance highlight as nodes approach\n",
    "                    G.nodes[en]['highlight'] = min(1.0, approach_factor * 2)\n",
    "                    G.nodes[de]['highlight'] = min(1.0, approach_factor * 2)\n",
    "                    \n",
    "                    # Merge nodes if they're close enough\n",
    "                    if np.linalg.norm(G.nodes[en]['position'] - G.nodes[de]['position']) < 0.2 and phase_progress > 0.9:\n",
    "                        # Mark these nodes as merged\n",
    "                        G.nodes[en]['merged'] = True\n",
    "                        G.nodes[de]['merged'] = True\n",
    "                        \n",
    "                        # Visually indicate they're merged by making them small and transparent\n",
    "                        G.nodes[en]['size'] = 0\n",
    "                        G.nodes[de]['size'] = 0\n",
    "                        G.nodes[en]['alpha'] = 0\n",
    "                        G.nodes[de]['alpha'] = 0\n",
    "                        \n",
    "                        # Create merged emoji node\n",
    "                        emoji = emoji_map.get(en, '‚ùì')\n",
    "                        merged_id = f\"merged_{en}_{de}\"\n",
    "                        \n",
    "                        G.add_node(merged_id,\n",
    "                                  type='merged',\n",
    "                                  emoji=emoji,\n",
    "                                  position=midpoint,\n",
    "                                  size=500,\n",
    "                                  alpha=1.0,\n",
    "                                  highlight=1.0,\n",
    "                                  color='#1abc9c',  # Teal color for merged nodes\n",
    "                                  rotation=0,\n",
    "                                  z_position=0.5,  # Pop up slightly\n",
    "                                  scale=1.0,\n",
    "                                  merged_from=[en, de])\n",
    "                        \n",
    "                        # Copy all connections from original nodes to merged node\n",
    "                        for source, target in list(G.edges()):\n",
    "                            if source == en or source == de:\n",
    "                                other = target\n",
    "                                # Skip the edge that connected the two merged nodes\n",
    "                                if other == en or other == de:\n",
    "                                    continue\n",
    "                                    \n",
    "                                # Get edge data\n",
    "                                if source == en:\n",
    "                                    edge_data = G.get_edge_data(en, other).copy()\n",
    "                                else:\n",
    "                                    edge_data = G.get_edge_data(de, other).copy()\n",
    "                                \n",
    "                                # Add edge from merged node to other\n",
    "                                G.add_edge(merged_id, other, **edge_data)\n",
    "                                \n",
    "                            elif target == en or target == de:\n",
    "                                other = source\n",
    "                                # Skip the edge that connected the two merged nodes\n",
    "                                if other == en or other == de:\n",
    "                                    continue\n",
    "                                    \n",
    "                                # Get edge data\n",
    "                                if target == en:\n",
    "                                    edge_data = G.get_edge_data(other, en).copy()\n",
    "                                else:\n",
    "                                    edge_data = G.get_edge_data(other, de).copy()\n",
    "                                \n",
    "                                # Add edge from other to merged node\n",
    "                                G.add_edge(other, merged_id, **edge_data)\n",
    "        \n",
    "        # Move sentence nodes as well based on progress\n",
    "        for sent_id, lang, text, roles in sentences:\n",
    "            node_id = f\"sentence_{sent_id}_{lang}\"\n",
    "            if node_id in G.nodes:\n",
    "                # Move English sentences left, German sentences right\n",
    "                x_target = -2.0 if lang == 'english' else 2.0\n",
    "                y_target = sentence_positions[(sent_id, lang)][1]\n",
    "                \n",
    "                current_pos = G.nodes[node_id]['position']\n",
    "                target_pos = np.array([x_target, y_target])\n",
    "                \n",
    "                # Calculate move factor based on progress\n",
    "                move_factor = phase_progress * 0.1\n",
    "                \n",
    "                # Add slight oscillation to sentence movement\n",
    "                oscillation = 0.01 * np.sin(phase_progress * 10 * np.pi) * np.array([1, 1])\n",
    "                \n",
    "                # Update position\n",
    "                G.nodes[node_id]['position'] = current_pos + (target_pos - current_pos) * move_factor + oscillation\n",
    "    \n",
    "    # Phase 4: Final stabilization and visual flourish (80-100% of frames)\n",
    "    if progress > 0.8:\n",
    "        # Enhance merged nodes\n",
    "        phase_progress = (progress - 0.8) / 0.2\n",
    "        pulse = 1.0 + 0.3 * np.sin(phase_progress * 10 * np.pi)\n",
    "        \n",
    "        # Process all merged nodes\n",
    "        for node in list(G.nodes()):\n",
    "            if node.startswith('merged_'):\n",
    "                # Pulsating size effect\n",
    "                G.nodes[node]['size'] = 500 * pulse\n",
    "                \n",
    "                # Color cycling effect\n",
    "                hue = (phase_progress * 2) % 1.0  # Cycle through hues\n",
    "                r, g, b = colorsys.hsv_to_rgb(hue, 0.7, 0.9)\n",
    "                G.nodes[node]['color'] = (r, g, b, 1.0)\n",
    "                \n",
    "                # Z-position bobbing\n",
    "                G.nodes[node]['z_position'] = 0.5 + 0.2 * np.sin(phase_progress * 15 * np.pi)\n",
    "                \n",
    "                # Add glow effect by increasing highlight\n",
    "                G.nodes[node]['highlight'] = 0.5 + 0.5 * np.sin(phase_progress * 8 * np.pi)\n",
    "                \n",
    "                # Connection energy flowing into merged node\n",
    "                for neighbor in G.neighbors(node):\n",
    "                    if G.has_edge(node, neighbor):\n",
    "                        # Increase particle generation on edges connected to merged nodes\n",
    "                        if random.random() < 0.3:\n",
    "                            G[node][neighbor]['particles'].append(random.random())\n",
    "                        \n",
    "                        # Faster particle movement\n",
    "                        new_particles = []\n",
    "                        for p in G[node][neighbor]['particles']:\n",
    "                            p += 0.06\n",
    "                            if p < 1.0:\n",
    "                                new_particles.append(p)\n",
    "                        G[node][neighbor]['particles'] = new_particles\n",
    "                        \n",
    "                        # Electric effect intensifies on these edges\n",
    "                        G[node][neighbor]['electric'] = (G[node][neighbor]['electric'] + 0.08) % 1.0\n",
    "        \n",
    "        # Add final graph-wide energy wave\n",
    "        wave_intensity = np.sin(phase_progress * 20 * np.pi) * 0.5 + 0.5\n",
    "        \n",
    "        # Add graph-wide energy pulse that ripples outward from merged nodes\n",
    "        for node in G.nodes():\n",
    "            if not node.startswith('merged_'):\n",
    "                # Skip merged source nodes\n",
    "                if G.nodes[node].get('merged', False):\n",
    "                    continue\n",
    "                \n",
    "                # Find closest merged node\n",
    "                min_dist = float('inf')\n",
    "                for merged_node in [n for n in G.nodes() if n.startswith('merged_')]:\n",
    "                    dist = np.linalg.norm(G.nodes[node]['position'] - G.nodes[merged_node]['position'])\n",
    "                    min_dist = min(min_dist, dist)\n",
    "                \n",
    "                # Apply ripple effect based on distance\n",
    "                ripple_factor = np.sin((phase_progress * 15 - min_dist * 2) * np.pi) * 0.5 + 0.5\n",
    "                G.nodes[node]['highlight'] = max(G.nodes[node].get('highlight', 0), ripple_factor * wave_intensity * 0.7)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Draw the graph with advanced visual effects\n",
    "def draw_graph(G, ax, frame, total_frames):\n",
    "    ax.clear()\n",
    "    \n",
    "    # Set up plot with dark background for visual impact\n",
    "    ax.set_facecolor('#1a1a2e')  # Dark blue background\n",
    "    fig = plt.gcf()\n",
    "    fig.set_facecolor('#1a1a2e')\n",
    "    \n",
    "    # Set up plot dimensions\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add title with glowing effect\n",
    "    percentage = int((frame / total_frames) * 100)\n",
    "    title_text = f\"Semantica: Cross-Lingual Relational Convergence - {percentage}%\"\n",
    "    title = ax.set_title(title_text, fontsize=16, color='white', fontweight='bold')\n",
    "    title.set_path_effects([path_effects.Stroke(linewidth=3, foreground='#0f52ba'),\n",
    "                           path_effects.Normal()])\n",
    "    \n",
    "    # Draw legend with enhanced styling\n",
    "    # Language Legend\n",
    "    legend_text = ax.text(-2.4, 1.3, \"Languages:\", fontsize=10, fontweight='bold', color='white')\n",
    "    legend_text.set_path_effects([path_effects.withStroke(linewidth=2, foreground='#0f52ba')])\n",
    "    \n",
    "    ax.plot(-2.3, 1.2, 'o', color=language_colors['english'], markersize=8)\n",
    "    eng_text = ax.text(-2.2, 1.2, \"English\", fontsize=9, color='white')\n",
    "    eng_text.set_path_effects([path_effects.withStroke(linewidth=1, foreground='#3498db')])\n",
    "    \n",
    "    ax.plot(-2.3, 1.1, 'o', color=language_colors['german'], markersize=8)\n",
    "    ger_text = ax.text(-2.2, 1.1, \"German\", fontsize=9, color='white')\n",
    "    ger_text.set_path_effects([path_effects.withStroke(linewidth=1, foreground='#e74c3c')])\n",
    "    \n",
    "    # Word Type Legend\n",
    "    type_text = ax.text(-2.4, 0.9, \"Word Types:\", fontsize=10, fontweight='bold', color='white')\n",
    "    type_text.set_path_effects([path_effects.withStroke(linewidth=2, foreground='#0f52ba')])\n",
    "    \n",
    "    shapes = {'fruit': 'o', 'animal': 's', 'plant': '^', 'object': 'd', 'property': 'p', 'sentence': 'h'}\n",
    "    y_pos = 0.8\n",
    "    for type_name, shape in shapes.items():\n",
    "        ax.plot(-2.3, y_pos, shape, color=type_colors[type_name], markersize=8)\n",
    "        type_label = ax.text(-2.2, y_pos, type_name, fontsize=9, color='white')\n",
    "        type_label.set_path_effects([path_effects.withStroke(linewidth=1, foreground=type_colors[type_name])])\n",
    "        y_pos -= 0.1\n",
    "    \n",
    "    # Relationship Role Legend\n",
    "    role_text = ax.text(1.8, 1.3, \"Relation Roles:\", fontsize=10, fontweight='bold', color='white')\n",
    "    role_text.set_path_effects([path_effects.withStroke(linewidth=2, foreground='#0f52ba')])\n",
    "    \n",
    "    y_pos = 1.2\n",
    "    for role, color in role_colors.items():\n",
    "        ax.plot([1.9, 2.1], [y_pos, y_pos], '-', color=color, linewidth=2)\n",
    "        role_label = ax.text(2.2, y_pos, role, fontsize=9, color='white')\n",
    "        role_label.set_path_effects([path_effects.withStroke(linewidth=1, foreground=color)])\n",
    "        y_pos -= 0.1\n",
    "    \n",
    "    # Progress indicator\n",
    "    progress = frame / total_frames\n",
    "    ax.text(0, -1.3, f\"Processing: {percentage}%\", \n",
    "           fontsize=10, ha='center', va='center', color='white',\n",
    "           bbox=dict(facecolor='#0f52ba', alpha=0.7, boxstyle=\"round,pad=0.3\"))\n",
    "    \n",
    "    # Phase indicator\n",
    "    phase_text = \"\"\n",
    "    if progress <= 0.25:\n",
    "        phase_text = \"Phase 1: Establishing Intra-Language Relations\"\n",
    "    elif progress <= 0.5:\n",
    "        phase_text = \"Phase 2: Creating Cross-Language Equivalences\"\n",
    "    elif progress <= 0.8:\n",
    "        phase_text = \"Phase 3: Semantic Convergence\"\n",
    "    else:\n",
    "        phase_text = \"Phase 4: Relational Stabilization\"\n",
    "    \n",
    "    phase_indicator = ax.text(0, -1.4, phase_text,\n",
    "                             fontsize=10, ha='center', va='center', color='white',\n",
    "                             bbox=dict(facecolor='#0f52ba', alpha=0.7, boxstyle=\"round,pad=0.3\"))\n",
    "    \n",
    "    # Draw animated background elements - subtle particle system\n",
    "    if progress > 0.5:\n",
    "        # Add more particles as we progress\n",
    "        num_particles = int(30 * (progress - 0.5) * 2)\n",
    "        for i in range(num_particles):\n",
    "            x = random.uniform(-2.5, 2.5)\n",
    "            y = random.uniform(-1.5, 1.5)\n",
    "            size = random.uniform(10, 30)\n",
    "            alpha = random.uniform(0.05, 0.2)\n",
    "            \n",
    "            # Generate color based on position (left=blue, right=red, middle=purple)\n",
    "            if x < -0.5:\n",
    "                # English side - blue hues\n",
    "                color = english_cmap(random.random())\n",
    "            elif x > 0.5:\n",
    "                # German side - red hues\n",
    "                color = german_cmap(random.random())\n",
    "            else:\n",
    "                # Convergence zone - purple hues\n",
    "                r = 0.5 + random.random() * 0.5\n",
    "                g = 0.3 + random.random() * 0.3\n",
    "                b = 0.8 + random.random() * 0.2\n",
    "                color = (r, g, b, alpha)\n",
    "            \n",
    "            # Draw particle\n",
    "            ax.scatter(x, y, s=size, color=color, alpha=alpha, edgecolors=None, zorder=1)\n",
    "    \n",
    "    # Draw edges with advanced visual effects\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Skip edges connected to merged nodes (original words that merged)\n",
    "        if (G.nodes[u].get('merged', False) and not u.startswith('merged_')) or \\\n",
    "           (G.nodes[v].get('merged', False) and not v.startswith('merged_')):\n",
    "            continue\n",
    "            \n",
    "        pos1 = G.nodes[u]['position']\n",
    "        pos2 = G.nodes[v]['position']\n",
    "        \n",
    "        # Adjust positions based on z-position\n",
    "        z1 = G.nodes[u].get('z_position', 0)\n",
    "        z2 = G.nodes[v].get('z_position', 0)\n",
    "        \n",
    "        # Simple perspective effect\n",
    "        perspective_factor = 0.1\n",
    "        pos1_adj = pos1 * (1 + z1 * perspective_factor)\n",
    "        pos2_adj = pos2 * (1 + z2 * perspective_factor)\n",
    "        \n",
    "        # Draw edge with appropriate thickness and transparency\n",
    "        weight = data.get('weight', 1.0)\n",
    "        alpha = data.get('alpha', 0.5)\n",
    "        color = data.get('color', 'gray')\n",
    "        \n",
    "        # Apply flow or electric effect\n",
    "        flow = data.get('flow', 0.0)\n",
    "        electric = data.get('electric', 0.0)\n",
    "        \n",
    "        # Draw main edge\n",
    "        if electric > 0:\n",
    "            # Electric effect for cross-language connections\n",
    "            # Create a zigzag pattern that animates\n",
    "            segment_points = 10\n",
    "            t_vals = np.linspace(0, 1, segment_points)\n",
    "            \n",
    "            # Create zigzag between the points\n",
    "            zx = []\n",
    "            zy = []\n",
    "            \n",
    "            for i, t in enumerate(t_vals):\n",
    "                # Base point along straight line\n",
    "                x = pos1_adj[0] + t * (pos2_adj[0] - pos1_adj[0])\n",
    "                y = pos1_adj[1] + t * (pos2_adj[1] - pos1_adj[1])\n",
    "                \n",
    "                # Add zigzag displacement\n",
    "                if i > 0 and i < segment_points - 1:\n",
    "                    # Calculate perpendicular direction\n",
    "                    dx = pos2_adj[0] - pos1_adj[0]\n",
    "                    dy = pos2_adj[1] - pos1_adj[1]\n",
    "                    length = np.sqrt(dx*dx + dy*dy)\n",
    "                    \n",
    "                    # Perpendicular unit vector\n",
    "                    if length > 0:\n",
    "                        px = -dy / length\n",
    "                        py = dx / length\n",
    "                        \n",
    "                        # Calculate zigzag displacement\n",
    "                        zigzag_amplitude = 0.05 * weight\n",
    "                        zigzag = zigzag_amplitude * np.sin((i + electric * 20) * np.pi)\n",
    "                        \n",
    "                        # Apply displacement\n",
    "                        x += px * zigzag\n",
    "                        y += py * zigzag\n",
    "                \n",
    "                zx.append(x)\n",
    "                zy.append(y)\n",
    "            \n",
    "            # Draw the zigzag path\n",
    "            ax.plot(zx, zy, '-', color=color, linewidth=weight, alpha=alpha, zorder=2)\n",
    "            \n",
    "            # Add glowing effect\n",
    "            glow = ax.plot(zx, zy, '-', color='white', linewidth=weight+1, alpha=alpha*0.3, zorder=1)\n",
    "        else:\n",
    "            # Regular flow effect for standard edges\n",
    "            arrow = FancyArrowPatch(\n",
    "                posA=pos1_adj, posB=pos2_adj,\n",
    "                arrowstyle='-',\n",
    "                color=color,\n",
    "                linewidth=weight,\n",
    "                alpha=alpha,\n",
    "                mutation_scale=10,\n",
    "                zorder=2\n",
    "            )\n",
    "            ax.add_patch(arrow)\n",
    "        \n",
    "        # Draw particles along the edge for animation\n",
    "        for p in data.get('particles', []):\n",
    "            # Interpolate position along the edge\n",
    "            px = pos1_adj[0] + p * (pos2_adj[0] - pos1_adj[0])\n",
    "            py = pos1_adj[1] + p * (pos2_adj[1] - pos1_adj[1])\n",
    "            \n",
    "            # Draw particle\n",
    "            ax.scatter(px, py, s=20*weight, color='white', alpha=0.8, zorder=3)\n",
    "        \n",
    "        # Add role label if it's a sentence-word edge\n",
    "        if (u.startswith('sentence_') or v.startswith('sentence_')) and 'role' in data:\n",
    "            # Calculate midpoint\n",
    "            mid_x = (pos1_adj[0] + pos2_adj[0]) / 2\n",
    "            mid_y = (pos1_adj[1] + pos2_adj[1]) / 2\n",
    "            \n",
    "            # Add small offset to avoid overlap\n",
    "            offset = 0.05\n",
    "            label_pos = (mid_x + offset, mid_y + offset)\n",
    "            \n",
    "            # Add role label\n",
    "            if alpha > 0.7:  # Only show labels for strong edges\n",
    "                role_label = ax.text(label_pos[0], label_pos[1], data['role'], fontsize=7, \n",
    "                       ha='center', va='center', color='white',\n",
    "                       bbox=dict(facecolor=color, alpha=0.7, edgecolor='none', pad=1))\n",
    "                role_label.set_path_effects([path_effects.withStroke(linewidth=1, foreground='white')])\n",
    "    \n",
    "    # Draw sentence nodes first (so they are in back)\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if node.startswith('sentence_') and not data.get('merged', False):\n",
    "            pos = data['position']\n",
    "            highlight = data.get('highlight', 0.0)\n",
    "            \n",
    "            # Draw the node as a rounded rectangle with glow effect\n",
    "            rect = plt.Rectangle(\n",
    "                (pos[0] - 0.3, pos[1] - 0.1),\n",
    "                0.6, 0.2,\n",
    "                facecolor=data['color'],\n",
    "                alpha=0.7 + highlight * 0.3,  # Increase alpha with highlight\n",
    "                edgecolor='white',\n",
    "                linewidth=1 + highlight,  # Thicker border when highlighted\n",
    "                zorder=1\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add glow effect when highlighted\n",
    "            if highlight > 0.3:\n",
    "                glow = plt.Rectangle(\n",
    "                    (pos[0] - 0.33, pos[1] - 0.13),\n",
    "                    0.66, 0.26,\n",
    "                    facecolor='white',\n",
    "                    alpha=highlight * 0.3,\n",
    "                    edgecolor=None,\n",
    "                    zorder=0\n",
    "                )\n",
    "                ax.add_patch(glow)\n",
    "            \n",
    "            # Add the sentence text\n",
    "            lang = data['lang']\n",
    "            text = data['text']\n",
    "            text_color = 'white'\n",
    "            \n",
    "            sentence_text = ax.text(pos[0], pos[1], text, fontsize=7, ha='center', va='center', \n",
    "                   color=text_color, zorder=2,\n",
    "                   bbox=dict(facecolor=data['color'], alpha=0.0, pad=1))\n",
    "            \n",
    "            # Add subtle glow to text when highlighted\n",
    "            if highlight > 0.3:\n",
    "                sentence_text.set_path_effects([path_effects.withStroke(linewidth=1+highlight, foreground='white')])\n",
    "    \n",
    "    # Draw word nodes with advanced styles\n",
    "    for node, data in G.nodes(data=True):\n",
    "        # Skip rendering merged original nodes\n",
    "        if data.get('merged', False) and not node.startswith('merged_'):\n",
    "            continue\n",
    "            \n",
    "        # Skip sentence nodes (already drawn)\n",
    "        if node.startswith('sentence_'):\n",
    "            continue\n",
    "            \n",
    "        pos = data['position']\n",
    "        highlight = data.get('highlight', 0.0)\n",
    "        z_pos = data.get('z_position', 0.0)\n",
    "        \n",
    "        # Apply perspective scaling based on z-position\n",
    "        size_scale = 1 + z_pos * 0.5\n",
    "        pos_scale = 1 + z_pos * 0.1\n",
    "        \n",
    "        # Adjust position with perspective\n",
    "        pos_adj = pos * pos_scale\n",
    "        \n",
    "        # For merged nodes, draw the emoji with special effects\n",
    "        if node.startswith('merged_'):\n",
    "            emoji = data['emoji']\n",
    "            size = data['size'] * size_scale\n",
    "            \n",
    "            # Draw a pulsating glow effect\n",
    "            glow_radius = 0.2 * (1 + highlight * 0.5)\n",
    "            glow = plt.Circle(\n",
    "                pos_adj, glow_radius,\n",
    "                facecolor=data['color'],\n",
    "                alpha=0.4 + highlight * 0.3,\n",
    "                edgecolor=None,\n",
    "                zorder=3\n",
    "            )\n",
    "            ax.add_patch(glow)\n",
    "            \n",
    "            # Draw energy rays emanating from merged node\n",
    "            if highlight > 0.3:\n",
    "                ray_count = 12\n",
    "                max_length = 0.3 * highlight\n",
    "                for i in range(ray_count):\n",
    "                    angle = (i / ray_count) * 2 * np.pi\n",
    "                    # Randomize ray length\n",
    "                    ray_length = max_length * (0.5 + 0.5 * random.random())\n",
    "                    end_x = pos_adj[0] + np.cos(angle) * ray_length\n",
    "                    end_y = pos_adj[1] + np.sin(angle) * ray_length\n",
    "                    \n",
    "                    # Draw ray\n",
    "                    ray = ax.plot([pos_adj[0], end_x], [pos_adj[1], end_y], '-', \n",
    "                                 color=data['color'], alpha=highlight*0.5, linewidth=1, zorder=4)\n",
    "            \n",
    "            # Draw the emoji with shadow effect\n",
    "            emoji_text = ax.text(pos_adj[0], pos_adj[1], emoji, \n",
    "                               fontsize=24 * size_scale, \n",
    "                               ha='center', va='center', zorder=5)\n",
    "            \n",
    "            # Create shadow/glow effect\n",
    "            emoji_text.set_path_effects([\n",
    "                path_effects.withStroke(linewidth=5, foreground='black', alpha=0.3),\n",
    "                path_effects.Normal()\n",
    "            ])\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # Regular word nodes with custom stylized shapes\n",
    "        node_type = data['type']\n",
    "        color = data['color']\n",
    "        alpha = data.get('alpha', 1.0)\n",
    "        size = data.get('size', 300) * size_scale\n",
    "        rotation = data.get('rotation', 0)\n",
    "        \n",
    "        # Calculate color variation based on highlighting\n",
    "        if highlight > 0:\n",
    "            # Create color gradient from base color to white based on highlight intensity\n",
    "            base_rgb = to_rgba(color)\n",
    "            r = min(1.0, base_rgb[0] + highlight * 0.5)\n",
    "            g = min(1.0, base_rgb[1] + highlight * 0.5)\n",
    "            b = min(1.0, base_rgb[2] + highlight * 0.5)\n",
    "            enhanced_color = (r, g, b, base_rgb[3])\n",
    "        else:\n",
    "            enhanced_color = color\n",
    "        \n",
    "        if size > 0:\n",
    "            # Draw custom node shape\n",
    "            x, y = custom_node_shape(node_type, size=0.1*size_scale)\n",
    "            \n",
    "            # Apply rotation if needed\n",
    "            if rotation != 0:\n",
    "                theta = np.radians(rotation)\n",
    "                x_rot = x * np.cos(theta) - y * np.sin(theta)\n",
    "                y_rot = x * np.sin(theta) + y * np.cos(theta)\n",
    "                x, y = x_rot, y_rot\n",
    "            \n",
    "            # Create polygon for the node shape\n",
    "            verts = np.column_stack([x + pos_adj[0], y + pos_adj[1]])\n",
    "            node_poly = Polygon(verts, closed=True, facecolor=enhanced_color, \n",
    "                              alpha=alpha, edgecolor='white', linewidth=1 + highlight, zorder=4)\n",
    "            ax.add_patch(node_poly)\n",
    "            \n",
    "            # Add glow effect when highlighted\n",
    "            if highlight > 0.3:\n",
    "                # Scale up the shape slightly for glow\n",
    "                glow_scale = 1.2\n",
    "                x_glow = x * glow_scale\n",
    "                y_glow = y * glow_scale\n",
    "                \n",
    "                # Apply same rotation if needed\n",
    "                if rotation != 0:\n",
    "                    theta = np.radians(rotation)\n",
    "                    x_rot = x_glow * np.cos(theta) - y_glow * np.sin(theta)\n",
    "                    y_rot = x_glow * np.sin(theta) + y_glow * np.cos(theta)\n",
    "                    x_glow, y_glow = x_rot, y_rot\n",
    "                \n",
    "                # Create glow polygon\n",
    "                glow_verts = np.column_stack([x_glow + pos_adj[0], y_glow + pos_adj[1]])\n",
    "                glow_poly = Polygon(glow_verts, closed=True, facecolor='white', \n",
    "                                  alpha=highlight * 0.3, edgecolor=None, zorder=3)\n",
    "                ax.add_patch(glow_poly)\n",
    "            \n",
    "            # Label the node with enhanced text effect\n",
    "            label_color = 'white'\n",
    "            node_label = ax.text(pos_adj[0], pos_adj[1], node, fontsize=9*size_scale, \n",
    "                               ha='center', va='center', color=label_color, \n",
    "                               fontweight='bold', zorder=5)\n",
    "            \n",
    "            # Add text shadow effect\n",
    "            node_label.set_path_effects([\n",
    "                path_effects.withStroke(linewidth=3, foreground='black', alpha=0.5),\n",
    "                path_effects.Normal()\n",
    "            ])\n",
    "\n",
    "# Create the animation function with enhanced post-processing\n",
    "def create_semantica_animation():\n",
    "    # Set up the output path\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_file = os.path.join(output_data_path, f\"semantica_advanced_{timestamp}.gif\")\n",
    "    \n",
    "    print(f\"Animation will be saved to: {output_file}\")\n",
    "    print(\"Initializing visualization...\")\n",
    "    \n",
    "    # Create figure with higher quality\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(14, 12), dpi=120)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Create initial graph\n",
    "    G, sentence_positions = create_initial_graph()\n",
    "    \n",
    "    # Set up animation\n",
    "    total_frames = 150  # Increased for smoother animation\n",
    "    \n",
    "    # Create list to store frames for GIF\n",
    "    frames = []\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    for frame in tqdm(range(total_frames + 1), desc=\"Generating visualization frames\"):\n",
    "        # Update graph based on frame\n",
    "        G = update_graph(G, sentence_positions, frame, total_frames)\n",
    "        \n",
    "        # Draw the current state\n",
    "        draw_graph(G, ax, frame, total_frames)\n",
    "        \n",
    "        # Add title watermark\n",
    "        plt.figtext(0.5, 0.01, \"Semantica: Theory of Relational Semantic Convergence\", \n",
    "                   ha='center', color='white', alpha=0.3, fontsize=8)\n",
    "        \n",
    "        # Capture the frame\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', dpi=120, facecolor='#1a1a2e')\n",
    "        buf.seek(0)\n",
    "        img = Image.open(buf)\n",
    "        \n",
    "        # Apply post-processing for enhanced visual quality\n",
    "        # Add subtle bloom effect\n",
    "        bloom = img.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "        bloom = ImageEnhance.Brightness(bloom).enhance(1.5)\n",
    "        \n",
    "        # Composite the original image with the bloom\n",
    "        result = Image.composite(bloom, img, ImageEnhance.Brightness(img).enhance(0.7))\n",
    "        \n",
    "        # Apply color correction\n",
    "        result = ImageEnhance.Contrast(result).enhance(1.2)\n",
    "        result = ImageEnhance.Color(result).enhance(1.3)\n",
    "        \n",
    "        frames.append(result)\n",
    "        \n",
    "        # Print progress\n",
    "        if frame % 30 == 0 and frame > 0:\n",
    "            print(f\"Processed {frame}/{total_frames} frames ({frame/total_frames*100:.1f}%)\")\n",
    "    \n",
    "    # Save the gif with enhanced settings\n",
    "    print(f\"Saving animation to {output_file}...\")\n",
    "    frames[0].save(\n",
    "        output_file,\n",
    "        format='GIF',\n",
    "        append_images=frames[1:],\n",
    "        save_all=True,\n",
    "        duration=80,  # Faster frame rate for smoother animation\n",
    "        loop=0,       # Loop forever\n",
    "        optimize=False  # Higher quality\n",
    "    )\n",
    "    print(\"Animation saved!\")\n",
    "    \n",
    "    plt.close()\n",
    "    return output_file\n",
    "\n",
    "# Execute the function to create the animation\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Semantica advanced visualization with enhanced visuals\")\n",
    "    print(\"This may take several minutes to process...\")\n",
    "    gif_path = create_semantica_animation()\n",
    "    print(f\"‚ú® Visualization masterpiece created at: {gif_path}\")\n",
    "    print(\"Process complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c761f58",
   "metadata": {},
   "source": [
    "# Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import uuid\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "class SemanticaPrimitive:\n",
    "    \"\"\"\n",
    "    Represents a semantic primitive with rich contextual metadata\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 value: str, \n",
    "                 primitive_type: str = 'generic', \n",
    "                 language: str = 'universal',\n",
    "                 embedding: np.ndarray = None):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.value = value\n",
    "        self.type = primitive_type\n",
    "        self.language = language\n",
    "        self.embedding = embedding if embedding is not None else np.random.rand(50)\n",
    "        self.metadata = {}\n",
    "        self.semantic_energy = 0.5  # New attribute to represent semantic potency\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Primitive(value={self.value}, type={self.type}, lang={self.language})\"\n",
    "\n",
    "@dataclass\n",
    "class SemanticRelation:\n",
    "    \"\"\"\n",
    "    Represents a relation between primitives with validation and confidence\n",
    "    \"\"\"\n",
    "    source: SemanticaPrimitive\n",
    "    target: SemanticaPrimitive\n",
    "    relation_type: str\n",
    "    confidence: float = 0.0\n",
    "    agent_validations: List[str] = field(default_factory=list)\n",
    "    semantic_intensity: float = 0.0\n",
    "    \n",
    "    def validate(self, agent_id: str):\n",
    "        \"\"\"Track agent validations and update confidence\"\"\"\n",
    "        if agent_id not in self.agent_validations:\n",
    "            self.agent_validations.append(agent_id)\n",
    "            self.confidence = min(1.0, self.confidence + 0.1)\n",
    "            self.semantic_intensity = len(self.agent_validations) * 0.2\n",
    "\n",
    "class SemanticAgent:\n",
    "    \"\"\"\n",
    "    An agent that can propose, validate, and modify semantic relations\n",
    "    \"\"\"\n",
    "    def __init__(self, agent_id: str, expertise: Dict[str, float] = None):\n",
    "        self.id = agent_id\n",
    "        self.expertise = expertise or {}\n",
    "        self.memory_graph = nx.DiGraph()\n",
    "\n",
    "    def propose_relation(self, \n",
    "                         source: SemanticaPrimitive, \n",
    "                         target: SemanticaPrimitive, \n",
    "                         relation_type: str) -> SemanticRelation:\n",
    "        \"\"\"Propose a semantic relation based on agent's expertise\"\"\"\n",
    "        confidence = self.expertise.get(relation_type, 0.5)\n",
    "        relation = SemanticRelation(source, target, relation_type, confidence)\n",
    "        relation.validate(self.id)\n",
    "        \n",
    "        # Boost semantic energy of primitives\n",
    "        source.semantic_energy += 0.1\n",
    "        target.semantic_energy += 0.1\n",
    "        \n",
    "        return relation\n",
    "\n",
    "class SemanticaGraph:\n",
    "    \"\"\"\n",
    "    Central semantic graph that manages primitives, relations, and convergence\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: str = None):\n",
    "        self.primitives = {}\n",
    "        self.relations = []\n",
    "        self.agents = {}\n",
    "        self.convergence_graph = nx.DiGraph()\n",
    "        self.output_dir = output_dir or os.path.join(os.getcwd(), 'output')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def add_primitive(self, primitive: SemanticaPrimitive):\n",
    "        \"\"\"Add a new primitive to the semantic space\"\"\"\n",
    "        self.primitives[primitive.id] = primitive\n",
    "        return primitive\n",
    "\n",
    "    def add_agent(self, agent: SemanticAgent):\n",
    "        \"\"\"Register a new semantic agent\"\"\"\n",
    "        self.agents[agent.id] = agent\n",
    "\n",
    "    def propose_relation(self, \n",
    "                         source_primitive: SemanticaPrimitive, \n",
    "                         target_primitive: SemanticaPrimitive, \n",
    "                         relation_type: str,\n",
    "                         proposing_agent: SemanticAgent):\n",
    "        \"\"\"Propose and potentially validate a semantic relation\"\"\"\n",
    "        relation = proposing_agent.propose_relation(\n",
    "            source_primitive, \n",
    "            target_primitive, \n",
    "            relation_type\n",
    "        )\n",
    "        \n",
    "        # Cross-validate with other agents\n",
    "        for agent in self.agents.values():\n",
    "            if agent.id != proposing_agent.id:\n",
    "                # More nuanced validation logic\n",
    "                validation_prob = 0.7 * (1 + agent.expertise.get(relation_type, 0.5))\n",
    "                if random.random() < validation_prob:\n",
    "                    relation.validate(agent.id)\n",
    "        \n",
    "        self.relations.append(relation)\n",
    "        \n",
    "        # Update convergence graph\n",
    "        self.convergence_graph.add_edge(\n",
    "            source_primitive.id, \n",
    "            target_primitive.id, \n",
    "            relation_type=relation_type,\n",
    "            confidence=relation.confidence,\n",
    "            semantic_intensity=relation.semantic_intensity\n",
    "        )\n",
    "\n",
    "    def animate_convergence(self, num_frames: int = 100):\n",
    "        \"\"\"\n",
    "        Create an animated visualization of semantic convergence\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(15, 10), facecolor='black')\n",
    "        plt.style.use('dark_background')\n",
    "\n",
    "        def update(frame):\n",
    "            ax.clear()\n",
    "            ax.set_facecolor('black')\n",
    "            plt.title(\"Semantic Convergence Dynamics\", color='white', fontsize=16)\n",
    "            \n",
    "            # Periodically add new relations or validate existing ones\n",
    "            if frame % 10 == 0:\n",
    "                # Simulate new relation proposals or validations\n",
    "                for _ in range(2):\n",
    "                    source = random.choice(list(self.primitives.values()))\n",
    "                    target = random.choice(list(self.primitives.values()))\n",
    "                    agent = random.choice(list(self.agents.values()))\n",
    "                    self.propose_relation(source, target, 'exploration', agent)\n",
    "            \n",
    "            # Prepare graph layout\n",
    "            pos = nx.spring_layout(self.convergence_graph, k=0.5, iterations=50)\n",
    "            \n",
    "            # Node colors and sizes based on semantic energy\n",
    "            node_colors = [self.primitives[node].semantic_energy for node in self.convergence_graph.nodes()]\n",
    "            node_sizes = [100 + self.primitives[node].semantic_energy * 500 for node in self.convergence_graph.nodes()]\n",
    "            \n",
    "            # Edge weights and colors based on relation confidence\n",
    "            edge_weights = [\n",
    "                self.convergence_graph[u][v].get('confidence', 0.1) * 5 \n",
    "                for (u, v) in self.convergence_graph.edges()\n",
    "            ]\n",
    "            edge_colors = [\n",
    "                plt.cm.plasma(self.convergence_graph[u][v].get('confidence', 0.1)) \n",
    "                for (u, v) in self.convergence_graph.edges()\n",
    "            ]\n",
    "            \n",
    "            # Draw nodes\n",
    "            nx.draw_networkx_nodes(\n",
    "                self.convergence_graph, \n",
    "                pos, \n",
    "                node_color=node_colors, \n",
    "                node_size=node_sizes,\n",
    "                cmap=plt.cm.viridis,\n",
    "                alpha=0.8\n",
    "            )\n",
    "            \n",
    "            # Draw edges\n",
    "            nx.draw_networkx_edges(\n",
    "                self.convergence_graph, \n",
    "                pos, \n",
    "                width=edge_weights,\n",
    "                edge_color=edge_colors,\n",
    "                alpha=0.6,\n",
    "                arrows=True,\n",
    "                connectionstyle='arc3,rad=0.1'\n",
    "            )\n",
    "            \n",
    "            # Draw labels\n",
    "            nx.draw_networkx_labels(\n",
    "                self.convergence_graph, \n",
    "                pos, \n",
    "                labels={node: self.primitives[node].value for node in self.convergence_graph.nodes()},\n",
    "                font_size=8,\n",
    "                font_color='white'\n",
    "            )\n",
    "            \n",
    "            # Add frame number and global semantic energy\n",
    "            total_semantic_energy = sum(p.semantic_energy for p in self.primitives.values())\n",
    "            ax.text(0.02, 0.98, f\"Frame: {frame}\", transform=ax.transAxes, color='white', fontsize=10, verticalalignment='top')\n",
    "            ax.text(0.02, 0.93, f\"Total Semantic Energy: {total_semantic_energy:.2f}\", transform=ax.transAxes, color='white', fontsize=10, verticalalignment='top')\n",
    "            \n",
    "            plt.axis('off')\n",
    "\n",
    "        # Create animation\n",
    "        anim = animation.FuncAnimation(fig, update, frames=num_frames, interval=200)\n",
    "        \n",
    "        # Save the animation\n",
    "        output_path = os.path.join(self.output_dir, f'semantic_convergence_{uuid.uuid4()}.gif')\n",
    "        anim.save(output_path, writer='pillow', fps=5)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f\"Animation saved to {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "# Demonstration of Semantica principles\n",
    "def semantic_convergence_demo(output_dir: str = None):\n",
    "    # Initialize Semantica Graph\n",
    "    semantica = SemanticaGraph(output_dir)\n",
    "\n",
    "    # Create some initial primitives with richer context\n",
    "    primitives = [\n",
    "        SemanticaPrimitive(\"apple\", primitive_type=\"fruit\", language=\"english\"),\n",
    "        SemanticaPrimitive(\"Apfel\", primitive_type=\"fruit\", language=\"german\"),\n",
    "        SemanticaPrimitive(\"red\", primitive_type=\"color\", language=\"english\"),\n",
    "        SemanticaPrimitive(\"rot\", primitive_type=\"color\", language=\"german\"),\n",
    "        SemanticaPrimitive(\"sweet\", primitive_type=\"taste\", language=\"english\"),\n",
    "        SemanticaPrimitive(\"s√º√ü\", primitive_type=\"taste\", language=\"german\")\n",
    "    ]\n",
    "\n",
    "    # Add primitives to the graph\n",
    "    for p in primitives:\n",
    "        semantica.add_primitive(p)\n",
    "\n",
    "    # Create agents with different expertise\n",
    "    agents = [\n",
    "        SemanticAgent(\"linguist1\", {\"translation\": 0.9, \"similarity\": 0.7, \"exploration\": 0.6}),\n",
    "        SemanticAgent(\"linguist2\", {\"translation\": 0.8, \"similarity\": 0.6, \"exploration\": 0.5}),\n",
    "        SemanticAgent(\"translator\", {\"translation\": 1.0, \"similarity\": 0.9, \"exploration\": 0.8})\n",
    "    ]\n",
    "\n",
    "    # Add agents to the graph\n",
    "    for agent in agents:\n",
    "        semantica.add_agent(agent)\n",
    "\n",
    "    # Propose some initial relations\n",
    "    semantica.propose_relation(\n",
    "        primitives[0],  # apple\n",
    "        primitives[1],  # Apfel\n",
    "        \"translation\",\n",
    "        agents[0]\n",
    "    )\n",
    "\n",
    "    semantica.propose_relation(\n",
    "        primitives[2],  # red\n",
    "        primitives[3],  # rot\n",
    "        \"color_mapping\",\n",
    "        agents[2]\n",
    "    )\n",
    "\n",
    "    # Create an animated visualization\n",
    "    output_path = semantica.animate_convergence(num_frames=150)\n",
    "\n",
    "    return semantica, output_path\n",
    "\n",
    "# Run the demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the output directory\n",
    "    output_dir = r\"C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Output\"\n",
    "    \n",
    "    # Run the demo\n",
    "    demo_graph, animation_path = semantic_convergence_demo(output_dir)\n",
    "    print(f\"Animation created at: {animation_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095829b5",
   "metadata": {},
   "source": [
    "# Phase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import uuid\n",
    "import os\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "from tqdm import tqdm  # Adding tqdm for visual progress tracking\n",
    "\n",
    "class SemanticField:\n",
    "    \"\"\"\n",
    "    Advanced semantic field with robust visualization and enhanced aesthetics\n",
    "    \"\"\"\n",
    "    def __init__(self, num_dimensions=5):\n",
    "        print(\"Initializing SemanticField with\", num_dimensions, \"dimensions...\")\n",
    "        # Higher dimensional semantic space\n",
    "        self.dimensions = num_dimensions\n",
    "        \n",
    "        # Initialize primitives with more robust vector generation\n",
    "        self.primitives = {}\n",
    "        self._initialize_primitives()\n",
    "        \n",
    "        # Interaction history and semantic memory\n",
    "        self.interaction_history = []\n",
    "        self.semantic_memory = nx.DiGraph()\n",
    "        print(\"Semantic field initialized successfully!\")\n",
    "    \n",
    "    def _initialize_primitives(self):\n",
    "        \"\"\"\n",
    "        Robust primitive initialization with controlled vector generation for better visualization\n",
    "        \"\"\"\n",
    "        print(\"Initializing semantic primitives...\")\n",
    "        primitive_definitions = [\n",
    "            ('apple', 'fruit', 'physical', 'english'),\n",
    "            ('Apfel', 'fruit', 'physical', 'german'),\n",
    "            ('red', 'color', 'perceptual', 'english'),\n",
    "            ('rot', 'color', 'perceptual', 'german'),\n",
    "            ('sweet', 'taste', 'sensory', 'english'),\n",
    "            ('s√º√ü', 'taste', 'sensory', 'german'),\n",
    "            ('car', 'vehicle', 'physical', 'english'),\n",
    "            ('blue', 'color', 'perceptual', 'english'),\n",
    "            ('sour', 'taste', 'sensory', 'english'),\n",
    "            ('hot', 'temperature', 'sensory', 'english'),\n",
    "            ('cold', 'temperature', 'sensory', 'english'),\n",
    "            ('large', 'size', 'perceptual', 'english'),\n",
    "            ('small', 'size', 'perceptual', 'english')\n",
    "        ]\n",
    "        \n",
    "        # First, create deterministic base vectors for each semantic type\n",
    "        # This ensures similar types will be close together\n",
    "        type_base_vectors = {}\n",
    "        domain_base_vectors = {}\n",
    "        language_base_vectors = {}\n",
    "        \n",
    "        # Create base vectors for types\n",
    "        types = list(set(t[1] for t in primitive_definitions))\n",
    "        for i, t in enumerate(types):\n",
    "            np.random.seed(hash(t) % 10000)\n",
    "            type_base_vectors[t] = np.random.randn(self.dimensions)\n",
    "            # Normalize to unit sphere\n",
    "            type_base_vectors[t] = type_base_vectors[t] / np.linalg.norm(type_base_vectors[t])\n",
    "        \n",
    "        # Create base vectors for domains\n",
    "        domains = list(set(t[2] for t in primitive_definitions))\n",
    "        for i, d in enumerate(domains):\n",
    "            np.random.seed(hash(d) % 10000)\n",
    "            domain_base_vectors[d] = np.random.randn(self.dimensions) * 0.5\n",
    "            domain_base_vectors[d] = domain_base_vectors[d] / np.linalg.norm(domain_base_vectors[d])\n",
    "        \n",
    "        # Create base vectors for languages\n",
    "        languages = list(set(t[3] for t in primitive_definitions))\n",
    "        for i, l in enumerate(languages):\n",
    "            np.random.seed(hash(l) % 10000)\n",
    "            language_base_vectors[l] = np.random.randn(self.dimensions) * 0.3\n",
    "            language_base_vectors[l] = language_base_vectors[l] / np.linalg.norm(language_base_vectors[l])\n",
    "        \n",
    "        # Now create primitives with these controlled base vectors\n",
    "        for name, sem_type, domain, language in tqdm(primitive_definitions, desc=\"Creating primitives\"):\n",
    "            try:\n",
    "                # Start with the type vector\n",
    "                base_vector = type_base_vectors[sem_type].copy() * 2.0\n",
    "                \n",
    "                # Add slight influence from domain and language\n",
    "                base_vector += domain_base_vectors[domain] * 1.0\n",
    "                base_vector += language_base_vectors[language] * 0.5\n",
    "                \n",
    "                # Add a small random variation for uniqueness\n",
    "                np.random.seed(hash(name) % 10000)\n",
    "                random_variation = np.random.randn(self.dimensions) * 0.2\n",
    "                base_vector += random_variation\n",
    "                \n",
    "                # Ensure similar terms in different languages are close\n",
    "                # E.g., 'apple' and 'Apfel' should be closer\n",
    "                translation_pairs = [\n",
    "                    ('apple', 'Apfel'),\n",
    "                    ('red', 'rot'),\n",
    "                    ('sweet', 's√º√ü')\n",
    "                ]\n",
    "                \n",
    "                for term1, term2 in translation_pairs:\n",
    "                    if name == term1 or name == term2:\n",
    "                        # Find the other term\n",
    "                        other_term = term2 if name == term1 else term1\n",
    "                        \n",
    "                        # Check if other term already exists\n",
    "                        if other_term in self.primitives:\n",
    "                            # Pull vectors closer together\n",
    "                            other_vector = self.primitives[other_term]['vector']\n",
    "                            base_vector = (base_vector * 0.7) + (other_vector * 0.3)\n",
    "                \n",
    "                # Normalize final vector \n",
    "                base_vector = base_vector / np.linalg.norm(base_vector) * 1.5\n",
    "                \n",
    "                self.primitives[name] = {\n",
    "                    'vector': base_vector,\n",
    "                    'type': sem_type,\n",
    "                    'domain': domain,\n",
    "                    'language': language,\n",
    "                    'potential': np.random.uniform(0.7, 1.3)  # Varied potential\n",
    "                }\n",
    "                print(f\"Created primitive: {name} ({sem_type})\")\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Could not create primitive {name}: {e}\")\n",
    "                # Fallback to a default vector\n",
    "                self.primitives[name] = {\n",
    "                    'vector': np.random.randn(self.dimensions),\n",
    "                    'type': sem_type,\n",
    "                    'domain': domain,\n",
    "                    'language': language,\n",
    "                    'potential': 1.0\n",
    "                }\n",
    "    \n",
    "    def compute_all_semantic_distances(self):\n",
    "        \"\"\"\n",
    "        Compute semantic distances between all primitives\n",
    "        \"\"\"\n",
    "        distances = {}\n",
    "        primitives = list(self.primitives.keys())\n",
    "        \n",
    "        for i, name1 in enumerate(primitives):\n",
    "            for j, name2 in enumerate(primitives):\n",
    "                if i < j:  # Only compute each pair once\n",
    "                    vec1 = self.primitives[name1]['vector']\n",
    "                    vec2 = self.primitives[name2]['vector']\n",
    "                    \n",
    "                    try:\n",
    "                        # Cosine distance - lower means more similar\n",
    "                        distance = cosine(vec1, vec2)\n",
    "                        distances[(name1, name2)] = distance\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(f\"Could not compute distance between {name1} and {name2}: {e}\")\n",
    "                        distances[(name1, name2)] = 0.5  # Default value\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def visualize_semantic_dynamics(self, \n",
    "                                    output_dir, \n",
    "                                    num_frames=150, \n",
    "                                    interaction_rounds=5,\n",
    "                                    make_animation=True):\n",
    "        \"\"\"\n",
    "        Create a visually appealing visualization of semantic convergence\n",
    "        \"\"\"\n",
    "        print(f\"Generating visualization with {num_frames} frames...\")\n",
    "        \n",
    "        # Ensure we have primitives\n",
    "        if not self.primitives:\n",
    "            print(\"No primitives found, initializing...\")\n",
    "            self._initialize_primitives()\n",
    "        \n",
    "        # Compute all semantic distances\n",
    "        print(\"Computing semantic distances...\")\n",
    "        all_distances = self.compute_all_semantic_distances()\n",
    "        \n",
    "        # Find important connections (closest relationships)\n",
    "        important_connections = []\n",
    "        for (name1, name2), distance in sorted(all_distances.items(), key=lambda x: x[1]):\n",
    "            # Only consider close relationships \n",
    "            if distance < 0.7:\n",
    "                important_connections.append((name1, name2, distance))\n",
    "        \n",
    "        # Create figure with elegant dark theme\n",
    "        plt.style.use('dark_background')\n",
    "        fig = plt.figure(figsize=(16, 12), facecolor='#0A0A1E')\n",
    "        ax = fig.add_subplot(111, projection='3d', facecolor='#0A0A1E')\n",
    "        \n",
    "        # Remove panes to reduce visual clutter\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        \n",
    "        # Make grid semi-transparent\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.xaxis.pane.set_alpha(0.1)\n",
    "        ax.yaxis.pane.set_alpha(0.1)\n",
    "        ax.zaxis.pane.set_alpha(0.1)\n",
    "        \n",
    "        # Set title with elegant typography\n",
    "        fig.suptitle('Semantica: Semantic Convergence', \n",
    "                     color='white', fontsize=24, fontweight='light', y=0.98)\n",
    "        \n",
    "        # More vibrant but still harmonious colors\n",
    "        type_colors = {\n",
    "            'fruit': '#e74c3c',      # Red\n",
    "            'color': '#3498db',      # Blue\n",
    "            'taste': '#f39c12',      # Orange\n",
    "            'vehicle': '#2ecc71',    # Green\n",
    "            'temperature': '#9b59b6', # Purple\n",
    "            'size': '#1abc9c',       # Teal\n",
    "            'generic': '#95a5a6'     # Gray\n",
    "        }\n",
    "        \n",
    "        # Pre-calculate some interaction dynamics\n",
    "        interaction_frames = {}\n",
    "        \n",
    "        print(\"Pre-calculating semantic interactions...\")\n",
    "        for frame in tqdm(range(num_frames), desc=\"Generating frame data\"):\n",
    "            # Calculate frame-specific interactions and positions\n",
    "            interactions = {}\n",
    "            \n",
    "            # Every few frames, simulate semantic interactions\n",
    "            if frame % (num_frames // (interaction_rounds * 2)) == 0:\n",
    "                # Focus on important connections\n",
    "                for name1, name2, distance in important_connections[:min(5, len(important_connections))]:\n",
    "                    interactions[(name1, name2)] = distance\n",
    "            \n",
    "            interaction_frames[frame] = interactions\n",
    "        \n",
    "        # Function to calculate dynamic positions based on frame\n",
    "        def calculate_positions(frame):\n",
    "            positions = {}\n",
    "            \n",
    "            for name, data in self.primitives.items():\n",
    "                # Get base vector\n",
    "                base_vector = data['vector'][:3]  # First 3 dimensions\n",
    "                \n",
    "                # Add subtle oscillation based on frame - much more subtle than before\n",
    "                oscillation = np.array([\n",
    "                    0.02 * np.sin(frame * 0.01 + hash(name) % 10),\n",
    "                    0.02 * np.cos(frame * 0.008 + hash(name + \"y\") % 10),\n",
    "                    0.02 * np.sin(frame * 0.005 + hash(name + \"z\") % 10)\n",
    "                ])\n",
    "                \n",
    "                # Scale oscillation based on semantic potential\n",
    "                oscillation *= data['potential'] * 0.5\n",
    "                \n",
    "                # Calculate position\n",
    "                position = base_vector + oscillation\n",
    "                \n",
    "                positions[name] = position\n",
    "            \n",
    "            return positions\n",
    "        \n",
    "        def update(frame):\n",
    "            ax.clear()\n",
    "            \n",
    "            # Set axis colors and labels\n",
    "            ax.set_facecolor('#0A0A1E')\n",
    "            ax.set_xlabel('Semantic Dimension 1', color='white', fontsize=12, labelpad=10)\n",
    "            ax.set_ylabel('Semantic Dimension 2', color='white', fontsize=12, labelpad=10)\n",
    "            ax.set_zlabel('Semantic Depth', color='white', fontsize=12, labelpad=10)\n",
    "            \n",
    "            # Set tick colors\n",
    "            ax.tick_params(axis='x', colors='white', labelsize=9)\n",
    "            ax.tick_params(axis='y', colors='white', labelsize=9)\n",
    "            ax.tick_params(axis='z', colors='white', labelsize=9)\n",
    "            \n",
    "            # Set consistent axis limits for stability\n",
    "            ax.set_xlim([-3, 3])\n",
    "            ax.set_ylim([-3, 3])\n",
    "            ax.set_zlim([-3, 3])\n",
    "            \n",
    "            # Set a subtle grid\n",
    "            ax.grid(True, linestyle=':', alpha=0.2, color='white')\n",
    "            \n",
    "            # Dynamic positions for this frame\n",
    "            positions = calculate_positions(frame)\n",
    "            \n",
    "            # Visualize primitives\n",
    "            plot_points = []\n",
    "            plot_colors = []\n",
    "            plot_sizes = []\n",
    "            plot_labels = []\n",
    "            plot_types = []\n",
    "            \n",
    "            for name, data in self.primitives.items():\n",
    "                try:\n",
    "                    # Get position\n",
    "                    if name in positions:\n",
    "                        x, y, z = positions[name]\n",
    "                    else:\n",
    "                        # Fallback to static position\n",
    "                        x, y, z = data['vector'][:3]\n",
    "                    \n",
    "                    # Dynamic size based on semantic potential - more moderate sizing\n",
    "                    size = max(80, 200 * data['potential'])\n",
    "                    \n",
    "                    # Color based on semantic type\n",
    "                    sem_type = data['type']\n",
    "                    color = type_colors.get(sem_type, '#95a5a6')  # Default to gray\n",
    "                    \n",
    "                    # Collect plot data\n",
    "                    plot_points.append([x, y, z])\n",
    "                    plot_colors.append(color)\n",
    "                    plot_sizes.append(size)\n",
    "                    plot_labels.append(name)\n",
    "                    plot_types.append(sem_type)\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Could not plot {name}: {e}\")\n",
    "            \n",
    "            # Ensure we have points\n",
    "            if not plot_points:\n",
    "                plot_points = [[0, 0, 0]]\n",
    "                plot_colors = ['white']\n",
    "                plot_sizes = [100]\n",
    "                plot_labels = ['default']\n",
    "                plot_types = ['generic']\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            plot_points = np.array(plot_points)\n",
    "            \n",
    "            # Plot points with white thin borders for better visibility\n",
    "            scatter = ax.scatter(\n",
    "                plot_points[:, 0], \n",
    "                plot_points[:, 1], \n",
    "                plot_points[:, 2], \n",
    "                c=plot_colors, \n",
    "                s=plot_sizes, \n",
    "                alpha=0.8, \n",
    "                edgecolor='white',\n",
    "                linewidth=0.5,\n",
    "                depthshade=True\n",
    "            )\n",
    "            \n",
    "            # Add labels with much improved readability\n",
    "            label_positions = {}  # Track label positions to avoid overlap\n",
    "            \n",
    "            for i, ((x, y, z), label, sem_type) in enumerate(zip(plot_points, plot_labels, plot_types)):\n",
    "                # Create a small offset to prevent label overlap with the point\n",
    "                label_offset = 0.15\n",
    "                text_pos = (x + label_offset, y + label_offset, z + label_offset)\n",
    "                \n",
    "                # Check for label position overlap and adjust if needed\n",
    "                position_key = f\"{text_pos[0]:.1f}_{text_pos[1]:.1f}_{text_pos[2]:.1f}\"\n",
    "                attempt = 0\n",
    "                while position_key in label_positions and attempt < 5:\n",
    "                    # Adjust position slightly\n",
    "                    offset_variation = 0.05 * (attempt + 1)\n",
    "                    text_pos = (\n",
    "                        x + label_offset + offset_variation * np.cos(attempt),\n",
    "                        y + label_offset + offset_variation * np.sin(attempt),\n",
    "                        z + label_offset\n",
    "                    )\n",
    "                    position_key = f\"{text_pos[0]:.1f}_{text_pos[1]:.1f}_{text_pos[2]:.1f}\"\n",
    "                    attempt += 1\n",
    "                \n",
    "                # Record this position\n",
    "                label_positions[position_key] = label\n",
    "                \n",
    "                # Add a crisp text label with semi-transparent black background\n",
    "                text = ax.text(\n",
    "                    text_pos[0], text_pos[1], text_pos[2],\n",
    "                    label, \n",
    "                    color='white', \n",
    "                    fontsize=10,\n",
    "                    fontweight='bold',\n",
    "                    bbox=dict(\n",
    "                        facecolor=(0, 0, 0, 0.3),  # More transparent background\n",
    "                        edgecolor=(1, 1, 1, 0.3),  # Subtle white edge\n",
    "                        boxstyle='round,pad=0.2',   # Smaller padding\n",
    "                        alpha=0.7\n",
    "                    ),\n",
    "                    ha='left', \n",
    "                    va='bottom',\n",
    "                    zorder=100  # Ensure labels stay on top\n",
    "                )\n",
    "            \n",
    "            # Draw connections between semantically related concepts\n",
    "            # First, draw important connections from precomputed data\n",
    "            for (name1, name2), distance in interaction_frames[frame].items():\n",
    "                if name1 in positions and name2 in positions:\n",
    "                    pos1 = positions[name1]\n",
    "                    pos2 = positions[name2]\n",
    "                    \n",
    "                    # Connection strength inversely proportional to distance\n",
    "                    connection_strength = max(0.5, 1.5 * (1.0 - distance))\n",
    "                    \n",
    "                    # Connection color - use a gradient based on distance\n",
    "                    # Close = green, Far = red\n",
    "                    if distance < 0.3:\n",
    "                        connection_color = (0.2, 0.8, 0.2, 0.7)  # Green, more opaque\n",
    "                    elif distance < 0.5:\n",
    "                        connection_color = (0.8, 0.8, 0.2, 0.6)  # Yellow\n",
    "                    else:\n",
    "                        connection_color = (0.8, 0.2, 0.2, 0.5)  # Red, more transparent\n",
    "                    \n",
    "                    # Draw connection line\n",
    "                    line = ax.plot(\n",
    "                        [pos1[0], pos2[0]], \n",
    "                        [pos1[1], pos2[1]], \n",
    "                        [pos1[2], pos2[2]], \n",
    "                        color=connection_color, \n",
    "                        linewidth=connection_strength,\n",
    "                        linestyle='-',\n",
    "                        alpha=0.7,\n",
    "                        zorder=50  # Ensure lines are below points but above grid\n",
    "                    )\n",
    "                    \n",
    "                    # Add distance label at midpoint with better positioning\n",
    "                    midpoint = [(pos1[i] + pos2[i])/2 for i in range(3)]\n",
    "                    \n",
    "                    # Only show distance if it's an important connection\n",
    "                    if distance < 0.6:\n",
    "                        dist_text = ax.text(\n",
    "                            midpoint[0], midpoint[1], midpoint[2],\n",
    "                            f\"{distance:.2f}\", \n",
    "                            color='white', \n",
    "                            fontsize=8,\n",
    "                            bbox=dict(\n",
    "                                facecolor=(0, 0, 0, 0.5),\n",
    "                                edgecolor='none',\n",
    "                                boxstyle='round,pad=0.1',\n",
    "                                alpha=0.7\n",
    "                            ),\n",
    "                            ha='center',\n",
    "                            va='center',\n",
    "                            zorder=75  # Between points and labels\n",
    "                        )\n",
    "            \n",
    "            # Also draw permanent relationship lines between language pairs\n",
    "            translation_pairs = [\n",
    "                ('apple', 'Apfel'),\n",
    "                ('red', 'rot'),\n",
    "                ('sweet', 's√º√ü')\n",
    "            ]\n",
    "            \n",
    "            for term1, term2 in translation_pairs:\n",
    "                if term1 in positions and term2 in positions:\n",
    "                    pos1 = positions[term1]\n",
    "                    pos2 = positions[term2]\n",
    "                    \n",
    "                    # Draw a dashed language relationship line\n",
    "                    language_line = ax.plot(\n",
    "                        [pos1[0], pos2[0]], \n",
    "                        [pos1[1], pos2[1]], \n",
    "                        [pos1[2], pos2[2]], \n",
    "                        color=(0.6, 0.6, 0.9, 0.7),  # Light blue\n",
    "                        linewidth=1.0,\n",
    "                        linestyle=':',  # Dotted line\n",
    "                        alpha=0.6,\n",
    "                        zorder=40\n",
    "                    )\n",
    "            \n",
    "            # Add legend for semantic types with cleaner layout\n",
    "            unique_types = sorted(list(set(plot_types)))\n",
    "            legend_elements = []\n",
    "            \n",
    "            for sem_type in unique_types:\n",
    "                color = type_colors.get(sem_type, '#95a5a6')\n",
    "                legend_elements.append(plt.Line2D(\n",
    "                    [0], [0], \n",
    "                    marker='o', \n",
    "                    color='w', \n",
    "                    label=sem_type, \n",
    "                    markerfacecolor=color,\n",
    "                    markersize=8\n",
    "                ))\n",
    "            \n",
    "            # Add frame counter and time indicator in top corner\n",
    "            ax.text2D(\n",
    "                0.02, 0.98, \n",
    "                f\"Frame: {frame}/{num_frames}\", \n",
    "                transform=ax.transAxes, \n",
    "                color='white', \n",
    "                fontsize=10,\n",
    "                bbox=dict(\n",
    "                    facecolor=(0, 0, 0, 0.3),\n",
    "                    edgecolor='none',\n",
    "                    boxstyle='round',\n",
    "                    alpha=0.7\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Add legend with better positioning and transparency\n",
    "            legend = ax.legend(\n",
    "                handles=legend_elements, \n",
    "                loc='upper right', \n",
    "                fontsize=9, \n",
    "                framealpha=0.4,\n",
    "                edgecolor='white',\n",
    "                facecolor=(0, 0, 0, 0.2)\n",
    "            )\n",
    "            \n",
    "            # Set legend text color to white\n",
    "            for text in legend.get_texts():\n",
    "                text.set_color('white')\n",
    "            \n",
    "            # Dynamic view rotation - slower and more stable\n",
    "            # Divide animation into segments for smoother transition\n",
    "            if frame < num_frames * 0.25:  # First quarter: gentle rotation\n",
    "                ax.view_init(elev=25, azim=frame * 0.3 + 30)\n",
    "            elif frame < num_frames * 0.5:  # Second quarter: change elevation\n",
    "                progress = (frame - num_frames * 0.25) / (num_frames * 0.25)\n",
    "                ax.view_init(\n",
    "                    elev=25 + progress * 15,\n",
    "                    azim=30 + num_frames * 0.25 * 0.3\n",
    "                )\n",
    "            elif frame < num_frames * 0.75:  # Third quarter: gentle rotation at new elevation\n",
    "                ax.view_init(\n",
    "                    elev=40,\n",
    "                    azim=(frame - num_frames * 0.5) * 0.3 + (30 + num_frames * 0.25 * 0.3)\n",
    "                )\n",
    "            else:  # Last quarter: return to original view\n",
    "                progress = (frame - num_frames * 0.75) / (num_frames * 0.25)\n",
    "                ax.view_init(\n",
    "                    elev=40 - progress * 15,\n",
    "                    azim=frame * 0.3 + 30\n",
    "                )\n",
    "            \n",
    "            # More elegant and descriptive title\n",
    "            plt.title(\n",
    "                f'Semantic Convergence - Frame {frame}', \n",
    "                color='white', \n",
    "                fontsize=14,\n",
    "                pad=20\n",
    "            )\n",
    "            \n",
    "            # Print progress occasionally\n",
    "            if frame % 25 == 0:\n",
    "                print(f\"Rendering frame {frame}/{num_frames}\")\n",
    "            \n",
    "            return scatter,\n",
    "        \n",
    "        # First, generate a static image for safety\n",
    "        print(\"Creating static visualization...\")\n",
    "        static_frame = num_frames // 2  # Use middle frame for static visualization\n",
    "        update(static_frame)\n",
    "        static_path = os.path.join(output_dir, f'semantic_static_{uuid.uuid4()}.png')\n",
    "        plt.savefig(static_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Static visualization saved to {static_path}\")\n",
    "        \n",
    "        # Try to generate animation if requested\n",
    "        if make_animation:\n",
    "            try:\n",
    "                print(\"Creating animation...\")\n",
    "                # Create animation with progress tracking\n",
    "                anim = animation.FuncAnimation(\n",
    "                    fig, update, frames=num_frames, \n",
    "                    interval=50,  # Faster for smoother appearance\n",
    "                    blit=True  # Use blitting for better performance\n",
    "                )\n",
    "                \n",
    "                # Save with high-quality settings\n",
    "                output_path = os.path.join(output_dir, f'semantic_dynamics_{uuid.uuid4()}.gif')\n",
    "                \n",
    "                print(f\"Saving animation to {output_path}...\")\n",
    "                anim.save(\n",
    "                    output_path, \n",
    "                    writer='pillow', \n",
    "                    fps=24,  # Higher for smoother animation\n",
    "                    dpi=120  # Higher resolution\n",
    "                )\n",
    "                print(\"Animation saved successfully!\")\n",
    "                plt.close(fig)\n",
    "                return output_path\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Animation save failed: {e}\")\n",
    "                print(f\"Animation failed: {e}\")\n",
    "                print(\"Defaulting to static visualization.\")\n",
    "                plt.close(fig)\n",
    "                return static_path\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "            return static_path\n",
    "\n",
    "def semantic_visualization(output_dir=None, make_animation=True):\n",
    "    \"\"\"\n",
    "    Create an advanced semantic visualization with enhanced aesthetics and feedback\n",
    "    \"\"\"\n",
    "    print(\"Starting semantic visualization process...\")\n",
    "    \n",
    "    # Use specified or default output directory\n",
    "    output_dir = output_dir or os.path.join(os.getcwd(), 'output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Initialize semantic field\n",
    "    semantic_field = SemanticField(num_dimensions=5)\n",
    "    \n",
    "    # Generate visualization\n",
    "    print(\"Generating visualization...\")\n",
    "    visualization_path = semantic_field.visualize_semantic_dynamics(\n",
    "        output_dir, \n",
    "        num_frames=150,    # Reasonable frame count for smooth animation\n",
    "        interaction_rounds=5,  # More frequent interactions\n",
    "        make_animation=make_animation  # Control whether to attempt animation\n",
    "    )\n",
    "    \n",
    "    print(\"Visualization complete!\")\n",
    "    return semantic_field, visualization_path\n",
    "\n",
    "# Run the demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the output directory\n",
    "    output_dir = r\"C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Output\"\n",
    "    print(f\"Initializing semantic visualization in {output_dir}\")\n",
    "    \n",
    "    # Run the semantic visualization with progress tracking\n",
    "    semantic_field, visualization_path = semantic_visualization(output_dir, make_animation=True)\n",
    "    print(f\"Semantic visualization created at: {visualization_path}\")\n",
    "    print(\"Process complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe35dd5",
   "metadata": {},
   "source": [
    "# Phase 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4553840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ConceptNet loaded with 3423004 assertions.\n",
      "German ConceptNet loaded with 1078946 assertions.\n",
      "                                                         start  \\\n",
      "/a/[/r/Antonym/,/c/en/0/n/,/c/en/1/]                /r/Antonym   \n",
      "/a/[/r/Antonym/,/c/en/12_hour_clock/n/,/c/en/24...  /r/Antonym   \n",
      "/a/[/r/Antonym/,/c/en/24_hour_clock/n/,/c/en/12...  /r/Antonym   \n",
      "/a/[/r/Antonym/,/c/en/5/n/,/c/en/3/]                /r/Antonym   \n",
      "/a/[/r/Antonym/,/c/en/a.c/n/,/c/en/d.c/]            /r/Antonym   \n",
      "\n",
      "                                                                      rel  \\\n",
      "/a/[/r/Antonym/,/c/en/0/n/,/c/en/1/]                            /c/en/0/n   \n",
      "/a/[/r/Antonym/,/c/en/12_hour_clock/n/,/c/en/24...  /c/en/12_hour_clock/n   \n",
      "/a/[/r/Antonym/,/c/en/24_hour_clock/n/,/c/en/12...  /c/en/24_hour_clock/n   \n",
      "/a/[/r/Antonym/,/c/en/5/n/,/c/en/3/]                            /c/en/5/n   \n",
      "/a/[/r/Antonym/,/c/en/a.c/n/,/c/en/d.c/]                      /c/en/a.c/n   \n",
      "\n",
      "                                                                    end  \\\n",
      "/a/[/r/Antonym/,/c/en/0/n/,/c/en/1/]                            /c/en/1   \n",
      "/a/[/r/Antonym/,/c/en/12_hour_clock/n/,/c/en/24...  /c/en/24_hour_clock   \n",
      "/a/[/r/Antonym/,/c/en/24_hour_clock/n/,/c/en/12...  /c/en/12_hour_clock   \n",
      "/a/[/r/Antonym/,/c/en/5/n/,/c/en/3/]                            /c/en/3   \n",
      "/a/[/r/Antonym/,/c/en/a.c/n/,/c/en/d.c/]                      /c/en/d.c   \n",
      "\n",
      "                                                                                               weight  \n",
      "/a/[/r/Antonym/,/c/en/0/n/,/c/en/1/]                {\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc...  \n",
      "/a/[/r/Antonym/,/c/en/12_hour_clock/n/,/c/en/24...  {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...  \n",
      "/a/[/r/Antonym/,/c/en/24_hour_clock/n/,/c/en/12...  {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...  \n",
      "/a/[/r/Antonym/,/c/en/5/n/,/c/en/3/]                {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...  \n",
      "/a/[/r/Antonym/,/c/en/a.c/n/,/c/en/d.c/]            {\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc...  \n",
      "                                                         start  \\\n",
      "/a/[/r/Antonym/,/c/de/a_posteriori/a/,/c/de/a_p...  /r/Antonym   \n",
      "/a/[/r/Antonym/,/c/de/ab/,/c/de/an/]                /r/Antonym   \n",
      "/a/[/r/Antonym/,/c/de/ab/,/c/de/auf/]               /r/Antonym   \n",
      "/a/[/r/Antonym/,/c/de/abberufen/v/,/c/de/berufen/]  /r/Antonym   \n",
      "/a/[/r/Antonym/,/c/de/abberufen/v/,/c/de/einset...  /r/Antonym   \n",
      "\n",
      "                                                                     rel  \\\n",
      "/a/[/r/Antonym/,/c/de/a_posteriori/a/,/c/de/a_p...  /c/de/a_posteriori/a   \n",
      "/a/[/r/Antonym/,/c/de/ab/,/c/de/an/]                            /c/de/ab   \n",
      "/a/[/r/Antonym/,/c/de/ab/,/c/de/auf/]                           /c/de/ab   \n",
      "/a/[/r/Antonym/,/c/de/abberufen/v/,/c/de/berufen/]     /c/de/abberufen/v   \n",
      "/a/[/r/Antonym/,/c/de/abberufen/v/,/c/de/einset...     /c/de/abberufen/v   \n",
      "\n",
      "                                                                end  \\\n",
      "/a/[/r/Antonym/,/c/de/a_posteriori/a/,/c/de/a_p...   /c/de/a_priori   \n",
      "/a/[/r/Antonym/,/c/de/ab/,/c/de/an/]                       /c/de/an   \n",
      "/a/[/r/Antonym/,/c/de/ab/,/c/de/auf/]                     /c/de/auf   \n",
      "/a/[/r/Antonym/,/c/de/abberufen/v/,/c/de/berufen/]    /c/de/berufen   \n",
      "/a/[/r/Antonym/,/c/de/abberufen/v/,/c/de/einset...  /c/de/einsetzen   \n",
      "\n",
      "                                                                                               weight  \n",
      "/a/[/r/Antonym/,/c/de/a_posteriori/a/,/c/de/a_p...  {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...  \n",
      "/a/[/r/Antonym/,/c/de/ab/,/c/de/an/]                {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...  \n",
      "/a/[/r/Antonym/,/c/de/ab/,/c/de/auf/]               {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...  \n",
      "/a/[/r/Antonym/,/c/de/abberufen/v/,/c/de/berufen/]  {\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc...  \n",
      "/a/[/r/Antonym/,/c/de/abberufen/v/,/c/de/einset...  {\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc...  \n",
      "Loading ConceptNet data...\n",
      "English ConceptNet loaded with 3423004 assertions.\n",
      "German ConceptNet loaded with 1078946 assertions.\n",
      "Starting ConceptNet processing pipeline...\n",
      "ConceptNetProcessor initialized\n",
      "Building semantic graph from ConceptNet data...\n",
      "Processing 3423004 English ConceptNet assertions...\n",
      "Processing 1078946 German ConceptNet assertions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing en assertions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 30395.20it/s]\n",
      "Processing de assertions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 35733.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting graph to top 100 concepts...\n",
      "Semantic graph built with 100 nodes and 235 edges\n",
      "Inferring semantic categories...\n",
      "Inferred categories: {'generic': 100}\n",
      "Generating 5-dimensional concept vectors...\n",
      "Generated vectors for 100 concepts\n",
      "Creating simplified test animation...\n",
      "Creating simple test animation...\n",
      "Saving simple animation to C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Output\\semantic_test_04bc6734-26f6-4f7f-bd04-9ed37c2567eb.gif...\n",
      "Rendering simplified frame 0/20\n",
      "Rendering simplified frame 0/20\n",
      "Rendering simplified frame 5/20\n",
      "Rendering simplified frame 10/20\n",
      "Rendering simplified frame 15/20\n",
      "Test animation saved! File size: 0.49 MB\n",
      "GIF contains 20 frames\n",
      "Visualization created at: C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Output\\semantic_test_04bc6734-26f6-4f7f-bd04-9ed37c2567eb.gif\n",
      "Process complete!\n"
     ]
    }
   ],
   "source": [
    "input_data_path = r'C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Input'\n",
    "\n",
    "english_conceptnet = pd.read_csv(os.path.join(input_data_path, 'conceptnet-assertions-5.7.0.en.tsv'), sep='\\t', header=None, names=['start', 'rel', 'end', 'weight'])\n",
    "german_conceptnet = pd.read_csv(os.path.join(input_data_path, 'conceptnet-assertions-5.7.0.de.tsv'), sep='\\t', header=None, names=['start', 'rel', 'end', 'weight'])\n",
    "\n",
    "print(\"English ConceptNet loaded with\", len(english_conceptnet), \"assertions.\")\n",
    "print(\"German ConceptNet loaded with\", len(german_conceptnet), \"assertions.\")\n",
    "\n",
    "print(english_conceptnet.head())\n",
    "print(german_conceptnet.head())\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "import io\n",
    "import base64\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "import warnings\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import PillowWriter  # Add this\n",
    "try:\n",
    "    import psutil  # Add this for memory monitoring\n",
    "except ImportError:\n",
    "    print(\"psutil not installed, memory monitoring will be disabled\")\n",
    "\n",
    "class ConceptNetProcessor:\n",
    "    \"\"\"\n",
    "    Process ConceptNet data for semantic visualization\n",
    "    \"\"\"\n",
    "    def __init__(self, english_data=None, german_data=None):\n",
    "        self.english_data = english_data\n",
    "        self.german_data = german_data\n",
    "        self.semantic_graph = nx.DiGraph()\n",
    "        self.concept_vectors = {}\n",
    "        self.relation_types = set()\n",
    "        \n",
    "        print(\"ConceptNetProcessor initialized\")\n",
    "    \n",
    "    def clean_concept_name(self, concept_str):\n",
    "        \"\"\"Extract clean concept name from ConceptNet format\"\"\"\n",
    "        if not isinstance(concept_str, str):\n",
    "            return \"unknown\"\n",
    "            \n",
    "        # Extract the concept name from the ConceptNet URI format\n",
    "        parts = concept_str.split('/')\n",
    "        if len(parts) >= 4:\n",
    "            # Format is typically /c/LANG/CONCEPT\n",
    "            concept = parts[-1]\n",
    "            # Remove part-of-speech tags if present\n",
    "            if '/' in concept:\n",
    "                concept = concept.split('/')[0]\n",
    "            return concept\n",
    "        return concept_str\n",
    "    \n",
    "    def extract_relation_type(self, relation_str):\n",
    "        \"\"\"Extract relation type from ConceptNet format\"\"\"\n",
    "        if not isinstance(relation_str, str):\n",
    "            return \"unknown\"\n",
    "            \n",
    "        parts = relation_str.split('/')\n",
    "        if len(parts) >= 3:\n",
    "            # Format is typically /r/RELATION_TYPE\n",
    "            return parts[-1]\n",
    "        return relation_str\n",
    "    \n",
    "    def extract_language(self, concept_str):\n",
    "        \"\"\"Extract language from ConceptNet concept URI\"\"\"\n",
    "        if not isinstance(concept_str, str):\n",
    "            return \"unknown\"\n",
    "            \n",
    "        parts = concept_str.split('/')\n",
    "        if len(parts) >= 4:\n",
    "            # Format is typically /c/LANG/CONCEPT\n",
    "            return parts[2]\n",
    "        return \"unknown\"\n",
    "    \n",
    "    def parse_weight(self, weight_str):\n",
    "        \"\"\"Parse weight JSON string to extract numeric weight\"\"\"\n",
    "        if not isinstance(weight_str, str):\n",
    "            return 1.0\n",
    "            \n",
    "        try:\n",
    "            weight_data = json.loads(weight_str)\n",
    "            # ConceptNet weights are typically in 'weight' field\n",
    "            return float(weight_data.get('weight', 1.0))\n",
    "        except:\n",
    "            return 1.0\n",
    "    \n",
    "    def build_semantic_graph(self, max_concepts=300, min_weight=1.0):\n",
    "        \"\"\"Build semantic graph from ConceptNet data\"\"\"\n",
    "        print(\"Building semantic graph from ConceptNet data...\")\n",
    "        \n",
    "        if self.english_data is None and self.german_data is None:\n",
    "            print(\"No ConceptNet data provided.\")\n",
    "            return\n",
    "        \n",
    "        # Combine datasets\n",
    "        all_data = []\n",
    "        if self.english_data is not None:\n",
    "            print(f\"Processing {len(self.english_data)} English ConceptNet assertions...\")\n",
    "            all_data.append(('en', self.english_data))\n",
    "        \n",
    "        if self.german_data is not None:\n",
    "            print(f\"Processing {len(self.german_data)} German ConceptNet assertions...\")\n",
    "            all_data.append(('de', self.german_data))\n",
    "        \n",
    "        # Track concepts and their occurrence count\n",
    "        concept_counts = {}\n",
    "        \n",
    "        # Process each language dataset\n",
    "        for lang, data in all_data:\n",
    "            # Sample to ensure manageable size if needed\n",
    "            if len(data) > 10000:\n",
    "                data_sample = data.sample(10000, random_state=42)\n",
    "            else:\n",
    "                data_sample = data\n",
    "            \n",
    "            # Process assertions\n",
    "            for _, row in tqdm(data_sample.iterrows(), desc=f\"Processing {lang} assertions\", total=len(data_sample)):\n",
    "                try:\n",
    "                    # Extract source and target concepts\n",
    "                    source_concept = self.clean_concept_name(row['rel'])\n",
    "                    target_concept = self.clean_concept_name(row['end'])\n",
    "                    \n",
    "                    # Extract relation type\n",
    "                    relation_type = self.extract_relation_type(row['start'])\n",
    "                    self.relation_types.add(relation_type)\n",
    "                    \n",
    "                    # Extract languages\n",
    "                    source_lang = self.extract_language(row['rel'])\n",
    "                    target_lang = self.extract_language(row['end'])\n",
    "                    \n",
    "                    # Parse weight\n",
    "                    weight = self.parse_weight(row['weight'])\n",
    "                    \n",
    "                    # Skip low-weight relationships\n",
    "                    if weight < min_weight:\n",
    "                        continue\n",
    "                    \n",
    "                    # Track concept occurrences\n",
    "                    concept_counts[source_concept] = concept_counts.get(source_concept, 0) + 1\n",
    "                    concept_counts[target_concept] = concept_counts.get(target_concept, 0) + 1\n",
    "                    \n",
    "                    # Add to graph\n",
    "                    self.semantic_graph.add_node(\n",
    "                        source_concept,\n",
    "                        lang=source_lang,\n",
    "                        count=concept_counts[source_concept]\n",
    "                    )\n",
    "                    \n",
    "                    self.semantic_graph.add_node(\n",
    "                        target_concept,\n",
    "                        lang=target_lang,\n",
    "                        count=concept_counts[target_concept]\n",
    "                    )\n",
    "                    \n",
    "                    # Add edge with relation data\n",
    "                    self.semantic_graph.add_edge(\n",
    "                        source_concept,\n",
    "                        target_concept,\n",
    "                        relation=relation_type,\n",
    "                        weight=weight\n",
    "                    )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Error processing assertion: {e}\")\n",
    "        \n",
    "        # Limit to top concepts if needed\n",
    "        if len(concept_counts) > max_concepts:\n",
    "            print(f\"Limiting graph to top {max_concepts} concepts...\")\n",
    "            top_concepts = sorted(concept_counts.items(), key=lambda x: x[1], reverse=True)[:max_concepts]\n",
    "            top_concept_names = {c[0] for c in top_concepts}\n",
    "            \n",
    "            # Create subgraph with only top concepts\n",
    "            subgraph = nx.DiGraph()\n",
    "            \n",
    "            for node in top_concept_names:\n",
    "                if self.semantic_graph.has_node(node):\n",
    "                    subgraph.add_node(\n",
    "                        node,\n",
    "                        **self.semantic_graph.nodes[node]\n",
    "                    )\n",
    "            \n",
    "            for source, target, data in self.semantic_graph.edges(data=True):\n",
    "                if source in top_concept_names and target in top_concept_names:\n",
    "                    subgraph.add_edge(\n",
    "                        source,\n",
    "                        target,\n",
    "                        **data\n",
    "                    )\n",
    "            \n",
    "            self.semantic_graph = subgraph\n",
    "        \n",
    "        print(f\"Semantic graph built with {self.semantic_graph.number_of_nodes()} nodes and {self.semantic_graph.number_of_edges()} edges\")\n",
    "        \n",
    "        # Infer semantic categories\n",
    "        self.infer_semantic_categories()\n",
    "        \n",
    "        return self.semantic_graph\n",
    "    \n",
    "    def infer_semantic_categories(self):\n",
    "        \"\"\"Infer semantic categories for concepts in the graph\"\"\"\n",
    "        print(\"Inferring semantic categories...\")\n",
    "        \n",
    "        # Define category patterns\n",
    "        category_patterns = {\n",
    "            'fruit': ['apple', 'banana', 'orange', 'fruit', 'berry', 'apfel'],\n",
    "            'color': ['red', 'blue', 'green', 'yellow', 'color', 'rot', 'blau', 'gr√ºn', 'gelb', 'farbe'],\n",
    "            'taste': ['sweet', 'sour', 'bitter', 'taste', 's√º√ü', 'sauer', 'geschmack'],\n",
    "            'vehicle': ['car', 'bus', 'train', 'vehicle', 'auto', 'fahrzeug'],\n",
    "            'temperature': ['hot', 'cold', 'warm', 'cool', 'hei√ü', 'kalt', 'temperatur'],\n",
    "            'size': ['big', 'small', 'large', 'tiny', 'gro√ü', 'klein', 'gr√∂√üe']\n",
    "        }\n",
    "        \n",
    "        # Use relation types to help infer categories\n",
    "        relation_category_map = {\n",
    "            'IsA': 'type',\n",
    "            'PartOf': 'part',\n",
    "            'HasA': 'property',\n",
    "            'UsedFor': 'function',\n",
    "            'CapableOf': 'capability',\n",
    "            'AtLocation': 'location',\n",
    "            'HasProperty': 'property'\n",
    "        }\n",
    "        \n",
    "        # Assign categories\n",
    "        for node in self.semantic_graph.nodes():\n",
    "            # Initialize with unknown category\n",
    "            self.semantic_graph.nodes[node]['category'] = 'generic'\n",
    "            \n",
    "            # Check for pattern matches\n",
    "            node_lower = str(node).lower()\n",
    "            \n",
    "            for category, patterns in category_patterns.items():\n",
    "                if any(pattern in node_lower for pattern in patterns):\n",
    "                    self.semantic_graph.nodes[node]['category'] = category\n",
    "                    break\n",
    "            \n",
    "            # Use incoming edges to help determine category\n",
    "            incoming_edges = list(self.semantic_graph.in_edges(node, data=True))\n",
    "            if incoming_edges:\n",
    "                for source, _, data in incoming_edges:\n",
    "                    relation = data.get('relation', '')\n",
    "                    if relation in relation_category_map:\n",
    "                        # Use the source node's category for certain relations\n",
    "                        if relation in ['IsA', 'PartOf'] and 'category' in self.semantic_graph.nodes[source]:\n",
    "                            source_category = self.semantic_graph.nodes[source]['category']\n",
    "                            if source_category != 'generic':\n",
    "                                self.semantic_graph.nodes[node]['category'] = source_category\n",
    "                                break\n",
    "        \n",
    "        # Count categories\n",
    "        categories = {}\n",
    "        for node, data in self.semantic_graph.nodes(data=True):\n",
    "            category = data.get('category', 'generic')\n",
    "            categories[category] = categories.get(category, 0) + 1\n",
    "        \n",
    "        print(\"Inferred categories:\", categories)\n",
    "    \n",
    "    def generate_concept_vectors(self, dimensions=5):\n",
    "        \"\"\"Generate semantic vectors for concepts in the graph\"\"\"\n",
    "        print(f\"Generating {dimensions}-dimensional concept vectors...\")\n",
    "        \n",
    "        if not self.semantic_graph:\n",
    "            print(\"No semantic graph available.\")\n",
    "            return {}\n",
    "        \n",
    "        # Group concepts by category\n",
    "        concepts_by_category = {}\n",
    "        for node, data in self.semantic_graph.nodes(data=True):\n",
    "            category = data.get('category', 'generic')\n",
    "            if category not in concepts_by_category:\n",
    "                concepts_by_category[category] = []\n",
    "            concepts_by_category[category].append(node)\n",
    "        \n",
    "        # Generate base vectors for each category\n",
    "        category_base_vectors = {}\n",
    "        for category in concepts_by_category.keys():\n",
    "            # Use deterministic seed for consistency, but ensure it's within valid range\n",
    "            seed_value = abs(hash(category)) % (2**32 - 1)  # Safe seed range for NumPy\n",
    "            np.random.seed(seed_value)\n",
    "            category_base_vectors[category] = np.random.randn(dimensions)\n",
    "            # Normalize to unit length\n",
    "            category_base_vectors[category] = category_base_vectors[category] / np.linalg.norm(category_base_vectors[category])\n",
    "        \n",
    "        # Generate vectors for concepts within each category\n",
    "        for category, concepts in concepts_by_category.items():\n",
    "            base_vector = category_base_vectors[category]\n",
    "            \n",
    "            for concept in concepts:\n",
    "                # Get node data\n",
    "                node_data = self.semantic_graph.nodes[concept]\n",
    "                count = node_data.get('count', 1)\n",
    "                lang = node_data.get('lang', 'unknown')\n",
    "                \n",
    "                # Create language-specific seed value\n",
    "                lang_seed = 0\n",
    "                if lang == 'en':\n",
    "                    lang_seed = 1\n",
    "                elif lang == 'de':\n",
    "                    lang_seed = 2\n",
    "                \n",
    "                # Scale by frequency/importance\n",
    "                importance_factor = np.log1p(count) / 10\n",
    "                \n",
    "                # Add controlled variation with proper seed range\n",
    "                # Fix: Ensure seed value is within valid range for NumPy\n",
    "                concept_hash = abs(hash(concept)) % (2**32 - 1)  # Get positive hash in valid range\n",
    "                seed_value = (concept_hash + lang_seed) % (2**32 - 1)  # Combine with language seed safely\n",
    "                np.random.seed(seed_value)\n",
    "                variation = np.random.randn(dimensions) * 0.2\n",
    "                \n",
    "                # Calculate final vector\n",
    "                concept_vector = base_vector * (1.0 + importance_factor) + variation\n",
    "                # Normalize for consistent visualization\n",
    "                concept_vector = concept_vector / np.linalg.norm(concept_vector) * 1.5\n",
    "                \n",
    "                # Store vector with metadata\n",
    "                self.concept_vectors[concept] = {\n",
    "                    'vector': concept_vector,\n",
    "                    'category': category,\n",
    "                    'lang': lang,\n",
    "                    'count': count,\n",
    "                    'potential': min(1.5, 0.5 + importance_factor)  # For visualization sizing\n",
    "                }\n",
    "        \n",
    "        # Process translation equivalents\n",
    "        self.process_translations()\n",
    "        \n",
    "        # Adjust vectors based on graph connections\n",
    "        self.adjust_vectors_by_relationships()\n",
    "        \n",
    "        print(f\"Generated vectors for {len(self.concept_vectors)} concepts\")\n",
    "        return self.concept_vectors\n",
    "    \n",
    "    def process_translations(self):\n",
    "        \"\"\"\n",
    "        Process translation equivalents to ensure they're positioned near each other\n",
    "        \"\"\"\n",
    "        # Look for translation pairs using typical patterns in ConceptNet\n",
    "        translation_pairs = []\n",
    "        \n",
    "        for source, target, data in self.semantic_graph.edges(data=True):\n",
    "            relation = data.get('relation', '')\n",
    "            \n",
    "            # Check for translation relationships\n",
    "            if relation in ['TranslationOf', 'Synonym']:\n",
    "                if source in self.concept_vectors and target in self.concept_vectors:\n",
    "                    source_lang = self.semantic_graph.nodes[source].get('lang', 'unknown')\n",
    "                    target_lang = self.semantic_graph.nodes[target].get('lang', 'unknown')\n",
    "                    \n",
    "                    # Only consider cross-language pairs or clear synonyms\n",
    "                    if source_lang != target_lang or relation == 'Synonym':\n",
    "                        translation_pairs.append((source, target))\n",
    "        \n",
    "        # Adjust vectors for translation pairs\n",
    "        for source, target in translation_pairs:\n",
    "            if source in self.concept_vectors and target in self.concept_vectors:\n",
    "                # Get vectors\n",
    "                source_vector = self.concept_vectors[source]['vector']\n",
    "                target_vector = self.concept_vectors[target]['vector']\n",
    "                \n",
    "                # Calculate average vector\n",
    "                avg_vector = (source_vector + target_vector) / 2\n",
    "                \n",
    "                # Move both vectors closer to average\n",
    "                self.concept_vectors[source]['vector'] = 0.7 * source_vector + 0.3 * avg_vector\n",
    "                self.concept_vectors[target]['vector'] = 0.7 * target_vector + 0.3 * avg_vector\n",
    "                \n",
    "                # Mark as translation pair\n",
    "                self.concept_vectors[source]['translation_pair'] = target\n",
    "                self.concept_vectors[target]['translation_pair'] = source\n",
    "    \n",
    "    def adjust_vectors_by_relationships(self):\n",
    "        \"\"\"\n",
    "        Adjust vectors based on semantic relationships\n",
    "        \"\"\"\n",
    "        # Iterate to refine positions\n",
    "        for _ in range(3):  \n",
    "            for source, target, data in self.semantic_graph.edges(data=True):\n",
    "                if source in self.concept_vectors and target in self.concept_vectors:\n",
    "                    weight = data.get('weight', 1.0)\n",
    "                    relation = data.get('relation', '')\n",
    "                    \n",
    "                    # Adjust factor based on relation type\n",
    "                    relation_factor = 0.1  # Default\n",
    "                    \n",
    "                    if relation in ['IsA', 'PartOf', 'MadeOf']:\n",
    "                        relation_factor = 0.2  # Stronger pull for hierarchical relations\n",
    "                    elif relation in ['HasProperty', 'HasA', 'CapableOf']:\n",
    "                        relation_factor = 0.15  # Medium pull\n",
    "                    elif relation in ['Antonym', 'DistinctFrom']:\n",
    "                        relation_factor = -0.1  # Push apart for opposite relationships\n",
    "                    \n",
    "                    # Apply adjustment factor\n",
    "                    source_vec = self.concept_vectors[source]['vector']\n",
    "                    target_vec = self.concept_vectors[target]['vector']\n",
    "                    \n",
    "                    # Calculate move vectors - weighted by relation\n",
    "                    diff_vec = (target_vec - source_vec) * weight * relation_factor\n",
    "                    \n",
    "                    # Move vectors\n",
    "                    self.concept_vectors[source]['vector'] += diff_vec\n",
    "                    \n",
    "                    # Only move target for positive relationships\n",
    "                    if relation_factor > 0:\n",
    "                        self.concept_vectors[target]['vector'] -= diff_vec * 0.5\n",
    "                    \n",
    "                    # Renormalize\n",
    "                    self.concept_vectors[source]['vector'] = self.concept_vectors[source]['vector'] / np.linalg.norm(self.concept_vectors[source]['vector']) * 1.5\n",
    "                    self.concept_vectors[target]['vector'] = self.concept_vectors[target]['vector'] / np.linalg.norm(self.concept_vectors[target]['vector']) * 1.5\n",
    "    \n",
    "    def compute_important_relationships(self, threshold=0.5, max_relationships=30):\n",
    "        \"\"\"Compute important relationships for visualization\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        for source, target, data in self.semantic_graph.edges(data=True):\n",
    "            if source in self.concept_vectors and target in self.concept_vectors:\n",
    "                weight = data.get('weight', 0.0)\n",
    "                relation = data.get('relation', 'related')\n",
    "                \n",
    "                # Only include relationships above threshold\n",
    "                if weight >= threshold:\n",
    "                    source_vec = self.concept_vectors[source]['vector']\n",
    "                    target_vec = self.concept_vectors[target]['vector']\n",
    "                    \n",
    "                    # Calculate actual vector distance\n",
    "                    try:\n",
    "                        distance = cosine(source_vec, target_vec)\n",
    "                    except:\n",
    "                        distance = 0.5\n",
    "                    \n",
    "                    relationships.append({\n",
    "                        'source': source,\n",
    "                        'target': target,\n",
    "                        'weight': weight,\n",
    "                        'relation': relation,\n",
    "                        'distance': distance\n",
    "                    })\n",
    "        \n",
    "        # Sort by weight and take top relationships\n",
    "        relationships = sorted(relationships, key=lambda x: x['weight'], reverse=True)[:max_relationships]\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "class SemanticVisualizer:\n",
    "    \"\"\"\n",
    "    Enhanced semantic field visualizer using ConceptNet data\n",
    "    \"\"\"\n",
    "    def __init__(self, concept_processor=None):\n",
    "        self.concept_processor = concept_processor\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "    \n",
    "    def setup_visualization(self):\n",
    "        \"\"\"Set up the visualization environment\"\"\"\n",
    "        # Create figure with elegant dark theme\n",
    "        plt.style.use('dark_background')\n",
    "        self.fig = plt.figure(figsize=(16, 12), facecolor='#0A0A1E')\n",
    "        self.ax = self.fig.add_subplot(111, projection='3d', facecolor='#0A0A1E')\n",
    "        \n",
    "        # Remove panes to reduce visual clutter\n",
    "        self.ax.xaxis.pane.fill = False\n",
    "        self.ax.yaxis.pane.fill = False\n",
    "        self.ax.zaxis.pane.fill = False\n",
    "        \n",
    "        # Make grid semi-transparent\n",
    "        self.ax.xaxis.pane.set_edgecolor('w')\n",
    "        self.ax.yaxis.pane.set_edgecolor('w')\n",
    "        self.ax.zaxis.pane.set_edgecolor('w')\n",
    "        self.ax.xaxis.pane.set_alpha(0.1)\n",
    "        self.ax.yaxis.pane.set_alpha(0.1)\n",
    "        self.ax.zaxis.pane.set_alpha(0.1)\n",
    "        \n",
    "        # Set title with elegant typography\n",
    "        self.fig.suptitle('Semantica: Semantic Convergence', \n",
    "                     color='white', fontsize=24, fontweight='light', y=0.98)\n",
    "        \n",
    "        # Set axis colors and labels\n",
    "        self.ax.set_xlabel('Semantic Dimension 1', color='white', fontsize=12, labelpad=10)\n",
    "        self.ax.set_ylabel('Semantic Dimension 2', color='white', fontsize=12, labelpad=10)\n",
    "        self.ax.set_zlabel('Semantic Depth', color='white', fontsize=12, labelpad=10)\n",
    "        \n",
    "        # Set tick colors\n",
    "        self.ax.tick_params(axis='x', colors='white', labelsize=9)\n",
    "        self.ax.tick_params(axis='y', colors='white', labelsize=9)\n",
    "        self.ax.tick_params(axis='z', colors='white', labelsize=9)\n",
    "        \n",
    "        # Set a subtle grid\n",
    "        self.ax.grid(True, linestyle=':', alpha=0.2, color='white')\n",
    "        \n",
    "        return self.fig, self.ax\n",
    "    \n",
    "    def visualize_concepts_enhanced(self, output_dir, num_frames=20):\n",
    "        \"\"\"\n",
    "        Create an enhanced animation with some additional visual elements\n",
    "        \"\"\"\n",
    "        print(\"Creating enhanced test animation...\")\n",
    "        \n",
    "        # Set up visualization\n",
    "        plt.close('all')  # Close all existing figures first\n",
    "        fig = plt.figure(figsize=(12, 10), facecolor='black')\n",
    "        ax = fig.add_subplot(111, projection='3d', facecolor='black')\n",
    "        \n",
    "        # Get concept vectors and important relationships\n",
    "        concept_vectors = self.concept_processor.concept_vectors\n",
    "        important_relationships = self.concept_processor.compute_important_relationships(threshold=1.0, max_relationships=10)\n",
    "        \n",
    "        # Simple color scheme for categories\n",
    "        category_colors = {\n",
    "            'fruit': '#e74c3c',      # Red\n",
    "            'color': '#3498db',      # Blue\n",
    "            'taste': '#f39c12',      # Orange\n",
    "            'vehicle': '#2ecc71',    # Green\n",
    "            'temperature': '#9b59b6', # Purple\n",
    "            'size': '#1abc9c',       # Teal\n",
    "            'generic': '#95a5a6'     # Gray\n",
    "        }\n",
    "        \n",
    "        # Create a simple function to update the plot\n",
    "        def update(frame):\n",
    "            ax.clear()\n",
    "            \n",
    "            # Set up a clean visualization space\n",
    "            ax.set_facecolor('black')\n",
    "            ax.xaxis.pane.fill = False\n",
    "            ax.yaxis.pane.fill = False\n",
    "            ax.zaxis.pane.fill = False\n",
    "            ax.grid(True, linestyle=':', alpha=0.3, color='white')\n",
    "            \n",
    "            # Set limits\n",
    "            ax.set_xlim([-3, 3])\n",
    "            ax.set_ylim([-3, 3])\n",
    "            ax.set_zlim([-3, 3])\n",
    "            \n",
    "            # Extract points from concept vectors with slight animation\n",
    "            points = {}\n",
    "            plot_points = []\n",
    "            plot_colors = []\n",
    "            plot_sizes = []\n",
    "            plot_labels = []\n",
    "            \n",
    "            for concept, data in concept_vectors.items():\n",
    "                vector = data['vector'][:3]  # First 3 dimensions\n",
    "                \n",
    "                # Add subtle animation\n",
    "                oscillation = np.array([\n",
    "                    0.05 * np.sin(frame * 0.2 + hash(concept) % 10),\n",
    "                    0.05 * np.cos(frame * 0.2 + hash(concept) % 10),\n",
    "                    0.05 * np.sin(frame * 0.2 + hash(concept) % 10)\n",
    "                ])\n",
    "                \n",
    "                position = vector + oscillation\n",
    "                points[concept] = position\n",
    "                \n",
    "                # Get color based on category\n",
    "                category = data.get('category', 'generic')\n",
    "                color = category_colors.get(category, '#95a5a6')\n",
    "                \n",
    "                # Size based on importance\n",
    "                size = 50 + 20 * data.get('potential', 1.0)\n",
    "                \n",
    "                plot_points.append(position)\n",
    "                plot_colors.append(color)\n",
    "                plot_sizes.append(size)\n",
    "                plot_labels.append(concept)\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            plot_points = np.array(plot_points)\n",
    "            \n",
    "            # Plot points\n",
    "            ax.scatter(plot_points[:, 0], plot_points[:, 1], plot_points[:, 2], \n",
    "                    c=plot_colors, s=plot_sizes, alpha=0.8, edgecolor='white', linewidth=0.5)\n",
    "            \n",
    "            # Draw a few key relationships\n",
    "            for i, rel in enumerate(important_relationships[:5]):  # Just draw top 5 for simplicity\n",
    "                source = rel['source']\n",
    "                target = rel['target']\n",
    "                \n",
    "                if source in points and target in points:\n",
    "                    # Draw line connecting related concepts\n",
    "                    ax.plot([points[source][0], points[target][0]], \n",
    "                            [points[source][1], points[target][1]], \n",
    "                            [points[source][2], points[target][2]], \n",
    "                            color='white', alpha=0.4, linewidth=1)\n",
    "            \n",
    "            # Add a few labels - just top 5 points to avoid clutter\n",
    "            if len(plot_points) > 0:\n",
    "                for i in range(min(5, len(plot_points))):\n",
    "                    pos = plot_points[i]\n",
    "                    label = plot_labels[i]\n",
    "                    ax.text(pos[0], pos[1], pos[2], label, color='white', fontsize=8, \n",
    "                        bbox=dict(facecolor=(0,0,0,0.3), edgecolor='none', alpha=0.7))\n",
    "            \n",
    "            # Add frame number\n",
    "            ax.text2D(0.05, 0.95, f\"Frame {frame}\", transform=ax.transAxes, \n",
    "                    color='white', fontsize=12)\n",
    "            \n",
    "            # Rotation with smooth transition\n",
    "            ax.view_init(elev=20 + 10 * np.sin(frame * 0.1), \n",
    "                        azim=frame * (360/num_frames) % 360)\n",
    "            \n",
    "            # Simple title\n",
    "            ax.set_title(\"Semantic Space Visualization\", color='white', fontsize=14)\n",
    "            \n",
    "            if frame % 5 == 0:\n",
    "                print(f\"Rendering enhanced frame {frame}/{num_frames}\")\n",
    "            \n",
    "            return ax\n",
    "        \n",
    "        # Create output directory if needed\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create animation\n",
    "        try:\n",
    "            # Create animation\n",
    "            anim = animation.FuncAnimation(\n",
    "                fig, update, frames=num_frames, \n",
    "                interval=100,  # Slower for more reliable saving\n",
    "                blit=False\n",
    "            )\n",
    "            \n",
    "            # Use a more reliable writer\n",
    "            output_path = os.path.join(output_dir, f'semantic_enhanced_{uuid.uuid4()}.gif')\n",
    "            print(f\"Saving enhanced animation to {output_path}...\")\n",
    "            \n",
    "            anim.save(\n",
    "                output_path, \n",
    "                writer='pillow', \n",
    "                fps=10,  # Lower fps for more reliability\n",
    "                dpi=80   # Lower resolution\n",
    "            )\n",
    "            \n",
    "            # Verify animation\n",
    "            if os.path.exists(output_path):\n",
    "                file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
    "                print(f\"Enhanced animation saved! File size: {file_size:.2f} MB\")\n",
    "                self.check_gif_validity(output_path)\n",
    "            \n",
    "            plt.close(fig)\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            print(f\"Enhanced animation failed: {e}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            plt.close(fig)\n",
    "            return None\n",
    "    \n",
    "    def visualize_concepts_simple(self, output_dir, num_frames=30):\n",
    "        \"\"\"\n",
    "        Create a simplified animation that focuses just on basic rotation to ensure animation works\n",
    "        \"\"\"\n",
    "        print(\"Creating simplified test animation...\")\n",
    "        \n",
    "        # Set up simple visualization\n",
    "        plt.close('all')  # Close all existing figures first\n",
    "        fig = plt.figure(figsize=(10, 8), facecolor='black')\n",
    "        ax = fig.add_subplot(111, projection='3d', facecolor='black')\n",
    "        \n",
    "        # Get concept vectors \n",
    "        concept_vectors = self.concept_processor.concept_vectors\n",
    "        \n",
    "        # Create a simple function to update the plot\n",
    "        def update(frame):\n",
    "            ax.clear()\n",
    "            \n",
    "            # Simple colored points\n",
    "            points = []\n",
    "            colors = []\n",
    "            \n",
    "            # Extract points from concept vectors\n",
    "            for concept, data in concept_vectors.items():\n",
    "                vector = data['vector'][:3]  # First 3 dimensions\n",
    "                points.append(vector)\n",
    "                colors.append('#3498db')  # Simple blue color\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            points = np.array(points)\n",
    "            \n",
    "            # Plot simple points\n",
    "            ax.scatter(points[:, 0], points[:, 1], points[:, 2], \n",
    "                    c=colors, s=50, alpha=0.8)\n",
    "            \n",
    "            # Add frame number\n",
    "            ax.text2D(0.05, 0.95, f\"Frame {frame}\", transform=ax.transAxes, \n",
    "                    color='white', fontsize=12)\n",
    "            \n",
    "            # Simple rotation\n",
    "            ax.view_init(elev=30, azim=frame * 5)\n",
    "            \n",
    "            # Set limits\n",
    "            ax.set_xlim([-3, 3])\n",
    "            ax.set_ylim([-3, 3])\n",
    "            ax.set_zlim([-3, 3])\n",
    "            \n",
    "            # Simple title\n",
    "            ax.set_title(\"Simple Semantic Space\", color='white', fontsize=14)\n",
    "            \n",
    "            if frame % 5 == 0:\n",
    "                print(f\"Rendering simplified frame {frame}/{num_frames}\")\n",
    "            \n",
    "            return ax\n",
    "        \n",
    "        # Create output directory if needed\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create animation\n",
    "        print(\"Creating simple test animation...\")\n",
    "        try:\n",
    "            # Create animation with fewer frames\n",
    "            anim = animation.FuncAnimation(\n",
    "                fig, update, frames=num_frames, \n",
    "                interval=100,  # Slower for more reliable saving\n",
    "                blit=False\n",
    "            )\n",
    "            \n",
    "            # Use a more reliable writer\n",
    "            output_path = os.path.join(output_dir, f'semantic_test_{uuid.uuid4()}.gif')\n",
    "            print(f\"Saving simple animation to {output_path}...\")\n",
    "            \n",
    "            # Use a more reliable writer with lower quality settings\n",
    "            anim.save(\n",
    "                output_path, \n",
    "                writer='pillow', \n",
    "                fps=10,  # Lower fps for more reliability\n",
    "                dpi=80   # Lower resolution\n",
    "            )\n",
    "            \n",
    "            # Verify animation\n",
    "            if os.path.exists(output_path):\n",
    "                file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
    "                print(f\"Test animation saved! File size: {file_size:.2f} MB\")\n",
    "                self.check_gif_validity(output_path)\n",
    "            \n",
    "            plt.close(fig)\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            print(f\"Test animation failed: {e}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            plt.close(fig)\n",
    "            return None\n",
    "    \n",
    "    def visualize_concepts(self, \n",
    "                        output_dir, \n",
    "                        num_frames=150, \n",
    "                        make_animation=True):\n",
    "        \"\"\"\n",
    "        Create a sophisticated visualization of concept relationships\n",
    "        \"\"\"\n",
    "        print(f\"Generating visualization with {num_frames} frames...\")\n",
    "        \n",
    "        # ADDITION: Basic validation check\n",
    "        if not self.concept_processor or not hasattr(self.concept_processor, 'concept_vectors') or not self.concept_processor.concept_vectors:\n",
    "            print(\"No concept vectors available. Please process ConceptNet data first.\")\n",
    "            return None\n",
    "        \n",
    "        # Set up visualization\n",
    "        try:\n",
    "            self.setup_visualization()\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to set up visualization: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            return None\n",
    "        \n",
    "        # Get concept vectors and important relationships\n",
    "        concept_vectors = self.concept_processor.concept_vectors\n",
    "        important_relationships = self.concept_processor.compute_important_relationships(threshold=1.0)\n",
    "\n",
    "        # More vibrant but still harmonious colors\n",
    "        category_colors = {\n",
    "            'fruit': '#e74c3c',      # Red\n",
    "            'color': '#3498db',      # Blue\n",
    "            'taste': '#f39c12',      # Orange\n",
    "            'vehicle': '#2ecc71',    # Green\n",
    "            'temperature': '#9b59b6', # Purple\n",
    "            'size': '#1abc9c',       # Teal\n",
    "            'generic': '#95a5a6'     # Gray\n",
    "        }\n",
    "\n",
    "        # Language markers\n",
    "        language_markers = {\n",
    "            'en': 'o',   # Circle for English\n",
    "            'de': '^',   # Triangle for German\n",
    "            'unknown': 's'  # Square for unknown\n",
    "        }\n",
    "\n",
    "        # Pre-calculate interaction frames\n",
    "        interaction_frames = {}\n",
    "        relationship_focus_frames = {}\n",
    "        \n",
    "        print(\"Pre-calculating semantic interactions...\")\n",
    "        for frame in tqdm(range(num_frames), desc=\"Generating frame data\"):\n",
    "            # Frame-specific relationship focus\n",
    "            relationship_index = (frame // 10) % max(1, len(important_relationships))\n",
    "            if relationship_index < len(important_relationships):\n",
    "                relationship_focus_frames[frame] = important_relationships[relationship_index]\n",
    "            \n",
    "            # Every few frames, focus on different relationships\n",
    "            if frame % (num_frames // 10) == 0:\n",
    "                interactions = {}\n",
    "                \n",
    "                # Select a subset of relationships to focus on\n",
    "                selection_start = (frame // (num_frames // 10)) % max(1, len(important_relationships) - 5)\n",
    "                selection_end = min(selection_start + 5, len(important_relationships))\n",
    "                \n",
    "                for rel in important_relationships[selection_start:selection_end]:\n",
    "                    source = rel['source']\n",
    "                    target = rel['target']\n",
    "                    interactions[(source, target)] = rel\n",
    "                \n",
    "                interaction_frames[frame] = interactions\n",
    "        \n",
    "        # Function to calculate dynamic positions with controlled oscillation\n",
    "        def calculate_positions(frame):\n",
    "            positions = {}\n",
    "            \n",
    "            for concept, data in concept_vectors.items():\n",
    "                # Get base vector\n",
    "                base_vector = data['vector'][:3]  # First 3 dimensions\n",
    "                \n",
    "                # Add subtle oscillation based on frame - much more subtle now\n",
    "                oscillation = np.array([\n",
    "                    0.01 * np.sin(frame * 0.01 + hash(concept) % 10),\n",
    "                    0.01 * np.cos(frame * 0.008 + hash(concept + \"y\") % 10),\n",
    "                    0.01 * np.sin(frame * 0.005 + hash(concept + \"z\") % 10)\n",
    "                ])\n",
    "                \n",
    "                # Scale oscillation based on semantic potential\n",
    "                oscillation *= data.get('potential', 1.0) * 0.5\n",
    "                \n",
    "                # Calculate position\n",
    "                position = base_vector + oscillation\n",
    "                \n",
    "                positions[concept] = position\n",
    "            \n",
    "            return positions\n",
    "        \n",
    "        # Define update function inside the method scope so it has access to all variables\n",
    "        def update(frame):\n",
    "            self.ax.clear()\n",
    "            \n",
    "            # Reset visualization environment\n",
    "            self.setup_visualization()\n",
    "            \n",
    "            # Set consistent axis limits for stability\n",
    "            self.ax.set_xlim([-3, 3])\n",
    "            self.ax.set_ylim([-3, 3])\n",
    "            self.ax.set_zlim([-3, 3])\n",
    "            \n",
    "            # Dynamic positions for this frame\n",
    "            positions = calculate_positions(frame)\n",
    "            \n",
    "            # Focused relationship for this frame\n",
    "            focus_relationship = relationship_focus_frames.get(frame, None)\n",
    "            \n",
    "            # Visualize concepts\n",
    "            plot_points = []\n",
    "            plot_colors = []\n",
    "            plot_sizes = []\n",
    "            plot_labels = []\n",
    "            plot_categories = []\n",
    "            plot_markers = []\n",
    "            plot_highlights = []  # Track highlighted nodes\n",
    "            \n",
    "            for concept, data in concept_vectors.items():\n",
    "                try:\n",
    "                    # Skip if no position available\n",
    "                    if concept not in positions:\n",
    "                        continue\n",
    "                    \n",
    "                    x, y, z = positions[concept]\n",
    "                    \n",
    "                    # Dynamic size based on semantic potential and node degree\n",
    "                    size = max(50, 150 * data.get('potential', 1.0))\n",
    "                    \n",
    "                    # Color based on semantic category\n",
    "                    category = data.get('category', 'generic')\n",
    "                    color = category_colors.get(category, '#95a5a6')  # Default to gray\n",
    "                    \n",
    "                    # Marker based on language\n",
    "                    lang = data.get('lang', 'unknown')\n",
    "                    marker = language_markers.get(lang, 'o')\n",
    "                    \n",
    "                    # Check if this concept is in the focused relationship\n",
    "                    is_highlighted = False\n",
    "                    if focus_relationship and (concept == focus_relationship['source'] or concept == focus_relationship['target']):\n",
    "                        is_highlighted = True\n",
    "                        size *= 1.5  # Make highlighted concepts larger\n",
    "                    \n",
    "                    # Collect plot data\n",
    "                    plot_points.append([x, y, z])\n",
    "                    plot_colors.append(color)\n",
    "                    plot_sizes.append(size)\n",
    "                    plot_labels.append(concept)\n",
    "                    plot_categories.append(category)\n",
    "                    plot_markers.append(marker)\n",
    "                    plot_highlights.append(is_highlighted)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Could not plot concept {concept}: {e}\")\n",
    "            \n",
    "            # Ensure we have points\n",
    "            if not plot_points:\n",
    "                plot_points = [[0, 0, 0]]\n",
    "                plot_colors = ['white']\n",
    "                plot_sizes = [100]\n",
    "                plot_labels = ['default']\n",
    "                plot_categories = ['generic']\n",
    "                plot_markers = ['o']\n",
    "                plot_highlights = [False]\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            plot_points = np.array(plot_points)\n",
    "            \n",
    "            # Plot points with language-specific markers\n",
    "            for i, ((x, y, z), color, size, marker, is_highlighted) in enumerate(\n",
    "                zip(plot_points, plot_colors, plot_sizes, plot_markers, plot_highlights)):\n",
    "                \n",
    "                # Adjust appearance for highlighted points\n",
    "                edgecolor = 'white'\n",
    "                linewidth = 0.5\n",
    "                alpha = 0.8\n",
    "                \n",
    "                if is_highlighted:\n",
    "                    edgecolor = '#FFD700'  # Gold for highlights\n",
    "                    linewidth = 1.5\n",
    "                    alpha = 1.0\n",
    "                \n",
    "                # Plot with appropriate marker\n",
    "                self.ax.scatter(\n",
    "                    x, y, z,\n",
    "                    c=[color],\n",
    "                    s=size,\n",
    "                    marker=marker,\n",
    "                    alpha=alpha,\n",
    "                    edgecolor=edgecolor,\n",
    "                    linewidth=linewidth,\n",
    "                    depthshade=True\n",
    "                )\n",
    "            \n",
    "            # Add labels with improved readability\n",
    "            label_positions = {}  # Track label positions to avoid overlap\n",
    "\n",
    "            for i, ((x, y, z), label, category, is_highlighted) in enumerate(\n",
    "                zip(plot_points, plot_labels, plot_categories, plot_highlights)):\n",
    "                \n",
    "                # Skip labels for non-highlighted nodes when we have a focus\n",
    "                if focus_relationship and not is_highlighted and frame % (num_frames // 5) != 0:\n",
    "                    continue\n",
    "                \n",
    "                # Create a small offset to prevent label overlap with the point\n",
    "                label_offset = 0.15\n",
    "                text_pos = (x + label_offset, y + label_offset, z + label_offset)\n",
    "                \n",
    "                # Check for label position overlap and adjust if needed\n",
    "                position_key = f\"{text_pos[0]:.1f}_{text_pos[1]:.1f}_{text_pos[2]:.1f}\"\n",
    "                attempt = 0\n",
    "                while position_key in label_positions and attempt < 5:\n",
    "                    # Adjust position slightly\n",
    "                    offset_variation = 0.05 * (attempt + 1)\n",
    "                    text_pos = (\n",
    "                        x + label_offset + offset_variation * np.cos(attempt),\n",
    "                        y + label_offset + offset_variation * np.sin(attempt),\n",
    "                        z + label_offset\n",
    "                    )\n",
    "                    position_key = f\"{text_pos[0]:.1f}_{text_pos[1]:.1f}_{text_pos[2]:.1f}\"\n",
    "                    attempt += 1\n",
    "                \n",
    "                # Record this position\n",
    "                label_positions[position_key] = label\n",
    "                \n",
    "                # Adjust appearance for highlighted labels\n",
    "                font_weight = 'normal'\n",
    "                font_size = 9\n",
    "                bg_alpha = 0.7\n",
    "                bg_color = (0, 0, 0, 0.3)\n",
    "                \n",
    "                if is_highlighted:\n",
    "                    font_weight = 'bold'\n",
    "                    font_size = 10\n",
    "                    bg_alpha = 0.8\n",
    "                    bg_color = (0, 0, 0, 0.5)\n",
    "                \n",
    "                # Add a crisp text label with semi-transparent black background\n",
    "                text = self.ax.text(\n",
    "                    text_pos[0], text_pos[1], text_pos[2],\n",
    "                    label, \n",
    "                    color='white', \n",
    "                    fontsize=font_size,\n",
    "                    fontweight=font_weight,\n",
    "                    bbox=dict(\n",
    "                        facecolor=bg_color,  # Semi-transparent background\n",
    "                        edgecolor=(1, 1, 1, 0.3),  # Subtle white edge\n",
    "                        boxstyle='round,pad=0.2',   # Smaller padding\n",
    "                        alpha=bg_alpha\n",
    "                    ),\n",
    "                    ha='left', \n",
    "                    va='bottom',\n",
    "                    zorder=100  # Ensure labels stay on top\n",
    "                )\n",
    "\n",
    "            # Draw connections between semantically related concepts\n",
    "            # First, draw highlighted relationship if available\n",
    "            if focus_relationship:\n",
    "                source = focus_relationship['source']\n",
    "                target = focus_relationship['target']\n",
    "                relation = focus_relationship['relation']\n",
    "                weight = focus_relationship['weight']\n",
    "                \n",
    "                if source in positions and target in positions:\n",
    "                    # Strong highlight for focused relationship\n",
    "                    self.draw_relationship(\n",
    "                        positions[source], \n",
    "                        positions[target], \n",
    "                        weight, \n",
    "                        relation,\n",
    "                        highlight=True\n",
    "                    )\n",
    "                    \n",
    "                    # Add relation label at midpoint\n",
    "                    midpoint = [(positions[source][i] + positions[target][i])/2 for i in range(3)]\n",
    "                    self.ax.text(\n",
    "                        midpoint[0], midpoint[1], midpoint[2],\n",
    "                        relation,\n",
    "                        color='white',\n",
    "                        fontsize=10,\n",
    "                        fontweight='bold',\n",
    "                        bbox=dict(\n",
    "                            facecolor=(0.1, 0.1, 0.3, 0.7),\n",
    "                            edgecolor='white',\n",
    "                            boxstyle='round',\n",
    "                            alpha=0.9\n",
    "                        ),\n",
    "                        ha='center',\n",
    "                        va='center',\n",
    "                        zorder=150\n",
    "                    )\n",
    "\n",
    "            # Draw other relationships from interaction frames\n",
    "            interactions = interaction_frames.get(frame, {})\n",
    "            for (source, target), rel_data in interactions.items():\n",
    "                # Skip if it's the same as the highlighted relationship\n",
    "                if focus_relationship and source == focus_relationship['source'] and target == focus_relationship['target']:\n",
    "                    continue\n",
    "                    \n",
    "                if source in positions and target in positions:\n",
    "                    relation = rel_data.get('relation', 'related')\n",
    "                    weight = rel_data.get('weight', 1.0)\n",
    "                    \n",
    "                    # Draw standard relationship\n",
    "                    self.draw_relationship(\n",
    "                        positions[source], \n",
    "                        positions[target], \n",
    "                        weight, \n",
    "                        relation,\n",
    "                        highlight=False\n",
    "                    )\n",
    "\n",
    "            # Also draw translation relationships (dotted lines)\n",
    "            for concept, data in concept_vectors.items():\n",
    "                if 'translation_pair' in data:\n",
    "                    translation = data['translation_pair']\n",
    "                    \n",
    "                    if concept in positions and translation in positions:\n",
    "                        # Draw translation relationship\n",
    "                        self.draw_translation_relationship(\n",
    "                            positions[concept],\n",
    "                            positions[translation]\n",
    "                        )\n",
    "\n",
    "            # Add legend for semantic categories with cleaner layout\n",
    "            unique_categories = sorted(list(set(plot_categories)))\n",
    "            legend_elements = []\n",
    "\n",
    "            for category in unique_categories:\n",
    "                color = category_colors.get(category, '#95a5a6')\n",
    "                legend_elements.append(plt.Line2D(\n",
    "                    [0], [0], \n",
    "                    marker='o', \n",
    "                    color='w', \n",
    "                    label=category, \n",
    "                    markerfacecolor=color,\n",
    "                    markersize=8\n",
    "                ))\n",
    "\n",
    "            # Add language marker legend\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', label='English', markersize=8))\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='^', color='w', label='German', markersize=8))\n",
    "\n",
    "            # Add frame counter and time indicator in top corner\n",
    "            self.ax.text2D(\n",
    "                0.02, 0.98, \n",
    "                f\"Frame: {frame}/{num_frames}\", \n",
    "                transform=self.ax.transAxes, \n",
    "                color='white', \n",
    "                fontsize=10,\n",
    "                bbox=dict(\n",
    "                    facecolor=(0, 0, 0, 0.3),\n",
    "                    edgecolor='none',\n",
    "                    boxstyle='round',\n",
    "                    alpha=0.7\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Add legend with better positioning and transparency\n",
    "            legend = self.ax.legend(\n",
    "                handles=legend_elements, \n",
    "                loc='upper right', \n",
    "                fontsize=9, \n",
    "                framealpha=0.4,\n",
    "                edgecolor='white',\n",
    "                facecolor=(0, 0, 0, 0.2)\n",
    "            )\n",
    "\n",
    "            # Set legend text color to white\n",
    "            for text in legend.get_texts():\n",
    "                text.set_color('white')\n",
    "\n",
    "            # If there's a focus relationship, add info panel\n",
    "            if focus_relationship:\n",
    "                self.add_info_panel(focus_relationship, frame)\n",
    "\n",
    "            # Dynamic view rotation - slower and more stable\n",
    "            # Divide animation into segments for smoother transition\n",
    "            if frame < num_frames * 0.25:  # First quarter: gentle rotation\n",
    "                self.ax.view_init(elev=25, azim=frame * 0.3 + 30)\n",
    "            elif frame < num_frames * 0.5:  # Second quarter: change elevation\n",
    "                progress = (frame - num_frames * 0.25) / (num_frames * 0.25)\n",
    "                self.ax.view_init(\n",
    "                    elev=25 + progress * 15,\n",
    "                    azim=30 + num_frames * 0.25 * 0.3\n",
    "                )\n",
    "            elif frame < num_frames * 0.75:  # Third quarter: gentle rotation at new elevation\n",
    "                self.ax.view_init(\n",
    "                    elev=40,\n",
    "                    azim=(frame - num_frames * 0.5) * 0.3 + (30 + num_frames * 0.25 * 0.3)\n",
    "                )\n",
    "            else:  # Last quarter: return to original view\n",
    "                progress = (frame - num_frames * 0.75) / (num_frames * 0.25)\n",
    "                self.ax.view_init(\n",
    "                    elev=40 - progress * 15,\n",
    "                    azim=frame * 0.3 + 30\n",
    "                )\n",
    "\n",
    "            # More elegant and descriptive title\n",
    "            plt.title(\n",
    "                f'Semantic Convergence - Frame {frame}', \n",
    "                color='white', \n",
    "                fontsize=14,\n",
    "                pad=20\n",
    "            )\n",
    "            \n",
    "            # Print progress occasionally\n",
    "            if frame % 25 == 0:\n",
    "                print(f\"Rendering frame {frame}/{num_frames}\")\n",
    "            \n",
    "            return self.ax\n",
    "        \n",
    "        # First, generate a static image for safety\n",
    "        print(\"Creating static visualization...\")\n",
    "        static_frame = num_frames // 2  # Use middle frame for static visualization\n",
    "        try:\n",
    "            update(static_frame)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to generate static frame: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            return None\n",
    "        \n",
    "        # Create output directory if needed\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        static_path = os.path.join(output_dir, f'semantic_static_{uuid.uuid4()}.png')\n",
    "        try:\n",
    "            plt.savefig(static_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"Static visualization saved to {static_path}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to save static image: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            return None\n",
    "        \n",
    "        # Try to generate animation if requested\n",
    "        if make_animation:\n",
    "            try:\n",
    "                print(\"Creating animation...\")\n",
    "                \n",
    "                # ADDITION: Check memory usage\n",
    "                try:\n",
    "                    import psutil\n",
    "                    process = psutil.Process()\n",
    "                    print(f\"Memory usage before animation: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "                except ImportError:\n",
    "                    print(\"psutil not installed, skipping memory usage check\")\n",
    "                \n",
    "                # Create animation with progress tracking\n",
    "                anim = animation.FuncAnimation(\n",
    "                    self.fig, update, frames=num_frames, \n",
    "                    interval=50,  # Faster for smoother appearance\n",
    "                    blit=False  # Don't use blitting for complex scenes\n",
    "                )\n",
    "                \n",
    "                # ADDITION: Try a different writer for animations\n",
    "                from matplotlib.animation import PillowWriter\n",
    "                writer = PillowWriter(fps=24)\n",
    "                \n",
    "                # Save with high-quality settings\n",
    "                output_path = os.path.join(output_dir, f'semantic_dynamics_{uuid.uuid4()}.gif')\n",
    "                \n",
    "                print(f\"Saving animation to {output_path}...\")\n",
    "                try:\n",
    "                    anim.save(\n",
    "                        output_path, \n",
    "                        writer=writer\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with PillowWriter: {e}. Trying default writer...\")\n",
    "                    anim.save(\n",
    "                        output_path, \n",
    "                        writer='pillow', \n",
    "                        fps=24,  # Higher for smoother animation\n",
    "                        dpi=120  # Higher resolution\n",
    "                    )\n",
    "                \n",
    "                # ADDITION: Verify animation success\n",
    "                if os.path.exists(output_path):\n",
    "                    file_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n",
    "                    print(f\"Animation saved successfully! File size: {file_size:.2f} MB\")\n",
    "                    \n",
    "                    # Check if file size seems reasonable\n",
    "                    if file_size < 0.5:  # Less than 0.5 MB might be suspicious for animation\n",
    "                        print(\"Warning: Animation file size is small. It might not be animated properly.\")\n",
    "                        \n",
    "                    # Check frames in GIF\n",
    "                    self.check_gif_validity(output_path)\n",
    "                \n",
    "                print(\"Animation saved successfully!\")\n",
    "                plt.close(self.fig)\n",
    "                return output_path\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Animation failed: {e}\")\n",
    "                import traceback\n",
    "                print(\"Full error details:\")\n",
    "                print(traceback.format_exc())\n",
    "                print(\"Defaulting to static visualization.\")\n",
    "                plt.close(self.fig)\n",
    "                return static_path\n",
    "        else:\n",
    "            plt.close(self.fig)\n",
    "            return static_path\n",
    "        \n",
    "    def draw_relationship(self, pos1, pos2, weight, relation_type, highlight=False):\n",
    "        \"\"\"\n",
    "        Draw a relationship line between two concepts\n",
    "        \"\"\"\n",
    "        # Connection strength based on weight\n",
    "        connection_strength = max(0.5, min(3.0, weight * 1.5))\n",
    "        \n",
    "        # Base color settings\n",
    "        alpha = 0.6\n",
    "        linestyle = '-'\n",
    "        \n",
    "        # Determine color based on relation type\n",
    "        if relation_type in ['IsA', 'PartOf', 'MadeOf']:\n",
    "            # Hierarchical relationships - blue hues\n",
    "            color = (0.2, 0.4, 0.8, alpha)\n",
    "        elif relation_type in ['HasProperty', 'HasA', 'CapableOf', 'UsedFor']:\n",
    "            # Property relationships - green hues\n",
    "            color = (0.2, 0.8, 0.4, alpha)\n",
    "        elif relation_type in ['Antonym', 'DistinctFrom']:\n",
    "            # Opposite relationships - red hues\n",
    "            color = (0.8, 0.2, 0.2, alpha)\n",
    "        elif relation_type in ['AtLocation', 'LocatedNear']:\n",
    "            # Location relationships - purple hues\n",
    "            color = (0.8, 0.2, 0.8, alpha)\n",
    "        else:\n",
    "            # Default relationships - gray\n",
    "            color = (0.7, 0.7, 0.7, alpha)\n",
    "        \n",
    "        # Enhance appearance for highlighted relationships\n",
    "        if highlight:\n",
    "            connection_strength += 1.0\n",
    "            alpha = 0.9\n",
    "            color = (1.0, 0.8, 0.0, alpha)  # Golden for highlights\n",
    "        \n",
    "        # Draw connection line\n",
    "        line = self.ax.plot(\n",
    "            [pos1[0], pos2[0]], \n",
    "            [pos1[1], pos2[1]], \n",
    "            [pos1[2], pos2[2]], \n",
    "            color=color, \n",
    "            linewidth=connection_strength,\n",
    "            linestyle=linestyle,\n",
    "            alpha=alpha,\n",
    "            zorder=50  # Ensure lines are below points but above grid\n",
    "        )\n",
    "        \n",
    "        return line\n",
    "    \n",
    "    def draw_translation_relationship(self, pos1, pos2):\n",
    "        \"\"\"\n",
    "        Draw a translation relationship line between concepts\n",
    "        \"\"\"\n",
    "        # Draw a dashed language relationship line\n",
    "        translation_line = self.ax.plot(\n",
    "            [pos1[0], pos2[0]], \n",
    "            [pos1[1], pos2[1]], \n",
    "            [pos1[2], pos2[2]], \n",
    "            color=(0.6, 0.6, 0.9, 0.7),  # Light blue\n",
    "            linewidth=1.0,\n",
    "            linestyle=':',  # Dotted line\n",
    "            alpha=0.6,\n",
    "            zorder=40\n",
    "        )\n",
    "        \n",
    "        return translation_line\n",
    "    \n",
    "    def add_info_panel(self, relationship, frame):\n",
    "        \"\"\"\n",
    "        Add an information panel showing details about the focused relationship\n",
    "        \"\"\"\n",
    "        source = relationship['source']\n",
    "        target = relationship['target']\n",
    "        relation = relationship['relation']\n",
    "        weight = relationship['weight']\n",
    "        \n",
    "        # Create info text\n",
    "        info_text = f\"Relationship Focus:\\n{source} --[{relation}]--> {target}\\nWeight: {weight:.2f}\"\n",
    "        \n",
    "        # Add info panel\n",
    "        self.ax.text2D(\n",
    "            0.02, 0.05,  # Bottom left\n",
    "            info_text,\n",
    "            transform=self.ax.transAxes,\n",
    "            color='white',\n",
    "            fontsize=10,\n",
    "            bbox=dict(\n",
    "                facecolor=(0.1, 0.1, 0.3, 0.7),\n",
    "                edgecolor='white',\n",
    "                boxstyle='round',\n",
    "                alpha=0.8\n",
    "            ),\n",
    "            family='monospace'\n",
    "        )\n",
    "    def check_gif_validity(self, file_path):\n",
    "        \"\"\"Check if a GIF file contains multiple frames (is animated)\"\"\"\n",
    "        try:\n",
    "            from PIL import Image\n",
    "            with Image.open(file_path) as img:\n",
    "                frames = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        frames += 1\n",
    "                        img.seek(img.tell() + 1)\n",
    "                except EOFError:\n",
    "                    pass\n",
    "                \n",
    "                print(f\"GIF contains {frames} frames\")\n",
    "                return frames > 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking GIF: {e}\")\n",
    "            return False\n",
    "\n",
    "# def process_conceptnet_data(english_data, german_data, output_dir, make_animation=True):\n",
    "#     \"\"\"\n",
    "#     Process ConceptNet data and create visualization\n",
    "#     \"\"\"\n",
    "#     print(\"Starting ConceptNet processing pipeline...\")\n",
    "    \n",
    "#     # Initialize ConceptNet processor\n",
    "#     concept_processor = ConceptNetProcessor(english_data, german_data)\n",
    "    \n",
    "#     # Build semantic graph from ConceptNet data\n",
    "#     concept_processor.build_semantic_graph(max_concepts=100)\n",
    "    \n",
    "#     # Generate concept vectors\n",
    "#     concept_processor.generate_concept_vectors(dimensions=5)\n",
    "    \n",
    "#     # Initialize visualizer\n",
    "#     visualizer = SemanticVisualizer(concept_processor)\n",
    "    \n",
    "#     # Create visualization\n",
    "#     output_path = visualizer.visualize_concepts(\n",
    "#         output_dir, \n",
    "#         num_frames=30,\n",
    "#         make_animation=make_animation\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Visualization created at: {output_path}\")\n",
    "#     return concept_processor, output_path\n",
    "\n",
    "# # Main execution function\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Specify input and output directories\n",
    "#     input_data_path = r'C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Input'\n",
    "#     output_dir = r'C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Output'\n",
    "    \n",
    "#     # Load ConceptNet data\n",
    "#     print(\"Loading ConceptNet data...\")\n",
    "#     english_conceptnet = pd.read_csv(os.path.join(input_data_path, 'conceptnet-assertions-5.7.0.en.tsv'), \n",
    "#                                     sep='\\t', header=None, names=['start', 'rel', 'end', 'weight'])\n",
    "    \n",
    "#     german_conceptnet = pd.read_csv(os.path.join(input_data_path, 'conceptnet-assertions-5.7.0.de.tsv'), \n",
    "#                                    sep='\\t', header=None, names=['start', 'rel', 'end', 'weight'])\n",
    "    \n",
    "#     print(f\"English ConceptNet loaded with {len(english_conceptnet)} assertions.\")\n",
    "#     print(f\"German ConceptNet loaded with {len(german_conceptnet)} assertions.\")\n",
    "    \n",
    "#     # Process data and create visualization\n",
    "#     processor, viz_path = process_conceptnet_data(\n",
    "#         english_conceptnet, \n",
    "#         german_conceptnet, \n",
    "#         output_dir,\n",
    "#         make_animation=True\n",
    "#     )\n",
    "    \n",
    "#     print(\"Process complete!\")\n",
    "# In your process_conceptnet_data function\n",
    "def process_conceptnet_data(english_data, german_data, output_dir, make_animation=True):\n",
    "    \"\"\"\n",
    "    Process ConceptNet data and create visualization\n",
    "    \"\"\"\n",
    "    print(\"Starting ConceptNet processing pipeline...\")\n",
    "    \n",
    "    # Initialize ConceptNet processor\n",
    "    concept_processor = ConceptNetProcessor(english_data, german_data)\n",
    "    \n",
    "    # Build semantic graph from ConceptNet data\n",
    "    concept_processor.build_semantic_graph(max_concepts=100)\n",
    "    \n",
    "    # Generate concept vectors\n",
    "    concept_processor.generate_concept_vectors(dimensions=5)\n",
    "    \n",
    "    # Initialize visualizer\n",
    "    visualizer = SemanticVisualizer(concept_processor)\n",
    "    \n",
    "    # Create visualization - use the simplified version instead\n",
    "    output_path = visualizer.visualize_concepts_simple(\n",
    "        output_dir, \n",
    "        num_frames=20  # Even fewer frames for testing\n",
    "    )\n",
    "    \n",
    "    print(f\"Visualization created at: {output_path}\")\n",
    "    return concept_processor, output_path\n",
    "\n",
    "# Main execution function\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify input and output directories\n",
    "    input_data_path = r'C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Input'\n",
    "    output_dir = r'C:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica\\Semantica\\Data\\Output'\n",
    "    \n",
    "    # Load ConceptNet data\n",
    "    print(\"Loading ConceptNet data...\")\n",
    "    english_conceptnet = pd.read_csv(os.path.join(input_data_path, 'conceptnet-assertions-5.7.0.en.tsv'), \n",
    "                                    sep='\\t', header=None, names=['start', 'rel', 'end', 'weight'])\n",
    "    \n",
    "    german_conceptnet = pd.read_csv(os.path.join(input_data_path, 'conceptnet-assertions-5.7.0.de.tsv'), \n",
    "                                   sep='\\t', header=None, names=['start', 'rel', 'end', 'weight'])\n",
    "    \n",
    "    print(f\"English ConceptNet loaded with {len(english_conceptnet)} assertions.\")\n",
    "    print(f\"German ConceptNet loaded with {len(german_conceptnet)} assertions.\")\n",
    "    \n",
    "    # Process data and create visualization\n",
    "    processor, viz_path = process_conceptnet_data(\n",
    "        english_conceptnet, \n",
    "        german_conceptnet, \n",
    "        output_dir,\n",
    "        make_animation=True\n",
    "    )\n",
    "    \n",
    "    print(\"Process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737f94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
